{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39189dfb",
   "metadata": {},
   "source": [
    "# 1. Install Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e906fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bioinformatics Tools (Ubuntu)\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y fastp flash bwa samtools\n",
    "\n",
    "# Python Library\n",
    "!pip3 install biopython cutadapt pysam --break-system-packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd54a0a6",
   "metadata": {},
   "source": [
    "# 2 Trimming and Discard trimmed sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac5eda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Specify the folder containing your input files\n",
    "# Specify the folder where you want to save the untrimmed sequences (adapter-free sequences)\n",
    "\n",
    "input_folder = \"fastq\"\n",
    "untrimmed_output_folder = \"fastq/A_Untrimmed_output\"\n",
    "\n",
    "# Define the adapter sequences for R1 and R2\n",
    "adapter_sequence_r1 = \"AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC\"\n",
    "adapter_sequence_r2 = \"AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT\"\n",
    "\n",
    "# Use glob to get a list of all input file pairs (R1 and R2) in the folder\n",
    "input_file_pairs = []\n",
    "for input_r1 in glob.glob(os.path.join(input_folder, \"*_R1.fastq.gz\")):\n",
    "    # Assuming R2 files have the same naming format as R1 files\n",
    "    input_r2 = input_r1.replace(\"_R1.fastq.gz\", \"_R2.fastq.gz\")\n",
    "    if os.path.exists(input_r2):  # Ensure R2 file exists\n",
    "        input_file_pairs.append({\"r1\": input_r1, \"r2\": input_r2})\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(untrimmed_output_folder, exist_ok=True)\n",
    "\n",
    "for input_files in input_file_pairs:\n",
    "    input_r1 = input_files[\"r1\"]\n",
    "    input_r2 = input_files[\"r2\"]\n",
    "\n",
    "    # Define output file paths for untrimmed (clean, adapter-free) sequences\n",
    "    untrimmed_r1 = os.path.join(untrimmed_output_folder, os.path.basename(input_r1).replace(\".fastq.gz\", \"_untrimmed.fastq.gz\"))\n",
    "    untrimmed_r2 = os.path.join(untrimmed_output_folder, os.path.basename(input_r2).replace(\".fastq.gz\", \"_untrimmed.fastq.gz\"))\n",
    "\n",
    "    # Use cutadapt to keep only untrimmed sequences (completely adapter-free)\n",
    "    result = subprocess.run([\n",
    "        \"cutadapt\",\n",
    "        \"-a\", adapter_sequence_r1,  # Adapter for R1\n",
    "        \"-A\", adapter_sequence_r2,  # Adapter for R2\n",
    "        \"-O\", \"15\",                 # Minimum overlap for adapter trimming\n",
    "        \"--discard-trimmed\",        # Discard sequences where trimming occurred\n",
    "        \"-o\", untrimmed_r1,         # Save only untrimmed R1 reads\n",
    "        \"-p\", untrimmed_r2,         # Save only untrimmed R2 reads\n",
    "        input_r1, input_r2\n",
    "    ], capture_output=True, text=True)\n",
    "\n",
    "    # Log result\n",
    "    if result.returncode == 0:\n",
    "        print(f\"Untrimmed sequences saved: {untrimmed_r1}, {untrimmed_r2}\")\n",
    "    else:\n",
    "        print(f\"Error processing {input_r1} and {input_r2}:\\n{result.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d43590a",
   "metadata": {},
   "source": [
    "# 3. Q filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330bc19f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Quality threshold (Phred score)\n",
    "quality_threshold = 30\n",
    "\n",
    "# Set input and output folders\n",
    "input_folder = \"fastq/A_Untrimmed_output\"\n",
    "output_folder = \"fastq/B_Qfiltered\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate through files in the input folder, processing only those ending with \"_untrimmed.fastq.gz\"\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\"_untrimmed.fastq.gz\"):\n",
    "        # Input file path\n",
    "        input_file = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Output filename (e.g., sample_untrimmed.fastq.gz -> sample_Qfiltered.fastq.gz)\n",
    "        output_file = os.path.join(\n",
    "            output_folder, \n",
    "            filename.replace(\"_untrimmed.fastq.gz\", \"_Qfiltered.fastq.gz\")\n",
    "        )\n",
    "        \n",
    "        # Execute fastp in single-end mode for each file\n",
    "        subprocess.call([\n",
    "            \"fastp\",\n",
    "            \"-i\", input_file,                      # Input file\n",
    "            \"-o\", output_file,                     # Output file\n",
    "            \"-q\", str(quality_threshold),          # Quality threshold for a base to be qualified\n",
    "            \"-u\", \"15\",                            # Discard reads if the percentage of unqualified bases is >= 15%\n",
    "            \"-l\", \"151\",                           # Minimum read length to keep\n",
    "            \"--cut_mean_quality\", \"30\",            # Discard reads if mean quality is less than 30\n",
    "            \"--html\", f\"{output_file}.html\",       # HTML report file path\n",
    "            \"--json\", f\"{output_file}.json\"        # JSON report file path\n",
    "        ])\n",
    "        \n",
    "        print(f\"Filtering for {filename} is complete.\\n\"\n",
    "              f\"Output FASTQ : {output_file}\\n\"\n",
    "              f\"Reports      : {output_file}.html / {output_file}.json\\n\")\n",
    "\n",
    "print(\"All filtering processes are done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34270d38",
   "metadata": {},
   "source": [
    "# 4. Match Paired-End Read IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f915a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def extract_matching_reads(r1_path, r2_path, out_r1_path, out_r2_path):\n",
    "    def get_read_id(header):\n",
    "        # Extract ID from the FASTQ header\n",
    "        return header.split()[0].replace('/1', '').replace('/2', '')\n",
    "\n",
    "    r1_ids = set()\n",
    "    r2_ids = set()\n",
    "\n",
    "    # Extract all read IDs from the R1 file\n",
    "    with gzip.open(r1_path, 'rt') as r1_file:\n",
    "        while True:\n",
    "            header = r1_file.readline()\n",
    "            if not header:\n",
    "                break\n",
    "            r1_ids.add(get_read_id(header.strip()))\n",
    "            # Skip the other 3 lines of the read (sequence, +, quality)\n",
    "            [r1_file.readline() for _ in range(3)]\n",
    "\n",
    "    with gzip.open(r2_path, 'rt') as r2_file:\n",
    "        while True:\n",
    "            header = r2_file.readline()\n",
    "            if not header:\n",
    "                break\n",
    "            r2_ids.add(get_read_id(header.strip()))\n",
    "            [r2_file.readline() for _ in range(3)]\n",
    "\n",
    "    # Find common and unique IDs\n",
    "    matching_ids = r1_ids & r2_ids\n",
    "    r1_only = r1_ids - r2_ids\n",
    "    r2_only = r2_ids - r1_ids\n",
    "\n",
    "    print(f\"Processing {os.path.basename(r1_path)} and {os.path.basename(r2_path)}\")\n",
    "    print(f\"Total R1 IDs: {len(r1_ids)}, Total R2 IDs: {len(r2_ids)}, Matching IDs: {len(matching_ids)}\")\n",
    "    print(f\"IDs only in R1: {len(r1_only)}, IDs only in R2: {len(r2_only)}\\n\")\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    for out_path in [out_r1_path, out_r2_path]:\n",
    "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    # Function to write only the reads with matching IDs to a new file\n",
    "    def write_matching_reads(input_path, output_path, matching_ids):\n",
    "        with gzip.open(input_path, 'rt') as infile, gzip.open(output_path, 'wt') as outfile:\n",
    "            while True:\n",
    "                lines = [infile.readline() for _ in range(4)]\n",
    "                if not lines[0]:\n",
    "                    break\n",
    "                read_id = get_read_id(lines[0].strip())\n",
    "                if read_id in matching_ids:\n",
    "                    outfile.writelines(lines)\n",
    "\n",
    "    # Write the filtered R1 and R2 files\n",
    "    write_matching_reads(r1_path, out_r1_path, matching_ids)\n",
    "    write_matching_reads(r2_path, out_r2_path, matching_ids)\n",
    "\n",
    "# --------------------------\n",
    "# Apply to all file pairs\n",
    "# --------------------------\n",
    "\n",
    "input_folder = \"fastq/B_Qfiltered\"\n",
    "output_folder = \"fastq/C_ID_matched\"\n",
    "\n",
    "# Find all R1 files\n",
    "r1_files = glob.glob(os.path.join(input_folder, \"*_R1_Qfiltered.fastq.gz\"))\n",
    "\n",
    "# For each R1, find the corresponding R2 file and run the process\n",
    "for r1_file in r1_files:\n",
    "    r2_file = r1_file.replace(\"_R1_Qfiltered.fastq.gz\", \"_R2_Qfiltered.fastq.gz\")\n",
    "    \n",
    "    if os.path.exists(r2_file):\n",
    "        # Set the output file paths\n",
    "        base_name = os.path.basename(r1_file).replace(\"_R1_Qfiltered.fastq.gz\", \"\")\n",
    "        out_r1 = os.path.join(output_folder, f\"{base_name}_ID_match_R1.fastq.gz\")\n",
    "        out_r2 = os.path.join(output_folder, f\"{base_name}_ID_match_R2.fastq.gz\")\n",
    "        \n",
    "        # Execute the function\n",
    "        extract_matching_reads(r1_file, r2_file, out_r1, out_r2)\n",
    "    else:\n",
    "        print(f\"Warning: {r2_file} not found. Skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf47834",
   "metadata": {},
   "source": [
    "# 5 Merge W/ Flash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa868eb0",
   "metadata": {},
   "source": [
    "## 5.1 [R1_back]-[R2_back] merge (FLASH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d88014",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "# === Folder Setup ===\n",
    "input_folder = \"fastq/C_id_matched\"\n",
    "output_folder = \"fastq/D_merged_output\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# === Set N-values (Overlap Length) per Sample Prefix ===\n",
    "sample_n_mapping = {\n",
    "    \"Temp\": 122,\n",
    "}\n",
    "\n",
    "# === Find List of all R1_B Files ===\n",
    "r1_files = glob.glob(os.path.join(input_folder, \"*_R1.fastq.gz\"))\n",
    "\n",
    "print(f\"🔎 Found {len(r1_files)} R1 files.\")\n",
    "\n",
    "# === Process Each R1_B File ===\n",
    "for r1_path in r1_files:\n",
    "    sample_base = os.path.basename(r1_path).replace(\"_R1.fastq.gz\", \"\")\n",
    "    r2_path = os.path.join(input_folder, f\"{sample_base}_R2.fastq.gz\")\n",
    "\n",
    "    if not os.path.exists(r2_path):\n",
    "        print(f\"⚠️ Matching R2 file not found for {sample_base} → Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Find the corresponding N value for the filename\n",
    "    matched_n = None\n",
    "    for prefix, n_value in sample_n_mapping.items():\n",
    "        if prefix in sample_base:\n",
    "            matched_n = n_value\n",
    "            break\n",
    "\n",
    "    if matched_n is None:\n",
    "        print(f\"⚠️ No N value matched for {sample_base} → Skipping.\")\n",
    "        continue\n",
    "\n",
    "    output_name = f\"{sample_base}_FLASH\"\n",
    "\n",
    "    print(f\"🔵 Running FLASH for sample: {sample_base} (N={matched_n})\")\n",
    "\n",
    "    try:\n",
    "        # Execute the FLASH command\n",
    "        subprocess.check_call([\n",
    "            \"flash\",\n",
    "            \"-m\", str(matched_n),   # minimum overlap\n",
    "            \"-M\", str(matched_n),   # Maximum overlap\n",
    "            \"-o\", output_name,      # Output file prefix\n",
    "            \"-d\", output_folder,    # Output directory\n",
    "            r1_path,\n",
    "            r2_path\n",
    "        ])\n",
    "        print(f\"✅ FLASH merging complete → {os.path.join(output_folder, output_name)}.fastq\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ FLASH merging failed for {sample_base}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d774d169",
   "metadata": {},
   "source": [
    "# 6. fastq -> fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a7fe75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "from Bio import SeqIO\n",
    "\n",
    "# Input and output folder paths\n",
    "input_folder = \"fastq/D_merged_output\"\n",
    "output_folder = \"fastq/E_fastq_to_fasta\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    # Process only files with .fastq or .fastq.gz extensions\n",
    "    if filename.endswith(\".fastq\") or filename.endswith(\".fastq.gz\"):\n",
    "        input_file = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Set output filename (.fasta extension)\n",
    "        output_file = os.path.join(\n",
    "            output_folder,\n",
    "            filename.replace(\".fastq.gz\", \".fasta\").replace(\".fastq\", \".fasta\")\n",
    "        )\n",
    "\n",
    "        # Choose open mode based on gzip\n",
    "        open_func = gzip.open if filename.endswith(\".gz\") else open\n",
    "\n",
    "        # Read FASTQ and convert to FASTA\n",
    "        with open_func(input_file, \"rt\") as fastq_file:\n",
    "            # open in text mode\n",
    "            records = list(SeqIO.parse(fastq_file, \"fastq\"))\n",
    "\n",
    "        # Save as FASTA\n",
    "        with open(output_file, \"w\") as fasta_file:\n",
    "            SeqIO.write(records, fasta_file, \"fasta\")\n",
    "\n",
    "        print(f\"Converted: {filename} → {os.path.basename(output_file)}\")\n",
    "\n",
    "print(\"All conversions are done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19e501c",
   "metadata": {},
   "source": [
    "# 7. Binary data reference seqeunce data generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb1ace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from itertools import product\n",
    "\n",
    "# Sequence definitions\n",
    "LOW = \"ACTCAATTAC\"\n",
    "HIGH = \"ACACATTATC\"\n",
    "CONNECTOR = \"ACTCATATAC\"\n",
    "\n",
    "# Set output directory\n",
    "output_dir = \"fastq/Logger_reference\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate sequences\n",
    "sequences_cl = []\n",
    "prefix = CONNECTOR + LOW  # Fixed prefix\n",
    "\n",
    "for bits in product([0, 1], repeat=8):  # 2^8 combinations\n",
    "    label = []\n",
    "    parts = [prefix]\n",
    "\n",
    "    for b in bits:\n",
    "        parts.append(CONNECTOR)\n",
    "        x = HIGH if b else LOW\n",
    "        label.append(\"H\" if b else \"L\")\n",
    "        parts.append(x)\n",
    "\n",
    "    seq_id = f\"seq_CLH_{''.join(label)}\"\n",
    "    sequence = ''.join(parts)\n",
    "    sequences_cl.append((seq_id, sequence))\n",
    "\n",
    "# Save FASTA file\n",
    "fasta_path = os.path.join(output_dir, \"Logger_reference.fasta\")\n",
    "with open(fasta_path, \"w\") as f:\n",
    "    for seq_id, seq in sequences_cl:\n",
    "        f.write(f\">{seq_id}\\n{seq}\\n\")\n",
    "\n",
    "# Save CSV file\n",
    "csv_path = os.path.join(output_dir, \"Logger_reference.csv\")\n",
    "with open(csv_path, \"w\", newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Sequence_ID\", \"Sequence\"])\n",
    "    for seq_id, seq in sequences_cl:\n",
    "        writer.writerow([seq_id, seq])\n",
    "\n",
    "print(\"✅ Logger_reference.fasta and Logger_reference.csv have been created successfully.\")\n",
    "print(f\" - Number of generated sequences: {len(sequences_cl)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d44813",
   "metadata": {},
   "source": [
    "# 8. Reference sequence - Sample Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eeee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index reference\n",
    "!bwa index \"fastq/Logger_reference/Logger_reference.fasta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed4f541",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set the path to the reference sequence file\n",
    "reference_file=\"fastq/Logger_reference/Logger_reference.fasta\"\n",
    "\n",
    "# Set the directory containing your filtered FASTA files\n",
    "fasta_directory=\"fastq/E_fastq_to_fasta\"\n",
    "# Set the output directory for aligned SAM files\n",
    "output_dir=\"fastq/1_align_sam\"\n",
    "\n",
    "# Make sure the output directory exists or create it if necessary\n",
    "mkdir -p \"$output_dir\"\n",
    "\n",
    "# Iterate through filtered FASTA files in the specified directory\n",
    "for fasta_file in \"$fasta_directory\"/*extendedFrags.fasta; do\n",
    "    # Generate an output file name based on the input filename\n",
    "    output_file=\"$output_dir/$(basename \"$fasta_file\" .fasta).sam\"\n",
    "\n",
    "    # Perform the BWA alignment \n",
    "    bwa mem -M -t 4 \"$reference_file\" \"$fasta_file\" > \"$output_file\"\n",
    "\n",
    "    echo \"Alignment completed for $fasta_file. Result saved as $output_file\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95e8498",
   "metadata": {},
   "source": [
    "## 8.1 sam to bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1bdfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set the path to the directory containing SAM files\n",
    "sam_dir=\"fastq/1_align_sam\"\n",
    "# Set the output directory for BAM files\n",
    "bam_dir=\"fastq/2_align_bam\"\n",
    "\n",
    "# Make sure the output directory exists or create it if necessary\n",
    "mkdir -p \"$bam_dir\"\n",
    "\n",
    "# Convert SAM files to BAM\n",
    "for sam_file in \"$sam_dir\"/*.sam; do\n",
    "    bam_file=\"$bam_dir/$(basename \"$sam_file\" .sam).bam\"\n",
    "    samtools view -bS \"$sam_file\" -o \"$bam_file\"\n",
    "    echo \"Conversion from $sam_file to $bam_file is complete.\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a50049b",
   "metadata": {},
   "source": [
    "## 8.2  Convert BAM to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "abe6073c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fastq/3_align_csv/Temp_30_12h_ID_match_FLASH.extendedFrags.csv',\n",
       " 'fastq/3_align_csv/Temp_20_12h_ID_match_FLASH.extendedFrags.csv',\n",
       " 'fastq/3_align_csv/Temp_60_12h_ID_match_FLASH.extendedFrags.csv',\n",
       " 'fastq/3_align_csv/Temp_40_12h_ID_match_FLASH.extendedFrags.csv',\n",
       " 'fastq/3_align_csv/Temp_50_12h_ID_match_FLASH.extendedFrags.csv']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pysam\n",
    "import pandas as pd\n",
    "\n",
    "# Input folder (path where BAM files are located)\n",
    "input_folder = \"fastq/2_align_bam\"\n",
    "# Output folder (path to save CSV files)\n",
    "output_folder = \"fastq/3_align_csv\"\n",
    "\n",
    "# Create the output folder if it does not exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Function to convert a BAM file to CSV, including optional fields\n",
    "def bam_to_csv(bam_file, output_folder):\n",
    "    output_csv = os.path.join(output_folder, os.path.basename(bam_file).replace(\".bam\", \".csv\"))\n",
    "    \n",
    "    # Read the BAM file\n",
    "    with pysam.AlignmentFile(bam_file, \"rb\") as bam:\n",
    "        records = []\n",
    "        \n",
    "        for read in bam:\n",
    "            # Standard BAM fields\n",
    "            record = {\n",
    "                \"QNAME\": read.query_name,\n",
    "                \"FLAG\": read.flag,\n",
    "                \"RNAME\": bam.get_reference_name(read.reference_id) if read.reference_id >= 0 else \"*\",\n",
    "                \"POS\": read.reference_start + 1,\n",
    "                \"MAPQ\": read.mapping_quality,\n",
    "                \"CIGAR\": read.cigarstring if read.cigarstring else \"*\",\n",
    "                \"RNEXT\": bam.get_reference_name(read.next_reference_id) if read.next_reference_id >= 0 else \"*\",\n",
    "                \"PNEXT\": read.next_reference_start + 1 if read.next_reference_start >= 0 else 0,\n",
    "                \"TLEN\": read.template_length,\n",
    "                \"SEQ\": read.query_sequence if read.query_sequence else \"*\",\n",
    "                \"QUAL\": read.qual if read.qual else \"*\",\n",
    "            }\n",
    "            \n",
    "            # Add optional fields (tags)\n",
    "            for tag, value in read.tags:\n",
    "                record[tag] = value\n",
    "\n",
    "            records.append(record)\n",
    "    \n",
    "    # Create a DataFrame from the list of records\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    # Fill any missing optional fields with \"*\" instead of NaN for consistency\n",
    "    df = df.fillna(\"*\")\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    return output_csv\n",
    "\n",
    "# Find all BAM files in the folder\n",
    "bam_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".bam\")]\n",
    "\n",
    "# Convert all BAM files to CSV\n",
    "csv_files = []\n",
    "for bam_file in bam_files:\n",
    "    csv_file = bam_to_csv(bam_file, output_folder)\n",
    "    csv_files.append(csv_file)\n",
    "\n",
    "# Print the list of converted CSV files\n",
    "csv_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3d24ca",
   "metadata": {},
   "source": [
    "## 8.3 Filter Alignments by MAPQ Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a9e2500b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Temp_20_12h_ID_match_FLASH.extendedFrags.csv → Temp_20_12h_ID_match_FLASH.extendedFrags.csv | kept=170, removed=315 | threshold=10, keep_nan=True\n",
      "✅ Temp_30_12h_ID_match_FLASH.extendedFrags.csv → Temp_30_12h_ID_match_FLASH.extendedFrags.csv | kept=79, removed=672 | threshold=10, keep_nan=True\n",
      "✅ Temp_40_12h_ID_match_FLASH.extendedFrags.csv → Temp_40_12h_ID_match_FLASH.extendedFrags.csv | kept=190, removed=842 | threshold=10, keep_nan=True\n",
      "✅ Temp_50_12h_ID_match_FLASH.extendedFrags.csv → Temp_50_12h_ID_match_FLASH.extendedFrags.csv | kept=170, removed=727 | threshold=10, keep_nan=True\n",
      "✅ Temp_60_12h_ID_match_FLASH.extendedFrags.csv → Temp_60_12h_ID_match_FLASH.extendedFrags.csv | kept=59, removed=730 | threshold=10, keep_nan=True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== Settings =====\n",
    "input_dir = Path(\"fastq/3_align_csv\") # Input folder containing CSV files\n",
    "output_dir = input_dir / \"MAPQ_removed\"  # Output folder for filtered CSV files\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MAPQ_THRESHOLD = 10     # Keep rows where MAPQ > this value\n",
    "KEEP_NAN = True         # Keep rows with NaN MAPQ values (e.g., unaligned reads)\n",
    "# ====================\n",
    "\n",
    "def process_one_csv(in_path: Path, out_dir: Path, mapq_threshold: int, keep_nan: bool = True):\n",
    "    out_path = out_dir / in_path.name\n",
    "\n",
    "    # Remove existing output file to avoid duplicates\n",
    "    if out_path.exists():\n",
    "        out_path.unlink()\n",
    "\n",
    "    # Read input CSV\n",
    "    try:\n",
    "        df = pd.read_csv(in_path)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Read fail: {in_path.name} -> {e}\")\n",
    "        return\n",
    "\n",
    "    # Skip if MAPQ column does not exist\n",
    "    if \"MAPQ\" not in df.columns:\n",
    "        print(f\"⚠️  Skip (no MAPQ column): {in_path.name}\")\n",
    "        return\n",
    "\n",
    "    # Convert MAPQ column to numeric (invalid entries become NaN)\n",
    "    m = pd.to_numeric(df[\"MAPQ\"], errors=\"coerce\")\n",
    "\n",
    "    # Filtering mask: keep MAPQ > threshold, optionally keep NaN\n",
    "    keep_mask = (m > mapq_threshold) | (m.isna() if keep_nan else False)\n",
    "\n",
    "    kept = int(keep_mask.sum())\n",
    "    removed = int((~keep_mask).sum())\n",
    "\n",
    "    # Save filtered CSV\n",
    "    df.loc[keep_mask].to_csv(out_path, index=False)\n",
    "    print(\n",
    "        f\"✅ {in_path.name} → {out_path.name} | kept={kept}, removed={removed} \"\n",
    "        f\"| threshold={mapq_threshold}, keep_nan={keep_nan}\"\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    csv_files = sorted(input_dir.glob(\"*.csv\"))\n",
    "    if not csv_files:\n",
    "        print(f\"⚠️  No CSV files in {input_dir}\")\n",
    "        return\n",
    "\n",
    "    for p in csv_files:\n",
    "        process_one_csv(p, output_dir, MAPQ_THRESHOLD, KEEP_NAN)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a5954e",
   "metadata": {},
   "source": [
    "# Histogram Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1820fb98",
   "metadata": {},
   "source": [
    "## A.1. point Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "05d731b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed: Temp_60_12h_ID_match_FLASH.extendedFrags.csv → fastq/4_point_histogram/Temp_60_12h_ID_match_FLASH.extendedFrags_pattern_processed.csv\n",
      "✅ Processed: Temp_50_12h_ID_match_FLASH.extendedFrags.csv → fastq/4_point_histogram/Temp_50_12h_ID_match_FLASH.extendedFrags_pattern_processed.csv\n",
      "✅ Processed: Temp_40_12h_ID_match_FLASH.extendedFrags.csv → fastq/4_point_histogram/Temp_40_12h_ID_match_FLASH.extendedFrags_pattern_processed.csv\n",
      "✅ Processed: Temp_20_12h_ID_match_FLASH.extendedFrags.csv → fastq/4_point_histogram/Temp_20_12h_ID_match_FLASH.extendedFrags_pattern_processed.csv\n",
      "✅ Processed: Temp_30_12h_ID_match_FLASH.extendedFrags.csv → fastq/4_point_histogram/Temp_30_12h_ID_match_FLASH.extendedFrags_pattern_processed.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def simplify_rname_pattern(input_folder, output_folder=None):\n",
    "    \"\"\"\n",
    "    From each CSV in `input_folder`, keep only QNAME/RNAME, strip 'seq_CLH_' from RNAME,\n",
    "    split the remaining pattern into per-character columns, and save results to `output_folder`.\n",
    "    If `output_folder` is None, save to <input_folder>/processed.\n",
    "    \"\"\"\n",
    "    if output_folder is None:\n",
    "        output_folder = os.path.join(input_folder, \"processed\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Collect only source CSVs (avoid re-processing already produced files)\n",
    "    csv_files = [\n",
    "        f for f in os.listdir(input_folder)\n",
    "        if f.endswith(\".csv\") and not f.endswith(\"_pattern_processed.csv\")\n",
    "    ]\n",
    "\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            if 'RNAME' not in df.columns or 'QNAME' not in df.columns:\n",
    "                print(f\"\\u26a0\\ufe0f Skipping {file} (missing 'QNAME' or 'RNAME' column)\")\n",
    "                continue\n",
    "\n",
    "            # Keep only QNAME and RNAME\n",
    "            df_simple = df[['QNAME', 'RNAME']].copy()\n",
    "\n",
    "            # Remove 'seq_CLH_' prefix\n",
    "            df_simple['Pattern'] = df_simple['RNAME'].str.replace(r'^seq_CLH_', '', regex=True)\n",
    "\n",
    "            # Split pattern string into individual characters\n",
    "            pattern_split = df_simple['Pattern'].apply(lambda x: pd.Series(list(x)))\n",
    "            pattern_split.columns = [f'Pattern{i}' for i in range(pattern_split.shape[1])]\n",
    "\n",
    "            # Concatenate\n",
    "            df_final = pd.concat([df_simple, pattern_split], axis=1)\n",
    "\n",
    "            # Save\n",
    "            output_name = file.replace(\".csv\", \"_pattern_processed.csv\")\n",
    "            output_path = os.path.join(output_folder, output_name)\n",
    "            df_final.to_csv(output_path, index=False)\n",
    "\n",
    "            print(f\"\\u2705 Processed: {file} \\u2192 {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\u26a0\\ufe0f Failed to process {file}: {e}\")\n",
    "\n",
    "# Example usage (custom output folder)\n",
    "simplify_rname_pattern(\n",
    "    input_folder=\"fastq/3_align_csv/MAPQ_removed\",\n",
    "    output_folder=\"fastq/4_point_histogram\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a394d9a",
   "metadata": {},
   "source": [
    "## A.2. point count PNG, Normalized PNG, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0c3e6208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Temp_20_12h_ID_match_FLASH.extendedFrags_pattern_processed.csv -> CSV:Temp_20_12h_ID_match_FLASH.extendedFrags_pattern_count.csv, PNG:Temp_20_12h_ID_match_FLASH.extendedFrags_pattern_count.png, PNG(Norm):Temp_20_12h_ID_match_FLASH.extendedFrags_pattern_count_normalized.png\n",
      "✅ Temp_60_12h_ID_match_FLASH.extendedFrags_pattern_processed.csv -> CSV:Temp_60_12h_ID_match_FLASH.extendedFrags_pattern_count.csv, PNG:Temp_60_12h_ID_match_FLASH.extendedFrags_pattern_count.png, PNG(Norm):Temp_60_12h_ID_match_FLASH.extendedFrags_pattern_count_normalized.png\n",
      "✅ Temp_40_12h_ID_match_FLASH.extendedFrags_pattern_processed.csv -> CSV:Temp_40_12h_ID_match_FLASH.extendedFrags_pattern_count.csv, PNG:Temp_40_12h_ID_match_FLASH.extendedFrags_pattern_count.png, PNG(Norm):Temp_40_12h_ID_match_FLASH.extendedFrags_pattern_count_normalized.png\n",
      "✅ Temp_30_12h_ID_match_FLASH.extendedFrags_pattern_processed.csv -> CSV:Temp_30_12h_ID_match_FLASH.extendedFrags_pattern_count.csv, PNG:Temp_30_12h_ID_match_FLASH.extendedFrags_pattern_count.png, PNG(Norm):Temp_30_12h_ID_match_FLASH.extendedFrags_pattern_count_normalized.png\n",
      "✅ Temp_50_12h_ID_match_FLASH.extendedFrags_pattern_processed.csv -> CSV:Temp_50_12h_ID_match_FLASH.extendedFrags_pattern_count.csv, PNG:Temp_50_12h_ID_match_FLASH.extendedFrags_pattern_count.png, PNG(Norm):Temp_50_12h_ID_match_FLASH.extendedFrags_pattern_count_normalized.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def summarize_and_plot_all_processed_files(\n",
    "    input_folder,\n",
    "    output_folder=None,\n",
    "    csv_dir=None,\n",
    "    png_dir=None,\n",
    "    png_norm_dir=None,\n",
    "    pattern_suffix=\"_pattern_processed.csv\"\n",
    "):\n",
    "    # ====== Resolve output directories ======\n",
    "    if output_folder is not None:\n",
    "        # If output_folder is given, use its subdirs unless explicit dirs are provided\n",
    "        if csv_dir is None:\n",
    "            csv_dir = os.path.join(output_folder, \"count_csv\")\n",
    "        if png_dir is None:\n",
    "            png_dir = os.path.join(output_folder, \"count_png\")\n",
    "        if png_norm_dir is None:\n",
    "            png_norm_dir = os.path.join(output_folder, \"count_png_normalized\")\n",
    "    else:\n",
    "        # Backward-compatible defaults under input_folder\n",
    "        if csv_dir is None:\n",
    "            csv_dir = os.path.join(input_folder, \"count_csv\")\n",
    "        if png_dir is None:\n",
    "            png_dir = os.path.join(input_folder, \"count_png\")\n",
    "        if png_norm_dir is None:\n",
    "            png_norm_dir = os.path.join(input_folder, \"count_png_normalized\")\n",
    "\n",
    "    # Create folders if they do not exist\n",
    "    os.makedirs(csv_dir, exist_ok=True)\n",
    "    os.makedirs(png_dir, exist_ok=True)\n",
    "    os.makedirs(png_norm_dir, exist_ok=True)\n",
    "\n",
    "    # Select only files matching *_pattern_processed.csv\n",
    "    processed_files = [f for f in os.listdir(input_folder) if f.endswith(pattern_suffix)]\n",
    "    if not processed_files:\n",
    "        print(f\"⚠️ No files matching *{pattern_suffix} in {input_folder}\")\n",
    "        return\n",
    "\n",
    "    for file in processed_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Check for columns Pattern0 ~ Pattern7 (8 columns)\n",
    "            pattern_cols = [f'Pattern{i}' for i in range(8)]\n",
    "            if not all(col in df.columns for col in pattern_cols):\n",
    "                print(f\"⚠️ Skipping {file} (missing Pattern columns)\")\n",
    "                continue\n",
    "\n",
    "            # Summarize counts for H/L per position\n",
    "            summary = {\n",
    "                val: [df[col].value_counts().get(val, 0) for col in pattern_cols]\n",
    "                for val in ['H', 'L']\n",
    "            }\n",
    "\n",
    "            df_summary = pd.DataFrame(summary, index=pattern_cols).T\n",
    "            df_summary.index.name = 'Value'\n",
    "\n",
    "            base_name = file.replace(pattern_suffix, \"\")\n",
    "\n",
    "            # 1) Save CSV (absolute counts)\n",
    "            csv_out = os.path.join(csv_dir, f\"{base_name}_pattern_count.csv\")\n",
    "            df_summary.to_csv(csv_out, index=True)\n",
    "\n",
    "            # 2) Plot absolute counts (bar chart)\n",
    "            ax = df_summary.T.plot(kind=\"bar\", figsize=(10, 6),\n",
    "                                   title=file, ylabel=\"Count\")\n",
    "            plt.xticks(rotation=0)\n",
    "            plt.tight_layout()\n",
    "            png_out = os.path.join(png_dir, f\"{base_name}_pattern_count.png\")\n",
    "            plt.savefig(png_out)\n",
    "            plt.close()\n",
    "\n",
    "            # 3) Plot normalized proportions (stacked bar)\n",
    "            df_norm = df_summary.T.div(df_summary.T.sum(axis=1), axis=0).fillna(0)\n",
    "            ax = df_norm.plot(kind=\"bar\", stacked=True, figsize=(10, 6),\n",
    "                              title=f\"{file} (Normalized)\", ylabel=\"Proportion\", ylim=(0, 1))\n",
    "            plt.xticks(rotation=0)\n",
    "            plt.tight_layout()\n",
    "            png_norm_out = os.path.join(png_norm_dir, f\"{base_name}_pattern_count_normalized.png\")\n",
    "            plt.savefig(png_norm_out)\n",
    "            plt.close()\n",
    "\n",
    "            print(f\"✅ {file} -> CSV:{os.path.basename(csv_out)}, PNG:{os.path.basename(png_out)}, PNG(Norm):{os.path.basename(png_norm_out)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to process {file}: {e}\")\n",
    "\n",
    "# ===== Example usage =====\n",
    "summarize_and_plot_all_processed_files(\n",
    "    input_folder=\"fastq/4_point_histogram\",\n",
    "    output_folder=\"fastq/4_point_histogram/point_summary\",\n",
    "    pattern_suffix=\"_pattern_processed.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97451e1e",
   "metadata": {},
   "source": [
    "## B.1. total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2951d37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved cleaned RNAME histogram: fastq/4_align_histogram/histogram_Temp_60_12h_.csv\n",
      "✅ Saved cleaned RNAME histogram: fastq/4_align_histogram/histogram_Temp_50_12h_.csv\n",
      "✅ Saved cleaned RNAME histogram: fastq/4_align_histogram/histogram_Temp_40_12h_.csv\n",
      "✅ Saved cleaned RNAME histogram: fastq/4_align_histogram/histogram_Temp_20_12h_.csv\n",
      "✅ Saved cleaned RNAME histogram: fastq/4_align_histogram/histogram_Temp_30_12h_.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 📁 Folder paths\n",
    "input_folder = \"fastq/3_align_csv/MAPQ_removed\"\n",
    "histogram_folder = \"fastq/4_align_histogram\"\n",
    "os.makedirs(histogram_folder, exist_ok=True)\n",
    "\n",
    "# 📄 Process all CSV files in the folder\n",
    "files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(input_folder, file_name)\n",
    "\n",
    "    # 🔧 Clean file name (remove unnecessary parts)\n",
    "    clean_name = file_name\n",
    "    clean_name = clean_name.replace(\"assemble\", \"\")\n",
    "    clean_name = clean_name.replace(\"ID_match_FLASH.extendedFrags\", \"\")\n",
    "    clean_name = clean_name.replace(\"__\", \"_\").strip(\"_\")  # remove duplicate/ending underscores\n",
    "    output_csv = os.path.join(histogram_folder, f\"histogram_{clean_name}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, dtype=str)\n",
    "        if 'RNAME' not in df.columns:\n",
    "            print(f\"⚠️ Skipping file: {file_name} (no 'RNAME' column found)\")\n",
    "            continue\n",
    "\n",
    "        # Count RNAME occurrences and normalize\n",
    "        rname_counts = df['RNAME'].value_counts().reset_index()\n",
    "        rname_counts.columns = ['RNAME', 'Count']\n",
    "        rname_counts.insert(0, 'File_Name', clean_name)\n",
    "        rname_counts['Count'] = rname_counts['Count'].astype(int)\n",
    "        total_count = rname_counts['Count'].sum()\n",
    "        rname_counts['Normalized_Count'] = rname_counts['Count'] / total_count\n",
    "\n",
    "        rname_counts.to_csv(output_csv, index=False)\n",
    "        print(f\"✅ Saved cleaned RNAME histogram: {output_csv}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing file '{file_name}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5626e25b",
   "metadata": {},
   "source": [
    "## B.2. total L H count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a645d0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed histogram_Temp_20_12h_.csv → L: 1294, H: 66\n",
      "✅ Processed histogram_Temp_30_12h_.csv → L: 416, H: 216\n",
      "✅ Processed histogram_Temp_40_12h_.csv → L: 555, H: 965\n",
      "✅ Processed histogram_Temp_50_12h_.csv → L: 130, H: 1230\n",
      "✅ Processed histogram_Temp_60_12h_.csv → L: 19, H: 453\n",
      "\n",
      "📄 Summary saved to: fastq/4_align_histogram/summary_LH_counts.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "input_folder = \"fastq/4_align_histogram\"\n",
    "output_path = os.path.join(input_folder, \"summary_LH_counts.csv\")\n",
    "\n",
    "summary = []\n",
    "\n",
    "# Select only target files for analysis\n",
    "files = [f for f in os.listdir(input_folder) if f.endswith('.csv') and f.startswith(\"histogram_\")]\n",
    "\n",
    "for file_name in sorted(files):\n",
    "    file_path = os.path.join(input_folder, file_name)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        total_L = 0\n",
    "        total_H = 0\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            seq_id = row.get('RNAME') or row.get('Sequence_ID')\n",
    "            count = int(row['Count'])\n",
    "\n",
    "            # Extract L/H label (e.g., seq_XC_LHLLLLL → LHLLLLL)\n",
    "            match = re.search(r'seq_[A-Z]+_([LH]+)', seq_id)\n",
    "            if not match:\n",
    "                continue\n",
    "            label = match.group(1)\n",
    "\n",
    "            l_count = label.count('L') * count\n",
    "            h_count = label.count('H') * count\n",
    "\n",
    "            total_L += l_count\n",
    "            total_H += h_count\n",
    "\n",
    "        summary.append({\n",
    "            \"File_Name\": file_name,\n",
    "            \"Total_L\": total_L,\n",
    "            \"Total_H\": total_H\n",
    "        })\n",
    "        print(f\"✅ Processed {file_name} → L: {total_L}, H: {total_H}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file_name}: {e}\")\n",
    "\n",
    "# Save results\n",
    "summary_df = pd.DataFrame(summary)\n",
    "summary_df.to_csv(output_path, index=False)\n",
    "print(f\"\\n📄 Summary saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333562d4",
   "metadata": {},
   "source": [
    "## B.3. total L H count plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ff3104f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Grouped bar plot saved to: fastq/4_align_histogram/summary_LH_grouped_barplot.png\n",
      "📊 Stacked bar plot saved to: fastq/4_align_histogram/summary_LH_stacked_barplot.png\n",
      "📊 Normalized stacked bar plot saved to: fastq/4_align_histogram/summary_LH_normalized_stacked_barplot.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "input_path = \"fastq/4_align_histogram/summary_LH_counts.csv\"\n",
    "output_dir = \"fastq/4_align_histogram\"\n",
    "output_grouped = os.path.join(output_dir, \"summary_LH_grouped_barplot.png\")\n",
    "output_stacked = os.path.join(output_dir, \"summary_LH_stacked_barplot.png\")\n",
    "output_normalized = os.path.join(output_dir, \"summary_LH_normalized_stacked_barplot.png\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Clean label\n",
    "df[\"Label\"] = df[\"File_Name\"].str.replace(\"histogram_\", \"\").str.replace(\".csv\", \"\")\n",
    "df = df.sort_values(\"Label\").reset_index(drop=True)  # Sort by label\n",
    "\n",
    "# X-axis settings\n",
    "x = range(len(df))\n",
    "labels = df[\"Label\"]\n",
    "bar_width = 0.4\n",
    "\n",
    "# ----------------------------------\n",
    "# 1. Grouped Bar Chart\n",
    "# ----------------------------------\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar([i - bar_width/2 for i in x], df[\"Total_L\"], width=bar_width, label=\"Total L\", color='skyblue')\n",
    "plt.bar([i + bar_width/2 for i in x], df[\"Total_H\"], width=bar_width, label=\"Total H\", color='salmon')\n",
    "plt.xticks(ticks=x, labels=labels, rotation=45, ha='right')\n",
    "plt.xlabel(\"File\")\n",
    "plt.ylabel(\"Base Count\")\n",
    "plt.title(\"Total L and H Base Counts per File (Grouped Bar Chart)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_grouped)\n",
    "plt.close()\n",
    "print(f\"📊 Grouped bar plot saved to: {output_grouped}\")\n",
    "\n",
    "# ----------------------------------\n",
    "# 2. Stacked Bar Chart (Raw Counts)\n",
    "# ----------------------------------\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(x, df[\"Total_L\"], label=\"Total L\", color='skyblue')\n",
    "plt.bar(x, df[\"Total_H\"], bottom=df[\"Total_L\"], label=\"Total H\", color='salmon')\n",
    "plt.xticks(ticks=x, labels=labels, rotation=45, ha='right')\n",
    "plt.xlabel(\"File\")\n",
    "plt.ylabel(\"Base Count\")\n",
    "plt.title(\"Total L and H Base Counts per File (Stacked Bar Chart)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_stacked)\n",
    "plt.close()\n",
    "print(f\"📊 Stacked bar plot saved to: {output_stacked}\")\n",
    "\n",
    "# ----------------------------------\n",
    "# 3. Normalized Stacked Bar Chart (Ratio 0~1)\n",
    "# ----------------------------------\n",
    "# Compute ratios\n",
    "df[\"Total\"] = df[\"Total_L\"] + df[\"Total_H\"]\n",
    "df[\"L_ratio\"] = df[\"Total_L\"] / df[\"Total\"]\n",
    "df[\"H_ratio\"] = df[\"Total_H\"] / df[\"Total\"]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(x, df[\"L_ratio\"], label=\"L (Ratio)\", color='skyblue')\n",
    "plt.bar(x, df[\"H_ratio\"], bottom=df[\"L_ratio\"], label=\"H (Ratio)\", color='salmon')\n",
    "plt.xticks(ticks=x, labels=labels, rotation=45, ha='right')\n",
    "plt.xlabel(\"File\")\n",
    "plt.ylabel(\"Ratio (0–1)\")\n",
    "plt.title(\"Normalized L and H Base Ratio per File (Stacked Bar Chart)\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_normalized)\n",
    "plt.close()\n",
    "print(f\"📊 Normalized stacked bar plot saved to: {output_normalized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddfad87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e758dc35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
