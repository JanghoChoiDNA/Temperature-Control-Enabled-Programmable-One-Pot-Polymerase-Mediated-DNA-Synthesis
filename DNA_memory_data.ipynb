{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "926b74f3",
   "metadata": {},
   "source": [
    "# 1. Install Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5867155f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://packages.microsoft.com/repos/code stable InRelease\n",
      "Hit:2 http://ports.ubuntu.com/ubuntu-ports noble InRelease                     \n",
      "Hit:3 http://ports.ubuntu.com/ubuntu-ports noble-updates InRelease             \n",
      "Hit:4 http://ports.ubuntu.com/ubuntu-ports noble-backports InRelease\n",
      "Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu noble InRelease\n",
      "Hit:6 http://ports.ubuntu.com/ubuntu-ports noble-security InRelease\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "fastp is already the newest version (0.23.4+dfsg-1).\n",
      "flash is already the newest version (1.2.11-2).\n",
      "bwa is already the newest version (0.7.17-7).\n",
      "samtools is already the newest version (1.19.2-1build2).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  pigz python3-xopen\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 275 not upgraded.\n",
      "Requirement already satisfied: biopython in /usr/local/lib/python3.12/dist-packages (1.84)\n",
      "Requirement already satisfied: cutadapt in /usr/local/lib/python3.12/dist-packages (5.0)\n",
      "Requirement already satisfied: pysam in /usr/local/lib/python3.12/dist-packages (0.22.1)\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (from biopython) (1.26.4)\n",
      "Requirement already satisfied: dnaio>=1.2.3 in /usr/local/lib/python3.12/dist-packages (from cutadapt) (1.2.3)\n",
      "Requirement already satisfied: xopen>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from cutadapt) (2.0.2)\n",
      "Requirement already satisfied: isal>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from xopen>=1.6.0->cutadapt) (1.7.1)\n",
      "Requirement already satisfied: zlib-ng>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from xopen>=1.6.0->cutadapt) (0.5.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Bioinformatics Tools (Ubuntu)\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y fastp flash bwa samtools\n",
    "\n",
    "# Python Library\n",
    "!pip3 install biopython cutadapt pysam --break-system-packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0d9529",
   "metadata": {},
   "source": [
    "# 2 Trimming and Discard trimmed sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40e1b17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrimmed sequences saved: sequence_merge_method/A_untrimmed_output/DNA_Data_3SP26_R1_untrimmed.fastq.gz, sequence_merge_method/A_untrimmed_output/DNA_Data_3SP26_R2_untrimmed.fastq.gz\n",
      "Untrimmed sequences saved: sequence_merge_method/A_untrimmed_output/DNA_Data_1D_R1_untrimmed.fastq.gz, sequence_merge_method/A_untrimmed_output/DNA_Data_1D_R2_untrimmed.fastq.gz\n",
      "Untrimmed sequences saved: sequence_merge_method/A_untrimmed_output/DNA_Data_0N_R1_untrimmed.fastq.gz, sequence_merge_method/A_untrimmed_output/DNA_Data_0N_R2_untrimmed.fastq.gz\n",
      "Untrimmed sequences saved: sequence_merge_method/A_untrimmed_output/DNA_Data_5I_R1_untrimmed.fastq.gz, sequence_merge_method/A_untrimmed_output/DNA_Data_5I_R2_untrimmed.fastq.gz\n",
      "Untrimmed sequences saved: sequence_merge_method/A_untrimmed_output/DNA_Data_2S_R1_untrimmed.fastq.gz, sequence_merge_method/A_untrimmed_output/DNA_Data_2S_R2_untrimmed.fastq.gz\n",
      "Untrimmed sequences saved: sequence_merge_method/A_untrimmed_output/DNA_Data_7T_R1_untrimmed.fastq.gz, sequence_merge_method/A_untrimmed_output/DNA_Data_7T_R2_untrimmed.fastq.gz\n",
      "Untrimmed sequences saved: sequence_merge_method/A_untrimmed_output/DNA_Data_4G_R1_untrimmed.fastq.gz, sequence_merge_method/A_untrimmed_output/DNA_Data_4G_R2_untrimmed.fastq.gz\n",
      "Untrimmed sequences saved: sequence_merge_method/A_untrimmed_output/DNA_Data_6S_R1_untrimmed.fastq.gz, sequence_merge_method/A_untrimmed_output/DNA_Data_6S_R2_untrimmed.fastq.gz\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Specify the folder containing your input files.\n",
    "# Specify the folder where you want to save the untrimmed (adapter-free) sequences.\n",
    "input_folder = \"fastq\"\n",
    "untrimmed_output_folder = \"sequence_merge_method/A_untrimmed_output\"\n",
    "\n",
    "# Define the adapter sequences for R1 and R2.\n",
    "adapter_sequence_r1 = \"AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC\"\n",
    "adapter_sequence_r2 = \"AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT\"\n",
    "\n",
    "# Use glob to get a list of all input file pairs (R1 and R2) in the folder.\n",
    "input_file_pairs = []\n",
    "for input_r1 in glob.glob(os.path.join(input_folder, \"*_R1.fastq.gz\")):\n",
    "    # Assuming R2 files have the same naming format as R1 files.\n",
    "    input_r2 = input_r1.replace(\"_R1.fastq.gz\", \"_R2.fastq.gz\")\n",
    "    if os.path.exists(input_r2):  # Ensure R2 file exists.\n",
    "        input_file_pairs.append({\"r1\": input_r1, \"r2\": input_r2})\n",
    "\n",
    "# Create the output folder if it doesn't exist.\n",
    "os.makedirs(untrimmed_output_folder, exist_ok=True)\n",
    "\n",
    "for input_files in input_file_pairs:\n",
    "    input_r1 = input_files[\"r1\"]\n",
    "    input_r2 = input_files[\"r2\"]\n",
    "\n",
    "    # Define output file paths for untrimmed (clean, adapter-free) sequences.\n",
    "    untrimmed_r1 = os.path.join(untrimmed_output_folder, os.path.basename(input_r1).replace(\".fastq.gz\", \"_untrimmed.fastq.gz\"))\n",
    "    untrimmed_r2 = os.path.join(untrimmed_output_folder, os.path.basename(input_r2).replace(\".fastq.gz\", \"_untrimmed.fastq.gz\"))\n",
    "\n",
    "    # Use cutadapt to keep only untrimmed sequences (completely adapter-free).\n",
    "    result = subprocess.run([\n",
    "        \"cutadapt\",\n",
    "        \"-a\", adapter_sequence_r1,  # Adapter for R1\n",
    "        \"-A\", adapter_sequence_r2,  # Adapter for R2\n",
    "        \"-O\", \"15\",                  # Minimum overlap for adapter trimming\n",
    "        \"--discard-trimmed\",         # Discard sequences where trimming occurred\n",
    "        \"-o\", untrimmed_r1,          # Save only untrimmed R1 reads\n",
    "        \"-p\", untrimmed_r2,          # Save only untrimmed R2 reads\n",
    "        input_r1, input_r2\n",
    "    ], capture_output=True, text=True)\n",
    "\n",
    "    # Log the result.\n",
    "    if result.returncode == 0:\n",
    "        print(f\"Untrimmed sequences saved: {untrimmed_r1}, {untrimmed_r2}\")\n",
    "    else:\n",
    "        print(f\"Error processing {input_r1} and {input_r2}:\\n{result.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7b17fe",
   "metadata": {},
   "source": [
    "# 3. Q filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31b123bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n",
      "Read1 before filtering:\n",
      "total reads: 867\n",
      "total bases: 130917\n",
      "Q20 bases: 110998(84.785%)\n",
      "Q30 bases: 96576(73.7689%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 213\n",
      "total bases: 32163\n",
      "Q20 bases: 30934(96.1788%)\n",
      "Q30 bases: 28893(89.833%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 213\n",
      "reads failed due to low quality: 654\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 12.9181%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/DNA_Data_4G_R1_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/DNA_Data_4G_R1_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/DNA_Data_4G_R1_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/DNA_Data_4G_R1_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/DNA_Data_4G_R1_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/DNA_Data_4G_R1_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 1 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for DNA_Data_4G_R1_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/DNA_Data_4G_R1_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/DNA_Data_4G_R1_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/DNA_Data_4G_R1_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 774\n",
      "total bases: 116874\n",
      "Q20 bases: 99962(85.5297%)\n",
      "Q30 bases: 87120(74.5418%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 212\n",
      "total bases: 32012\n",
      "Q20 bases: 31048(96.9886%)\n",
      "Q30 bases: 29026(90.6722%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 212\n",
      "reads failed due to low quality: 562\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 14.9871%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/DNA_Data_5I_R1_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/DNA_Data_5I_R1_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/DNA_Data_5I_R1_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/DNA_Data_5I_R1_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/DNA_Data_5I_R1_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/DNA_Data_5I_R1_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for DNA_Data_5I_R1_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/DNA_Data_5I_R1_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/DNA_Data_5I_R1_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/DNA_Data_5I_R1_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 771\n",
      "total bases: 116421\n",
      "Q20 bases: 99711(85.6469%)\n",
      "Q30 bases: 86468(74.2718%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 214\n",
      "total bases: 32314\n",
      "Q20 bases: 31248(96.7011%)\n",
      "Q30 bases: 29065(89.9455%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 214\n",
      "reads failed due to low quality: 557\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 14.5266%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/DNA_Data_7T_R1_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/DNA_Data_7T_R1_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/DNA_Data_7T_R1_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/DNA_Data_7T_R1_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/DNA_Data_7T_R1_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/DNA_Data_7T_R1_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for DNA_Data_7T_R1_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/DNA_Data_7T_R1_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/DNA_Data_7T_R1_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/DNA_Data_7T_R1_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 774\n",
      "total bases: 116874\n",
      "Q20 bases: 100577(86.0559%)\n",
      "Q30 bases: 86450(73.9685%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 216\n",
      "total bases: 32616\n",
      "Q20 bases: 31631(96.98%)\n",
      "Q30 bases: 29247(89.6707%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 216\n",
      "reads failed due to low quality: 558\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 14.4703%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/DNA_Data_5I_R2_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/DNA_Data_5I_R2_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/DNA_Data_5I_R2_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/DNA_Data_5I_R2_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/DNA_Data_5I_R2_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/DNA_Data_5I_R2_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 1 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for DNA_Data_5I_R2_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/DNA_Data_5I_R2_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/DNA_Data_5I_R2_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/DNA_Data_5I_R2_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 771\n",
      "total bases: 116421\n",
      "Q20 bases: 99857(85.7723%)\n",
      "Q30 bases: 85933(73.8123%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 230\n",
      "total bases: 34730\n",
      "Q20 bases: 33795(97.3078%)\n",
      "Q30 bases: 31170(89.7495%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 230\n",
      "reads failed due to low quality: 541\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 18.9364%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/DNA_Data_7T_R2_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/DNA_Data_7T_R2_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/DNA_Data_7T_R2_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/DNA_Data_7T_R2_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/DNA_Data_7T_R2_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/DNA_Data_7T_R2_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for DNA_Data_7T_R2_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/DNA_Data_7T_R2_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/DNA_Data_7T_R2_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/DNA_Data_7T_R2_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 867\n",
      "total bases: 130917\n",
      "Q20 bases: 113222(86.4838%)\n",
      "Q30 bases: 97238(74.2745%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 250\n",
      "total bases: 37750\n",
      "Q20 bases: 36731(97.3007%)\n",
      "Q30 bases: 33973(89.9947%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 250\n",
      "reads failed due to low quality: 617\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 20.7612%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/DNA_Data_4G_R2_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/DNA_Data_4G_R2_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/DNA_Data_4G_R2_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/DNA_Data_4G_R2_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/DNA_Data_4G_R2_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/DNA_Data_4G_R2_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for DNA_Data_4G_R2_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/DNA_Data_4G_R2_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/DNA_Data_4G_R2_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/DNA_Data_4G_R2_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 4144\n",
      "total bases: 625744\n",
      "Q20 bases: 567365(90.6705%)\n",
      "Q30 bases: 523150(83.6045%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 2527\n",
      "total bases: 381577\n",
      "Q20 bases: 374028(98.0216%)\n",
      "Q30 bases: 359194(94.1341%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 2527\n",
      "reads failed due to low quality: 1617\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 41.4817%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/DNA_Data_3SP26_R2_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/DNA_Data_3SP26_R2_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/DNA_Data_3SP26_R2_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/DNA_Data_3SP26_R2_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/DNA_Data_3SP26_R2_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/DNA_Data_3SP26_R2_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for DNA_Data_3SP26_R2_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/DNA_Data_3SP26_R2_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/DNA_Data_3SP26_R2_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/DNA_Data_3SP26_R2_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 804\n",
      "total bases: 121404\n",
      "Q20 bases: 105164(86.6232%)\n",
      "Q30 bases: 90694(74.7043%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 231\n",
      "total bases: 34881\n",
      "Q20 bases: 33915(97.2306%)\n",
      "Q30 bases: 31283(89.6849%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 231\n",
      "reads failed due to low quality: 573\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 21.7662%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/DNA_Data_1D_R2_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/DNA_Data_1D_R2_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/DNA_Data_1D_R2_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/DNA_Data_1D_R2_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/DNA_Data_1D_R2_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/DNA_Data_1D_R2_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 1 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for DNA_Data_1D_R2_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/DNA_Data_1D_R2_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/DNA_Data_1D_R2_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/DNA_Data_1D_R2_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 804\n",
      "total bases: 121404\n",
      "Q20 bases: 104389(85.9848%)\n",
      "Q30 bases: 91086(75.0272%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 218\n",
      "total bases: 32918\n",
      "Q20 bases: 31587(95.9566%)\n",
      "Q30 bases: 29409(89.3402%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 218\n",
      "reads failed due to low quality: 586\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 13.3085%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/DNA_Data_1D_R1_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/DNA_Data_1D_R1_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/DNA_Data_1D_R1_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/DNA_Data_1D_R1_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/DNA_Data_1D_R1_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/DNA_Data_1D_R1_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for DNA_Data_1D_R1_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/DNA_Data_1D_R1_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/DNA_Data_1D_R1_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/DNA_Data_1D_R1_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 4144\n",
      "total bases: 625744\n",
      "Q20 bases: 569626(91.0318%)\n",
      "Q30 bases: 520618(83.1998%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 2438\n",
      "total bases: 368138\n",
      "Q20 bases: 362244(98.399%)\n",
      "Q30 bases: 348230(94.5922%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 2438\n",
      "reads failed due to low quality: 1706\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 45.6081%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/DNA_Data_3SP26_R1_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/DNA_Data_3SP26_R1_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/DNA_Data_3SP26_R1_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/DNA_Data_3SP26_R1_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/DNA_Data_3SP26_R1_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/DNA_Data_3SP26_R1_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for DNA_Data_3SP26_R1_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/DNA_Data_3SP26_R1_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/DNA_Data_3SP26_R1_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/DNA_Data_3SP26_R1_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 825\n",
      "total bases: 124575\n",
      "Q20 bases: 106281(85.3149%)\n",
      "Q30 bases: 92297(74.0895%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 227\n",
      "total bases: 34277\n",
      "Q20 bases: 33136(96.6712%)\n",
      "Q30 bases: 30917(90.1975%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 227\n",
      "reads failed due to low quality: 598\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 14.5455%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/DNA_Data_6S_R1_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/DNA_Data_6S_R1_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/DNA_Data_6S_R1_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/DNA_Data_6S_R1_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/DNA_Data_6S_R1_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/DNA_Data_6S_R1_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for DNA_Data_6S_R1_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/DNA_Data_6S_R1_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/DNA_Data_6S_R1_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/DNA_Data_6S_R1_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 808\n",
      "total bases: 122008\n",
      "Q20 bases: 104834(85.9239%)\n",
      "Q30 bases: 89977(73.7468%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 200\n",
      "total bases: 30200\n",
      "Q20 bases: 29240(96.8212%)\n",
      "Q30 bases: 26938(89.1987%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 200\n",
      "reads failed due to low quality: 608\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 16.2129%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/DNA_Data_2S_R2_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/DNA_Data_2S_R2_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/DNA_Data_2S_R2_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/DNA_Data_2S_R2_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/DNA_Data_2S_R2_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/DNA_Data_2S_R2_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 1 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for DNA_Data_2S_R2_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/DNA_Data_2S_R2_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/DNA_Data_2S_R2_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/DNA_Data_2S_R2_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 721\n",
      "total bases: 108871\n",
      "Q20 bases: 94451(86.755%)\n",
      "Q30 bases: 81732(75.0723%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 236\n",
      "total bases: 35636\n",
      "Q20 bases: 34651(97.2359%)\n",
      "Q30 bases: 32101(90.0803%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 236\n",
      "reads failed due to low quality: 485\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 23.8558%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/DNA_Data_0N_R2_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/DNA_Data_0N_R2_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/DNA_Data_0N_R2_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/DNA_Data_0N_R2_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/DNA_Data_0N_R2_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/DNA_Data_0N_R2_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for DNA_Data_0N_R2_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/DNA_Data_0N_R2_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/DNA_Data_0N_R2_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/DNA_Data_0N_R2_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 808\n",
      "total bases: 122008\n",
      "Q20 bases: 104048(85.2797%)\n",
      "Q30 bases: 90596(74.2541%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 200\n",
      "total bases: 30200\n",
      "Q20 bases: 29104(96.3709%)\n",
      "Q30 bases: 27065(89.6192%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 200\n",
      "reads failed due to low quality: 608\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 9.77723%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/DNA_Data_2S_R1_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/DNA_Data_2S_R1_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/DNA_Data_2S_R1_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/DNA_Data_2S_R1_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/DNA_Data_2S_R1_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/DNA_Data_2S_R1_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for DNA_Data_2S_R1_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/DNA_Data_2S_R1_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/DNA_Data_2S_R1_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/DNA_Data_2S_R1_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 721\n",
      "total bases: 108871\n",
      "Q20 bases: 92957(85.3827%)\n",
      "Q30 bases: 81028(74.4257%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 194\n",
      "total bases: 29294\n",
      "Q20 bases: 28171(96.1665%)\n",
      "Q30 bases: 26218(89.4996%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 194\n",
      "reads failed due to low quality: 527\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 14.9792%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/DNA_Data_0N_R1_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/DNA_Data_0N_R1_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/DNA_Data_0N_R1_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/DNA_Data_0N_R1_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/DNA_Data_0N_R1_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/DNA_Data_0N_R1_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for DNA_Data_0N_R1_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/DNA_Data_0N_R1_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/DNA_Data_0N_R1_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/DNA_Data_0N_R1_Qfiltered.fastq.gz.json\n",
      "\n",
      "Filtering for DNA_Data_6S_R2_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/DNA_Data_6S_R2_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/DNA_Data_6S_R2_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/DNA_Data_6S_R2_Qfiltered.fastq.gz.json\n",
      "\n",
      "All filtering processes are done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 825\n",
      "total bases: 124575\n",
      "Q20 bases: 107887(86.6041%)\n",
      "Q30 bases: 92988(74.6442%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 228\n",
      "total bases: 34428\n",
      "Q20 bases: 33388(96.9792%)\n",
      "Q30 bases: 30766(89.3633%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 228\n",
      "reads failed due to low quality: 597\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 15.3939%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/DNA_Data_6S_R2_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/DNA_Data_6S_R2_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/DNA_Data_6S_R2_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/DNA_Data_6S_R2_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/DNA_Data_6S_R2_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/DNA_Data_6S_R2_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 1 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Quality threshold (Phred score)\n",
    "quality_threshold = 30\n",
    "\n",
    "# Set input and output folders\n",
    "input_folder = \"sequence_merge_method/A_untrimmed_output\"\n",
    "output_folder = \"sequence_merge_method/B_Qfiltered\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate through files in the input folder, processing only those ending with \"_untrimmed.fastq.gz\"\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\"_untrimmed.fastq.gz\"):\n",
    "        # Input file path\n",
    "        input_file = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Output filename (e.g., sample_untrimmed.fastq.gz -> sample_Qfiltered.fastq.gz)\n",
    "        output_file = os.path.join(\n",
    "            output_folder, \n",
    "            filename.replace(\"_untrimmed.fastq.gz\", \"_Qfiltered.fastq.gz\")\n",
    "        )\n",
    "        \n",
    "        # Execute fastp in single-end mode for each file\n",
    "        subprocess.call([\n",
    "            \"fastp\",\n",
    "            \"-i\", input_file,                      # Input file\n",
    "            \"-o\", output_file,                     # Output file\n",
    "            \"-q\", str(quality_threshold),          # Quality threshold for a base to be qualified\n",
    "            \"-u\", \"15\",                            # Discard reads if the percentage of unqualified bases is >= 15%\n",
    "            \"-l\", \"151\",                           # Minimum read length to keep\n",
    "            \"--cut_mean_quality\", \"30\",            # Discard reads if mean quality is less than 30\n",
    "            \"--html\", f\"{output_file}.html\",       # HTML report file path\n",
    "            \"--json\", f\"{output_file}.json\"        # JSON report file path\n",
    "        ])\n",
    "        \n",
    "        print(f\"Filtering for {filename} is complete.\\n\"\n",
    "              f\"Output FASTQ : {output_file}\\n\"\n",
    "              f\"Reports      : {output_file}.html / {output_file}.json\\n\")\n",
    "\n",
    "print(\"All filtering processes are done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b121a804",
   "metadata": {},
   "source": [
    "# 4. Match Paired-End Read IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cc591cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DNA_Data_1D_R1_Qfiltered.fastq.gz and DNA_Data_1D_R2_Qfiltered.fastq.gz\n",
      "Total R1 IDs: 218, Total R2 IDs: 231, Matching IDs: 146\n",
      "IDs only in R1: 72, IDs only in R2: 85\n",
      "\n",
      "Processing DNA_Data_3SP26_R1_Qfiltered.fastq.gz and DNA_Data_3SP26_R2_Qfiltered.fastq.gz\n",
      "Total R1 IDs: 2438, Total R2 IDs: 2527, Matching IDs: 2148\n",
      "IDs only in R1: 290, IDs only in R2: 379\n",
      "\n",
      "Processing DNA_Data_0N_R1_Qfiltered.fastq.gz and DNA_Data_0N_R2_Qfiltered.fastq.gz\n",
      "Total R1 IDs: 194, Total R2 IDs: 236, Matching IDs: 136\n",
      "IDs only in R1: 58, IDs only in R2: 100\n",
      "\n",
      "Processing DNA_Data_2S_R1_Qfiltered.fastq.gz and DNA_Data_2S_R2_Qfiltered.fastq.gz\n",
      "Total R1 IDs: 200, Total R2 IDs: 200, Matching IDs: 119\n",
      "IDs only in R1: 81, IDs only in R2: 81\n",
      "\n",
      "Processing DNA_Data_6S_R1_Qfiltered.fastq.gz and DNA_Data_6S_R2_Qfiltered.fastq.gz\n",
      "Total R1 IDs: 227, Total R2 IDs: 228, Matching IDs: 140\n",
      "IDs only in R1: 87, IDs only in R2: 88\n",
      "\n",
      "Processing DNA_Data_4G_R1_Qfiltered.fastq.gz and DNA_Data_4G_R2_Qfiltered.fastq.gz\n",
      "Total R1 IDs: 213, Total R2 IDs: 250, Matching IDs: 152\n",
      "IDs only in R1: 61, IDs only in R2: 98\n",
      "\n",
      "Processing DNA_Data_7T_R1_Qfiltered.fastq.gz and DNA_Data_7T_R2_Qfiltered.fastq.gz\n",
      "Total R1 IDs: 214, Total R2 IDs: 230, Matching IDs: 139\n",
      "IDs only in R1: 75, IDs only in R2: 91\n",
      "\n",
      "Processing DNA_Data_5I_R1_Qfiltered.fastq.gz and DNA_Data_5I_R2_Qfiltered.fastq.gz\n",
      "Total R1 IDs: 212, Total R2 IDs: 216, Matching IDs: 135\n",
      "IDs only in R1: 77, IDs only in R2: 81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def extract_matching_reads(r1_path, r2_path, out_r1_path, out_r2_path):\n",
    "    def get_read_id(header):\n",
    "        # Extract ID from the FASTQ header\n",
    "        return header.split()[0].replace('/1', '').replace('/2', '')\n",
    "\n",
    "    r1_ids = set()\n",
    "    r2_ids = set()\n",
    "\n",
    "    # Extract all read IDs from the R1 file\n",
    "    with gzip.open(r1_path, 'rt') as r1_file:\n",
    "        while True:\n",
    "            header = r1_file.readline()\n",
    "            if not header:\n",
    "                break\n",
    "            r1_ids.add(get_read_id(header.strip()))\n",
    "            # Skip the other 3 lines of the read (sequence, +, quality)\n",
    "            [r1_file.readline() for _ in range(3)] \n",
    "\n",
    "    # Extract all read IDs from the R2 file\n",
    "    with gzip.open(r2_path, 'rt') as r2_file:\n",
    "        while True:\n",
    "            header = r2_file.readline()\n",
    "            if not header:\n",
    "                break\n",
    "            r2_ids.add(get_read_id(header.strip()))\n",
    "            [r2_file.readline() for _ in range(3)]\n",
    "\n",
    "    # Find common and unique IDs\n",
    "    matching_ids = r1_ids & r2_ids\n",
    "    r1_only = r1_ids - r2_ids\n",
    "    r2_only = r2_ids - r1_ids\n",
    "\n",
    "    print(f\"Processing {os.path.basename(r1_path)} and {os.path.basename(r2_path)}\")\n",
    "    print(f\"Total R1 IDs: {len(r1_ids)}, Total R2 IDs: {len(r2_ids)}, Matching IDs: {len(matching_ids)}\")\n",
    "    print(f\"IDs only in R1: {len(r1_only)}, IDs only in R2: {len(r2_only)}\\n\")\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(out_r1_path), exist_ok=True)\n",
    "\n",
    "    # Function to write only the reads with matching IDs to a new file\n",
    "    def write_matching_reads(input_path, output_path, matching_ids):\n",
    "        with gzip.open(input_path, 'rt') as infile, gzip.open(output_path, 'wt') as outfile:\n",
    "            while True:\n",
    "                lines = [infile.readline() for _ in range(4)]\n",
    "                if not lines[0]:\n",
    "                    break\n",
    "                read_id = get_read_id(lines[0].strip())\n",
    "                if read_id in matching_ids:\n",
    "                    outfile.writelines(lines)\n",
    "\n",
    "    # Write the filtered R1 and R2 files\n",
    "    write_matching_reads(r1_path, out_r1_path, matching_ids)\n",
    "    write_matching_reads(r2_path, out_r2_path, matching_ids)\n",
    "\n",
    "# --------------------------\n",
    "# Apply to all file pairs\n",
    "# --------------------------\n",
    "\n",
    "input_folder = \"sequence_merge_method/B_Qfiltered\"\n",
    "output_folder = \"sequence_merge_method/C_id_matched\"\n",
    "\n",
    "# Find all R1 files\n",
    "r1_files = glob.glob(os.path.join(input_folder, \"*_R1_Qfiltered.fastq.gz\"))\n",
    "\n",
    "# For each R1, find the corresponding R2 file and run the process\n",
    "for r1_file in r1_files:\n",
    "    r2_file = r1_file.replace(\"_R1_Qfiltered.fastq.gz\", \"_R2_Qfiltered.fastq.gz\")\n",
    "    \n",
    "    if os.path.exists(r2_file):\n",
    "        # Set the output file paths\n",
    "        base_name = os.path.basename(r1_file).replace(\"_R1_Qfiltered.fastq.gz\", \"\")\n",
    "        out_r1 = os.path.join(output_folder, f\"{base_name}_ID_match_R1.fastq.gz\")\n",
    "        out_r2 = os.path.join(output_folder, f\"{base_name}_ID_match_R2.fastq.gz\")\n",
    "        \n",
    "        # Execute the function\n",
    "        extract_matching_reads(r1_file, r2_file, out_r1, out_r2)\n",
    "    else:\n",
    "        print(f\"Warning: Corresponding R2 file not found for {r1_file}. Skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e00e67",
   "metadata": {},
   "source": [
    "# 5 Merge W/ Flash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09a7b98",
   "metadata": {},
   "source": [
    "## 5.1 R1(Front, Back), R2(Front, Back) Fragmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b24827b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split complete for: DNA_Data_2S → sequence_merge_method/D_split_reads (N=126)\n",
      "✅ Split complete for: DNA_Data_4G → sequence_merge_method/D_split_reads (N=126)\n",
      "✅ Split complete for: DNA_Data_6S → sequence_merge_method/D_split_reads (N=124)\n",
      "✅ Split complete for: DNA_Data_1D → sequence_merge_method/D_split_reads (N=126)\n",
      "✅ Split complete for: DNA_Data_7T → sequence_merge_method/D_split_reads (N=120)\n",
      "✅ Split complete for: DNA_Data_3SP26 → sequence_merge_method/D_split_reads (N=122)\n",
      "✅ Split complete for: DNA_Data_5I → sequence_merge_method/D_split_reads (N=126)\n",
      "✅ Split complete for: DNA_Data_0N → sequence_merge_method/D_split_reads (N=126)\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def split_fastq_by_position(r1_path, r2_path, n, output_dir):\n",
    "    \"\"\"Splits each read in R1 and R2 files into front and back parts.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    sample_base = os.path.basename(r1_path).replace(\"_ID_match_R1.fastq.gz\", \"\")\n",
    "    r1_f_path = os.path.join(output_dir, f\"{sample_base}_R1_F.fastq.gz\")\n",
    "    r1_b_path = os.path.join(output_dir, f\"{sample_base}_R1_B.fastq.gz\")\n",
    "    r2_f_path = os.path.join(output_dir, f\"{sample_base}_R2_F.fastq.gz\")\n",
    "    r2_b_path = os.path.join(output_dir, f\"{sample_base}_R2_B.fastq.gz\")\n",
    "\n",
    "    with gzip.open(r1_path, 'rt') as r1_file, \\\n",
    "         gzip.open(r2_path, 'rt') as r2_file, \\\n",
    "         gzip.open(r1_f_path, 'wt') as r1_f_out, \\\n",
    "         gzip.open(r1_b_path, 'wt') as r1_b_out, \\\n",
    "         gzip.open(r2_f_path, 'wt') as r2_f_out, \\\n",
    "         gzip.open(r2_b_path, 'wt') as r2_b_out:\n",
    "\n",
    "        while True:\n",
    "            r1_lines = [r1_file.readline() for _ in range(4)]\n",
    "            r2_lines = [r2_file.readline() for _ in range(4)]\n",
    "\n",
    "            if not r1_lines[0] or not r2_lines[0]:\n",
    "                break\n",
    "\n",
    "            header1, seq1, plus1, qual1 = [line.strip() for line in r1_lines]\n",
    "            header2, seq2, plus2, qual2 = [line.strip() for line in r2_lines]\n",
    "\n",
    "            # Split R1 read\n",
    "            r1_f_out.write(f\"{header1}\\n{seq1[:151-n]}\\n{plus1}\\n{qual1[:151-n]}\\n\")\n",
    "            r1_b_out.write(f\"{header1}\\n{seq1[-n:]}\\n{plus1}\\n{qual1[-n:]}\\n\")\n",
    "            # Split R2 read\n",
    "            r2_f_out.write(f\"{header2}\\n{seq2[:151-n]}\\n{plus2}\\n{qual2[:151-n]}\\n\")\n",
    "            r2_b_out.write(f\"{header2}\\n{seq2[-n:]}\\n{plus2}\\n{qual2[-n:]}\\n\")\n",
    "\n",
    "    print(f\"✅ Split complete for: {sample_base} → {output_dir} (N={n})\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Apply the split function to all files\n",
    "# -----------------------------------\n",
    "\n",
    "input_folder = \"sequence_merge_method/C_id_matched\"\n",
    "output_folder = \"sequence_merge_method/D_split_reads\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define the N-value (length of the back part) for each sample prefix\n",
    "sample_n_mapping = {\n",
    "    \"0N\": 126,\n",
    "    \"1D\": 126,\n",
    "    \"2S\": 126,\n",
    "    \"3G\": 124,\n",
    "    \"4I\": 128,\n",
    "    \"5S\": 124,\n",
    "    \"6T\": 122,\n",
    "    \"5K\": 124,\n",
    "    \"1X8\": 116,\n",
    "    \"0X8\": 132,\n",
    "    \"4G\": 126,\n",
    "    \"5I\": 126,\n",
    "    \"6S\": 124,\n",
    "    \"7T\": 120,\n",
    "    \"3SP26\": 122,\n",
    "    \"3SP31\": 118   \n",
    "}\n",
    "\n",
    "\n",
    "# Find all R1 files\n",
    "r1_files = glob.glob(os.path.join(input_folder, \"*_ID_match_R1.fastq.gz\"))\n",
    "\n",
    "for r1_file in r1_files:\n",
    "    r2_file = r1_file.replace(\"_R1.fastq.gz\", \"_R2.fastq.gz\")\n",
    "\n",
    "    if not os.path.exists(r2_file):\n",
    "        print(f\"⚠️ Matching R2 file not found: {r2_file}\")\n",
    "        continue\n",
    "\n",
    "    # Find the corresponding N value based on the filename prefix\n",
    "    matched_n = None\n",
    "    for prefix, n_value in sample_n_mapping.items():\n",
    "        if prefix in os.path.basename(r1_file):\n",
    "            matched_n = n_value\n",
    "            break\n",
    "\n",
    "    if matched_n is None:\n",
    "        print(f\"⚠️ Could not find N value for: {r1_file} → Skipping\")\n",
    "        continue\n",
    "\n",
    "    # Execute the split function\n",
    "    split_fastq_by_position(r1_file, r2_file, matched_n, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59cd6b2",
   "metadata": {},
   "source": [
    "## 5.2 R2 DNA reverse complementary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5fc929e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Reverse complemented: DNA_Data_6S_R2_F_revcomp.fastq.gz\n",
      "✅ Reverse complemented: DNA_Data_7T_R2_F_revcomp.fastq.gz\n",
      "✅ Reverse complemented: DNA_Data_1D_R2_B_revcomp.fastq.gz\n",
      "✅ Reverse complemented: DNA_Data_5I_R2_B_revcomp.fastq.gz\n",
      "✅ Reverse complemented: DNA_Data_3SP26_R2_F_revcomp.fastq.gz\n",
      "✅ Reverse complemented: DNA_Data_0N_R2_B_revcomp.fastq.gz\n",
      "✅ Reverse complemented: DNA_Data_4G_R2_B_revcomp.fastq.gz\n",
      "✅ Reverse complemented: DNA_Data_2S_R2_F_revcomp.fastq.gz\n",
      "✅ Reverse complemented: DNA_Data_4G_R2_F_revcomp.fastq.gz\n",
      "✅ Reverse complemented: DNA_Data_2S_R2_B_revcomp.fastq.gz\n",
      "✅ Reverse complemented: DNA_Data_0N_R2_F_revcomp.fastq.gz\n",
      "✅ Reverse complemented: DNA_Data_3SP26_R2_B_revcomp.fastq.gz\n",
      "✅ Reverse complemented: DNA_Data_5I_R2_F_revcomp.fastq.gz\n",
      "✅ Reverse complemented: DNA_Data_7T_R2_B_revcomp.fastq.gz\n",
      "✅ Reverse complemented: DNA_Data_6S_R2_B_revcomp.fastq.gz\n",
      "✅ Reverse complemented: DNA_Data_1D_R2_F_revcomp.fastq.gz\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import glob\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "def reverse_complement_fastq(input_fastq_path, output_fastq_path):\n",
    "    # Reads a FASTQ file, creates the reverse complement of each record, and writes it to a new file.\n",
    "    with gzip.open(input_fastq_path, \"rt\") as infile, gzip.open(output_fastq_path, \"wt\") as outfile:\n",
    "        for record in SeqIO.parse(infile, \"fastq\"):\n",
    "            # Create the reverse complement record, preserving the ID and description\n",
    "            rev_comp_record = record.reverse_complement(id=True, description=True)\n",
    "            SeqIO.write(rev_comp_record, outfile, \"fastq\")\n",
    "            \n",
    "    print(f\"✅ Reverse complemented: {os.path.basename(output_fastq_path)}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Perform reverse complement on all relevant R2 files\n",
    "# --------------------------------------------------\n",
    "\n",
    "input_folder = \"sequence_merge_method/D_split_reads\"\n",
    "os.makedirs(input_folder, exist_ok=True)\n",
    "\n",
    "# Find only the R2 front (F) and back (B) fragment files\n",
    "input_files = glob.glob(os.path.join(input_folder, \"*_R2_[BF].fastq.gz\"))\n",
    "\n",
    "for input_path in input_files:\n",
    "    base = os.path.basename(input_path)\n",
    "    # Remove the .fastq.gz extension to create a new filename\n",
    "    name_without_ext = base.replace(\".fastq.gz\", \"\")\n",
    "    output_path = os.path.join(input_folder, f\"{name_without_ext}_revcomp.fastq.gz\")\n",
    "    \n",
    "    reverse_complement_fastq(input_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc11851f",
   "metadata": {},
   "source": [
    "## 5.3 [R1_back]-[R2_back] merge (FLASH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f9a5c1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Found 8 R1_B files.\n",
      "🔵 Running FLASH for sample: DNA_Data_3SP26 (N=122)\n",
      "[FLASH] Starting FLASH v1.2.11\n",
      "[FLASH] Fast Length Adjustment of SHort reads\n",
      "[FLASH]  \n",
      "[FLASH] Input files:\n",
      "[FLASH]     sequence_merge_method/D_split_reads/DNA_Data_3SP26_R1_B.fastq.gz\n",
      "[FLASH]     sequence_merge_method/D_split_reads/DNA_Data_3SP26_R2_B.fastq.gz\n",
      "[FLASH]  \n",
      "[FLASH] Output files:\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_3SP26_FLASH.extendedFrags.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_3SP26_FLASH.notCombined_1.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_3SP26_FLASH.notCombined_2.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_3SP26_FLASH.hist\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_3SP26_FLASH.histogram\n",
      "[FLASH]  \n",
      "[FLASH] Parameters:\n",
      "[FLASH]     Min overlap:           122\n",
      "[FLASH]     Max overlap:           122\n",
      "[FLASH]     Max mismatch density:  0.250000\n",
      "[FLASH]     Allow \"outie\" pairs:   false\n",
      "[FLASH]     Cap mismatch quals:    false\n",
      "[FLASH]     Combiner threads:      4\n",
      "[FLASH]     Input format:          FASTQ, phred_offset=33\n",
      "[FLASH]     Output format:         FASTQ, phred_offset=33\n",
      "[FLASH]  \n",
      "[FLASH] Starting reader and writer threads\n",
      "[FLASH] Starting 4 combiner threads\n",
      "[FLASH] Processed 2148 read pairs\n",
      "[FLASH]  \n",
      "[FLASH] Read combination statistics:\n",
      "[FLASH]     Total pairs:      2148\n",
      "[FLASH]     Combined pairs:   420\n",
      "[FLASH]     Uncombined pairs: 1728\n",
      "[FLASH]     Percent combined: 19.55%\n",
      "[FLASH]  \n",
      "[FLASH] Writing histogram files.\n",
      "[FLASH]  \n",
      "[FLASH] FLASH v1.2.11 complete!\n",
      "[FLASH] 0.038 seconds elapsed\n",
      "✅ FLASH merging complete → sequence_merge_method/E_merged_output/DNA_Data_3SP26_FLASH.extendedFrags.fastq\n",
      "🔵 Running FLASH for sample: DNA_Data_7T (N=120)\n",
      "[FLASH] Starting FLASH v1.2.11\n",
      "[FLASH] Fast Length Adjustment of SHort reads\n",
      "[FLASH]  \n",
      "[FLASH] Input files:\n",
      "[FLASH]     sequence_merge_method/D_split_reads/DNA_Data_7T_R1_B.fastq.gz\n",
      "[FLASH]     sequence_merge_method/D_split_reads/DNA_Data_7T_R2_B.fastq.gz\n",
      "[FLASH]  \n",
      "[FLASH] Output files:\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_7T_FLASH.extendedFrags.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_7T_FLASH.notCombined_1.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_7T_FLASH.notCombined_2.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_7T_FLASH.hist\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_7T_FLASH.histogram\n",
      "[FLASH]  \n",
      "[FLASH] Parameters:\n",
      "[FLASH]     Min overlap:           120\n",
      "[FLASH]     Max overlap:           120\n",
      "[FLASH]     Max mismatch density:  0.250000\n",
      "[FLASH]     Allow \"outie\" pairs:   false\n",
      "[FLASH]     Cap mismatch quals:    false\n",
      "[FLASH]     Combiner threads:      4\n",
      "[FLASH]     Input format:          FASTQ, phred_offset=33\n",
      "[FLASH]     Output format:         FASTQ, phred_offset=33\n",
      "[FLASH]  \n",
      "[FLASH] Starting reader and writer threads\n",
      "[FLASH] Starting 4 combiner threads\n",
      "[FLASH] Processed 139 read pairs\n",
      "[FLASH]  \n",
      "[FLASH] Read combination statistics:\n",
      "[FLASH]     Total pairs:      139\n",
      "[FLASH]     Combined pairs:   50\n",
      "[FLASH]     Uncombined pairs: 89\n",
      "[FLASH]     Percent combined: 35.97%\n",
      "[FLASH]  \n",
      "[FLASH] Writing histogram files.\n",
      "[FLASH]  \n",
      "[FLASH] FLASH v1.2.11 complete!\n",
      "[FLASH] 0.015 seconds elapsed\n",
      "✅ FLASH merging complete → sequence_merge_method/E_merged_output/DNA_Data_7T_FLASH.extendedFrags.fastq\n",
      "🔵 Running FLASH for sample: DNA_Data_6S (N=124)\n",
      "[FLASH] Starting FLASH v1.2.11\n",
      "[FLASH] Fast Length Adjustment of SHort reads\n",
      "[FLASH]  \n",
      "[FLASH] Input files:\n",
      "[FLASH]     sequence_merge_method/D_split_reads/DNA_Data_6S_R1_B.fastq.gz\n",
      "[FLASH]     sequence_merge_method/D_split_reads/DNA_Data_6S_R2_B.fastq.gz\n",
      "[FLASH]  \n",
      "[FLASH] Output files:\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_6S_FLASH.extendedFrags.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_6S_FLASH.notCombined_1.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_6S_FLASH.notCombined_2.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_6S_FLASH.hist\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_6S_FLASH.histogram\n",
      "[FLASH]  \n",
      "[FLASH] Parameters:\n",
      "[FLASH]     Min overlap:           124\n",
      "[FLASH]     Max overlap:           124\n",
      "[FLASH]     Max mismatch density:  0.250000\n",
      "[FLASH]     Allow \"outie\" pairs:   false\n",
      "[FLASH]     Cap mismatch quals:    false\n",
      "[FLASH]     Combiner threads:      4\n",
      "[FLASH]     Input format:          FASTQ, phred_offset=33\n",
      "[FLASH]     Output format:         FASTQ, phred_offset=33\n",
      "[FLASH]  \n",
      "[FLASH] Starting reader and writer threads\n",
      "[FLASH] Starting 4 combiner threads\n",
      "[FLASH] Processed 140 read pairs\n",
      "[FLASH]  \n",
      "[FLASH] Read combination statistics:\n",
      "[FLASH]     Total pairs:      140\n",
      "[FLASH]     Combined pairs:   47\n",
      "[FLASH]     Uncombined pairs: 93\n",
      "[FLASH]     Percent combined: 33.57%\n",
      "[FLASH]  \n",
      "[FLASH] Writing histogram files.\n",
      "[FLASH]  \n",
      "[FLASH] FLASH v1.2.11 complete!\n",
      "[FLASH] 0.019 seconds elapsed\n",
      "✅ FLASH merging complete → sequence_merge_method/E_merged_output/DNA_Data_6S_FLASH.extendedFrags.fastq\n",
      "🔵 Running FLASH for sample: DNA_Data_2S (N=126)\n",
      "[FLASH] Starting FLASH v1.2.11\n",
      "[FLASH] Fast Length Adjustment of SHort reads\n",
      "[FLASH]  \n",
      "[FLASH] Input files:\n",
      "[FLASH]     sequence_merge_method/D_split_reads/DNA_Data_2S_R1_B.fastq.gz\n",
      "[FLASH]     sequence_merge_method/D_split_reads/DNA_Data_2S_R2_B.fastq.gz\n",
      "[FLASH]  \n",
      "[FLASH] Output files:\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_2S_FLASH.extendedFrags.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_2S_FLASH.notCombined_1.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_2S_FLASH.notCombined_2.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_2S_FLASH.hist\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_2S_FLASH.histogram\n",
      "[FLASH]  \n",
      "[FLASH] Parameters:\n",
      "[FLASH]     Min overlap:           126\n",
      "[FLASH]     Max overlap:           126\n",
      "[FLASH]     Max mismatch density:  0.250000\n",
      "[FLASH]     Allow \"outie\" pairs:   false\n",
      "[FLASH]     Cap mismatch quals:    false\n",
      "[FLASH]     Combiner threads:      4\n",
      "[FLASH]     Input format:          FASTQ, phred_offset=33\n",
      "[FLASH]     Output format:         FASTQ, phred_offset=33\n",
      "[FLASH]  \n",
      "[FLASH] Starting reader and writer threads\n",
      "[FLASH] Starting 4 combiner threads\n",
      "[FLASH] Processed 119 read pairs\n",
      "[FLASH]  \n",
      "[FLASH] Read combination statistics:\n",
      "[FLASH]     Total pairs:      119\n",
      "[FLASH]     Combined pairs:   47\n",
      "[FLASH]     Uncombined pairs: 72\n",
      "[FLASH]     Percent combined: 39.50%\n",
      "[FLASH]  \n",
      "[FLASH] Writing histogram files.\n",
      "[FLASH]  \n",
      "[FLASH] FLASH v1.2.11 complete!\n",
      "[FLASH] 0.015 seconds elapsed\n",
      "✅ FLASH merging complete → sequence_merge_method/E_merged_output/DNA_Data_2S_FLASH.extendedFrags.fastq\n",
      "🔵 Running FLASH for sample: DNA_Data_0N (N=126)\n",
      "[FLASH] Starting FLASH v1.2.11\n",
      "[FLASH] Fast Length Adjustment of SHort reads\n",
      "[FLASH]  \n",
      "[FLASH] Input files:\n",
      "[FLASH]     sequence_merge_method/D_split_reads/DNA_Data_0N_R1_B.fastq.gz\n",
      "[FLASH]     sequence_merge_method/D_split_reads/DNA_Data_0N_R2_B.fastq.gz\n",
      "[FLASH]  \n",
      "[FLASH] Output files:\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_0N_FLASH.extendedFrags.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_0N_FLASH.notCombined_1.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_0N_FLASH.notCombined_2.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_0N_FLASH.hist\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_0N_FLASH.histogram\n",
      "[FLASH]  \n",
      "[FLASH] Parameters:\n",
      "[FLASH]     Min overlap:           126\n",
      "[FLASH]     Max overlap:           126\n",
      "[FLASH]     Max mismatch density:  0.250000\n",
      "[FLASH]     Allow \"outie\" pairs:   false\n",
      "[FLASH]     Cap mismatch quals:    false\n",
      "[FLASH]     Combiner threads:      4\n",
      "[FLASH]     Input format:          FASTQ, phred_offset=33\n",
      "[FLASH]     Output format:         FASTQ, phred_offset=33\n",
      "[FLASH]  \n",
      "[FLASH] Starting reader and writer threads\n",
      "[FLASH] Starting 4 combiner threads\n",
      "[FLASH] Processed 136 read pairs\n",
      "[FLASH]  \n",
      "[FLASH] Read combination statistics:\n",
      "[FLASH]     Total pairs:      136\n",
      "[FLASH]     Combined pairs:   50\n",
      "[FLASH]     Uncombined pairs: 86\n",
      "[FLASH]     Percent combined: 36.76%\n",
      "[FLASH]  \n",
      "[FLASH] Writing histogram files.\n",
      "[FLASH]  \n",
      "[FLASH] FLASH v1.2.11 complete!\n",
      "[FLASH] 0.015 seconds elapsed\n",
      "✅ FLASH merging complete → sequence_merge_method/E_merged_output/DNA_Data_0N_FLASH.extendedFrags.fastq\n",
      "🔵 Running FLASH for sample: DNA_Data_4G (N=126)\n",
      "[FLASH] Starting FLASH v1.2.11\n",
      "[FLASH] Fast Length Adjustment of SHort reads\n",
      "[FLASH]  \n",
      "[FLASH] Input files:\n",
      "[FLASH]     sequence_merge_method/D_split_reads/DNA_Data_4G_R1_B.fastq.gz\n",
      "[FLASH]     sequence_merge_method/D_split_reads/DNA_Data_4G_R2_B.fastq.gz\n",
      "[FLASH]  \n",
      "[FLASH] Output files:\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_4G_FLASH.extendedFrags.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_4G_FLASH.notCombined_1.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_4G_FLASH.notCombined_2.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_4G_FLASH.hist\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_4G_FLASH.histogram\n",
      "[FLASH]  \n",
      "[FLASH] Parameters:\n",
      "[FLASH]     Min overlap:           126\n",
      "[FLASH]     Max overlap:           126\n",
      "[FLASH]     Max mismatch density:  0.250000\n",
      "[FLASH]     Allow \"outie\" pairs:   false\n",
      "[FLASH]     Cap mismatch quals:    false\n",
      "[FLASH]     Combiner threads:      4\n",
      "[FLASH]     Input format:          FASTQ, phred_offset=33\n",
      "[FLASH]     Output format:         FASTQ, phred_offset=33\n",
      "[FLASH]  \n",
      "[FLASH] Starting reader and writer threads\n",
      "[FLASH] Starting 4 combiner threads\n",
      "[FLASH] Processed 152 read pairs\n",
      "[FLASH]  \n",
      "[FLASH] Read combination statistics:\n",
      "[FLASH]     Total pairs:      152\n",
      "[FLASH]     Combined pairs:   61\n",
      "[FLASH]     Uncombined pairs: 91\n",
      "[FLASH]     Percent combined: 40.13%\n",
      "[FLASH]  \n",
      "[FLASH] Writing histogram files.\n",
      "[FLASH]  \n",
      "[FLASH] FLASH v1.2.11 complete!\n",
      "[FLASH] 0.016 seconds elapsed\n",
      "✅ FLASH merging complete → sequence_merge_method/E_merged_output/DNA_Data_4G_FLASH.extendedFrags.fastq\n",
      "🔵 Running FLASH for sample: DNA_Data_1D (N=126)\n",
      "[FLASH] Starting FLASH v1.2.11\n",
      "[FLASH] Fast Length Adjustment of SHort reads\n",
      "[FLASH]  \n",
      "[FLASH] Input files:\n",
      "[FLASH]     sequence_merge_method/D_split_reads/DNA_Data_1D_R1_B.fastq.gz\n",
      "[FLASH]     sequence_merge_method/D_split_reads/DNA_Data_1D_R2_B.fastq.gz\n",
      "[FLASH]  \n",
      "[FLASH] Output files:\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_1D_FLASH.extendedFrags.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_1D_FLASH.notCombined_1.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_1D_FLASH.notCombined_2.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_1D_FLASH.hist\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_1D_FLASH.histogram\n",
      "[FLASH]  \n",
      "[FLASH] Parameters:\n",
      "[FLASH]     Min overlap:           126\n",
      "[FLASH]     Max overlap:           126\n",
      "[FLASH]     Max mismatch density:  0.250000\n",
      "[FLASH]     Allow \"outie\" pairs:   false\n",
      "[FLASH]     Cap mismatch quals:    false\n",
      "[FLASH]     Combiner threads:      4\n",
      "[FLASH]     Input format:          FASTQ, phred_offset=33\n",
      "[FLASH]     Output format:         FASTQ, phred_offset=33\n",
      "[FLASH]  \n",
      "[FLASH] Starting reader and writer threads\n",
      "[FLASH] Starting 4 combiner threads\n",
      "[FLASH] Processed 146 read pairs\n",
      "[FLASH]  \n",
      "[FLASH] Read combination statistics:\n",
      "[FLASH]     Total pairs:      146\n",
      "[FLASH]     Combined pairs:   63\n",
      "[FLASH]     Uncombined pairs: 83\n",
      "[FLASH]     Percent combined: 43.15%\n",
      "[FLASH]  \n",
      "[FLASH] Writing histogram files.\n",
      "[FLASH]  \n",
      "[FLASH] FLASH v1.2.11 complete!\n",
      "[FLASH] 0.014 seconds elapsed\n",
      "✅ FLASH merging complete → sequence_merge_method/E_merged_output/DNA_Data_1D_FLASH.extendedFrags.fastq\n",
      "🔵 Running FLASH for sample: DNA_Data_5I (N=126)\n",
      "[FLASH] Starting FLASH v1.2.11\n",
      "[FLASH] Fast Length Adjustment of SHort reads\n",
      "[FLASH]  \n",
      "[FLASH] Input files:\n",
      "[FLASH]     sequence_merge_method/D_split_reads/DNA_Data_5I_R1_B.fastq.gz\n",
      "[FLASH]     sequence_merge_method/D_split_reads/DNA_Data_5I_R2_B.fastq.gz\n",
      "[FLASH]  \n",
      "[FLASH] Output files:\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_5I_FLASH.extendedFrags.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_5I_FLASH.notCombined_1.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_5I_FLASH.notCombined_2.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_5I_FLASH.hist\n",
      "[FLASH]     sequence_merge_method/E_merged_output/DNA_Data_5I_FLASH.histogram\n",
      "[FLASH]  \n",
      "[FLASH] Parameters:\n",
      "[FLASH]     Min overlap:           126\n",
      "[FLASH]     Max overlap:           126\n",
      "[FLASH]     Max mismatch density:  0.250000\n",
      "[FLASH]     Allow \"outie\" pairs:   false\n",
      "[FLASH]     Cap mismatch quals:    false\n",
      "[FLASH]     Combiner threads:      4\n",
      "[FLASH]     Input format:          FASTQ, phred_offset=33\n",
      "[FLASH]     Output format:         FASTQ, phred_offset=33\n",
      "[FLASH]  \n",
      "[FLASH] Starting reader and writer threads\n",
      "[FLASH] Starting 4 combiner threads\n",
      "[FLASH] Processed 135 read pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FLASH]  \n",
      "[FLASH] Read combination statistics:\n",
      "[FLASH]     Total pairs:      135\n",
      "[FLASH]     Combined pairs:   47\n",
      "[FLASH]     Uncombined pairs: 88\n",
      "[FLASH]     Percent combined: 34.81%\n",
      "[FLASH]  \n",
      "[FLASH] Writing histogram files.\n",
      "[FLASH]  \n",
      "[FLASH] FLASH v1.2.11 complete!\n",
      "[FLASH] 0.016 seconds elapsed\n",
      "✅ FLASH merging complete → sequence_merge_method/E_merged_output/DNA_Data_5I_FLASH.extendedFrags.fastq\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "# === Folder Setup ===\n",
    "input_folder = \"sequence_merge_method/D_split_reads\"\n",
    "output_folder = \"sequence_merge_method/E_merged_output\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# === Set N-values (Overlap Length) per Sample Prefix ===\n",
    "sample_n_mapping = {\n",
    "    \"0N\": 126,\n",
    "    \"1D\": 126,\n",
    "    \"2S\": 126,\n",
    "    \"3G\": 124,\n",
    "    \"4I\": 128,\n",
    "    \"5S\": 124,\n",
    "    \"6T\": 122,\n",
    "    \"5K\": 124,\n",
    "    \"1X8\": 116,\n",
    "    \"0X8\": 132,\n",
    "    \"4G\": 126,\n",
    "    \"5I\": 126,\n",
    "    \"6S\": 124,\n",
    "    \"7T\": 120,\n",
    "    \"3SP26\": 122,\n",
    "    \"3SP31\": 118   \n",
    "}\n",
    "\n",
    "# === Find List of all R1_B Files ===\n",
    "r1_files = glob.glob(os.path.join(input_folder, \"*_R1_B.fastq.gz\"))\n",
    "\n",
    "print(f\"🔎 Found {len(r1_files)} R1_B files.\")\n",
    "\n",
    "# === Process Each R1_B File ===\n",
    "for r1_path in r1_files:\n",
    "    sample_base = os.path.basename(r1_path).replace(\"_R1_B.fastq.gz\", \"\")\n",
    "    r2_path = os.path.join(input_folder, f\"{sample_base}_R2_B.fastq.gz\")\n",
    "\n",
    "    if not os.path.exists(r2_path):\n",
    "        print(f\"⚠️ Matching R2_B file not found for {sample_base} → Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Find the corresponding N value for the filename\n",
    "    matched_n = None\n",
    "    for prefix, n_value in sample_n_mapping.items():\n",
    "        if prefix in sample_base:\n",
    "            matched_n = n_value\n",
    "            break\n",
    "\n",
    "    if matched_n is None:\n",
    "        print(f\"⚠️ No N value matched for {sample_base} → Skipping.\")\n",
    "        continue\n",
    "\n",
    "    output_name = f\"{sample_base}_FLASH\"\n",
    "\n",
    "    print(f\"🔵 Running FLASH for sample: {sample_base} (N={matched_n})\")\n",
    "\n",
    "    try:\n",
    "        # Execute the FLASH command\n",
    "        subprocess.check_call([\n",
    "            \"flash\",\n",
    "            \"-m\", str(matched_n),   # minimum overlap\n",
    "            \"-M\", str(matched_n),   # Maximum overlap\n",
    "            \"-o\", output_name,      # Output file prefix\n",
    "            \"-d\", output_folder,    # Output directory\n",
    "            r1_path,\n",
    "            r2_path\n",
    "        ])\n",
    "        print(f\"✅ FLASH merging complete → {os.path.join(output_folder, output_name)}.extendedFrags.fastq\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ FLASH merging failed for {sample_base}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077413cc",
   "metadata": {},
   "source": [
    "## 5.4 Assemble \n",
    "## R1_Front - [R1_Back]-[R2_Back]_merged (FLASH) - R2_Front_ReverseComplement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45a3c6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Found 8 merged samples to assemble.\n",
      "🔄 Assembling for sample: DNA_Data_7T_assemble.fastq.gz\n",
      "✅ Assembled FASTQ saved: sequence_merge_method/1_assemble/DNA_Data_7T_assemble.fastq.gz\n",
      "🔄 Assembling for sample: DNA_Data_5I_assemble.fastq.gz\n",
      "✅ Assembled FASTQ saved: sequence_merge_method/1_assemble/DNA_Data_5I_assemble.fastq.gz\n",
      "🔄 Assembling for sample: DNA_Data_6S_assemble.fastq.gz\n",
      "✅ Assembled FASTQ saved: sequence_merge_method/1_assemble/DNA_Data_6S_assemble.fastq.gz\n",
      "🔄 Assembling for sample: DNA_Data_4G_assemble.fastq.gz\n",
      "✅ Assembled FASTQ saved: sequence_merge_method/1_assemble/DNA_Data_4G_assemble.fastq.gz\n",
      "🔄 Assembling for sample: DNA_Data_1D_assemble.fastq.gz\n",
      "✅ Assembled FASTQ saved: sequence_merge_method/1_assemble/DNA_Data_1D_assemble.fastq.gz\n",
      "🔄 Assembling for sample: DNA_Data_3SP26_assemble.fastq.gz\n",
      "✅ Assembled FASTQ saved: sequence_merge_method/1_assemble/DNA_Data_3SP26_assemble.fastq.gz\n",
      "🔄 Assembling for sample: DNA_Data_2S_assemble.fastq.gz\n",
      "✅ Assembled FASTQ saved: sequence_merge_method/1_assemble/DNA_Data_2S_assemble.fastq.gz\n",
      "🔄 Assembling for sample: DNA_Data_0N_assemble.fastq.gz\n",
      "✅ Assembled FASTQ saved: sequence_merge_method/1_assemble/DNA_Data_0N_assemble.fastq.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "import glob\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "def load_fastq_to_dict(file_path):\n",
    "    \"\"\"Loads a FASTQ file into a dictionary: key=read_id, value=(sequence, quality).\"\"\"\n",
    "    data = {}\n",
    "    open_func = gzip.open if file_path.endswith(\".gz\") else open\n",
    "\n",
    "    with open_func(file_path, \"rt\") as handle:\n",
    "        for record in SeqIO.parse(handle, \"fastq\"):\n",
    "            seq = str(record.seq)\n",
    "            qual = record.letter_annotations[\"phred_quality\"]\n",
    "            data[record.id] = (seq, qual)\n",
    "    return data\n",
    "\n",
    "def assemble_fastq(r1_path, merged_path, r2_path, output_path):\n",
    "    \"\"\"Assembles the final sequence from R1_F, Merged, and R2_F_revcomp fragments.\"\"\"\n",
    "    print(f\"🔄 Assembling for sample: {os.path.basename(output_path)}\")\n",
    "    r1_dict = load_fastq_to_dict(r1_path)\n",
    "    r2_dict = load_fastq_to_dict(r2_path)\n",
    "\n",
    "    with open(merged_path, \"r\") as merged_file, gzip.open(output_path, \"wt\") as output_file:\n",
    "        for record in SeqIO.parse(merged_file, \"fastq\"):\n",
    "            read_id = record.id\n",
    "            merged_seq = str(record.seq)\n",
    "            merged_qual = record.letter_annotations[\"phred_quality\"]\n",
    "\n",
    "            # A read must have corresponding R1 and R2 fragments to be assembled.\n",
    "            if read_id not in r1_dict or read_id not in r2_dict:\n",
    "                continue  \n",
    "\n",
    "            r1_seq, r1_qual = r1_dict[read_id]\n",
    "            r2_seq, r2_qual = r2_dict[read_id]\n",
    "\n",
    "            # Concatenate in order: R1_F → Merged_Fragment → R2_F_revcomp\n",
    "            full_seq = r1_seq + merged_seq + r2_seq\n",
    "            full_qual = r1_qual + merged_qual + r2_qual\n",
    "\n",
    "            new_record = SeqRecord(\n",
    "                Seq(full_seq),\n",
    "                id=read_id,\n",
    "                description=\"\",\n",
    "                letter_annotations={\"phred_quality\": full_qual}\n",
    "            )\n",
    "\n",
    "            SeqIO.write(new_record, output_file, \"fastq\")\n",
    "\n",
    "    print(f\"✅ Assembled FASTQ saved: {output_path}\")\n",
    "\n",
    "# ===== Batch processing =====\n",
    "\n",
    "# Path setup\n",
    "input_merged_folder = \"sequence_merge_method/E_merged_output\"\n",
    "input_split_folder = \"sequence_merge_method/D_split_reads\"\n",
    "output_folder = \"sequence_merge_method/1_assemble\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get a list of all merged files from FLASH\n",
    "merged_files = glob.glob(os.path.join(input_merged_folder, \"*_FLASH.extendedFrags.fastq\"))\n",
    "\n",
    "print(f\"🔍 Found {len(merged_files)} merged samples to assemble.\")\n",
    "\n",
    "for merged_file in merged_files:\n",
    "    sample_base = os.path.basename(merged_file).replace(\"_FLASH.extendedFrags.fastq\", \"\")\n",
    "\n",
    "    r1_path = os.path.join(input_split_folder, f\"{sample_base}_R1_F.fastq.gz\")\n",
    "    r2_path = os.path.join(input_split_folder, f\"{sample_base}_R2_F_revcomp.fastq.gz\")\n",
    "    output_path = os.path.join(output_folder, f\"{sample_base}_assemble.fastq.gz\")\n",
    "\n",
    "    if os.path.exists(r1_path) and os.path.exists(r2_path):\n",
    "        assemble_fastq(r1_path, merged_file, r2_path, output_path)\n",
    "    else:\n",
    "        print(f\"⚠️ Missing split files for {sample_base}, skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea775e95",
   "metadata": {},
   "source": [
    "# 6. fastq -> fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5d839a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted: DNA_Data_1D_assemble.fastq.gz → DNA_Data_1D_assemble.fasta\n",
      "Converted: DNA_Data_4G_assemble.fastq.gz → DNA_Data_4G_assemble.fasta\n",
      "Converted: DNA_Data_0N_assemble.fastq.gz → DNA_Data_0N_assemble.fasta\n",
      "Converted: DNA_Data_5I_assemble.fastq.gz → DNA_Data_5I_assemble.fasta\n",
      "Converted: DNA_Data_6S_assemble.fastq.gz → DNA_Data_6S_assemble.fasta\n",
      "Converted: DNA_Data_7T_assemble.fastq.gz → DNA_Data_7T_assemble.fasta\n",
      "Converted: DNA_Data_2S_assemble.fastq.gz → DNA_Data_2S_assemble.fasta\n",
      "Converted: DNA_Data_3SP26_assemble.fastq.gz → DNA_Data_3SP26_assemble.fasta\n",
      "All conversions are done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "from Bio import SeqIO\n",
    "\n",
    "# Input and output folder paths\n",
    "input_folder = \"sequence_merge_method/1_assemble\"\n",
    "output_folder = \"sequence_merge_method/2_fastq_to_fasta\"\n",
    "\n",
    "# Create the output folder if it doesn't exist.\n",
    "os.makedirs(output_folder, exist_ok=True)  \n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    # Process only files with .fastq or .fastq.gz extensions\n",
    "    if filename.endswith(\".fastq\") or filename.endswith(\".fastq.gz\"):\n",
    "        input_file = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Set output filename (.fasta extension)\n",
    "        output_file = os.path.join(\n",
    "            output_folder,\n",
    "            filename.replace(\".fastq.gz\", \".fasta\").replace(\".fastq\", \".fasta\")\n",
    "        )\n",
    "\n",
    "        # Choose open mode based on gzip\n",
    "        open_func = gzip.open if filename.endswith(\".gz\") else open\n",
    "\n",
    "        # Read FASTQ and convert to FASTA\n",
    "        with open_func(input_file, \"rt\") as fastq_file:\n",
    "            # open in text mode\n",
    "            records = list(SeqIO.parse(fastq_file, \"fastq\"))\n",
    "\n",
    "        # Save as FASTA\n",
    "        with open(output_file, \"w\") as fasta_file:\n",
    "            SeqIO.write(records, fasta_file, \"fasta\")\n",
    "\n",
    "        print(f\"Converted: {filename} → {os.path.basename(output_file)}\")\n",
    "\n",
    "print(\"All conversions are done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87eb850",
   "metadata": {},
   "source": [
    "# 7. Binary data reference seqeunce data generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c367766f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 8-bit (256) barcodes FASTA saved: reference_sequence/8bit_reference.fasta\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def generate_sequences_for_bit(bit_length: int):\n",
    "    \"\"\"\n",
    "    Generate DNA sequences for all binary combinations of the given bit_length.\n",
    "    (bit_length=8 -> 256 barcodes)\n",
    "    \"\"\"\n",
    "    sequences = {}\n",
    "\n",
    "    seq_0 = \"ACTCATATACACACTTAATC\"\n",
    "    seq_1 = \"ACTCATATACATACACTTAATC\"\n",
    "    prefix = \"ACACTTAATC\"\n",
    "\n",
    "    for i in range(2 ** bit_length):\n",
    "        binary_str = format(i, f'0{bit_length}b')\n",
    "        sequence = ''.join(seq_1 if bit == '1' else seq_0 for bit in binary_str)\n",
    "        full_sequence = prefix + sequence\n",
    "        seq_id = f\"seq_{i:04d}_{binary_str}\"\n",
    "        sequences[seq_id] = full_sequence\n",
    "\n",
    "    return sequences\n",
    "\n",
    "def write_fasta(sequences: dict, output_path: str):\n",
    "    \"\"\"Write sequences to a FASTA file.\"\"\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for seq_id, sequence in sequences.items():\n",
    "            f.write(f\">{seq_id}\\n{sequence}\\n\")\n",
    "\n",
    "# ===== Settings: exactly 8 bits (256 barcodes) =====\n",
    "BIT_LENGTH = 8  # 2^8 = 256\n",
    "output_dir = Path(\"reference_sequence\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_path = output_dir / \"8bit_reference.fasta\"\n",
    "# ==================================================\n",
    "\n",
    "seqs = generate_sequences_for_bit(BIT_LENGTH)\n",
    "write_fasta(seqs, output_path)\n",
    "print(f\"✅ 8-bit (256) barcodes FASTA saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d4c714",
   "metadata": {},
   "source": [
    "# 8. Reference sequence - Sample Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3b8de4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bwa_index] Pack FASTA... 0.00 sec\r\n",
      "[bwa_index] Construct BWT for the packed sequence...\r\n",
      "[bwa_index] 0.00 seconds elapse.\r\n",
      "[bwa_index] Update BWT... 0.00 sec\r\n",
      "[bwa_index] Pack forward-only FASTA... 0.00 sec\r\n",
      "[bwa_index] Construct SA from BWT and Occ... 0.00 sec\r\n",
      "[main] Version: 0.7.17-r1188\r\n",
      "[main] CMD: bwa index reference_sequence/8bit_reference.fasta\r\n",
      "[main] Real time: 0.049 sec; CPU: 0.012 sec\r\n"
     ]
    }
   ],
   "source": [
    "# Index reference\n",
    "!bwa index \"reference_sequence/8bit_reference.fasta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6dedb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[M::bwa_idx_load_from_disk] read 0 ALT contigs\n",
      "[M::process] read 50 sequences (8800 bp)...\n",
      "[M::mem_process_seqs] Processed 50 reads in 0.024 CPU sec, 0.008 real sec\n",
      "[main] Version: 0.7.17-r1188\n",
      "[main] CMD: bwa mem -M -t 4 reference_sequence/8bit_reference.fasta sequence_merge_method/2_fastq_to_fasta/DNA_Data_0N_assemble.fasta\n",
      "[main] Real time: 0.033 sec; CPU: 0.027 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment completed for sequence_merge_method/2_fastq_to_fasta/DNA_Data_0N_assemble.fasta. Result saved as sequence_merge_method/3_align_sam/DNA_Data_0N_assemble.sam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[M::bwa_idx_load_from_disk] read 0 ALT contigs\n",
      "[M::process] read 63 sequences (11088 bp)...\n",
      "[M::mem_process_seqs] Processed 63 reads in 0.024 CPU sec, 0.011 real sec\n",
      "[main] Version: 0.7.17-r1188\n",
      "[main] CMD: bwa mem -M -t 4 reference_sequence/8bit_reference.fasta sequence_merge_method/2_fastq_to_fasta/DNA_Data_1D_assemble.fasta\n",
      "[main] Real time: 0.041 sec; CPU: 0.028 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment completed for sequence_merge_method/2_fastq_to_fasta/DNA_Data_1D_assemble.fasta. Result saved as sequence_merge_method/3_align_sam/DNA_Data_1D_assemble.sam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[M::bwa_idx_load_from_disk] read 0 ALT contigs\n",
      "[M::process] read 47 sequences (8272 bp)...\n",
      "[M::mem_process_seqs] Processed 47 reads in 0.027 CPU sec, 0.014 real sec\n",
      "[main] Version: 0.7.17-r1188\n",
      "[main] CMD: bwa mem -M -t 4 reference_sequence/8bit_reference.fasta sequence_merge_method/2_fastq_to_fasta/DNA_Data_2S_assemble.fasta\n",
      "[main] Real time: 0.040 sec; CPU: 0.031 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment completed for sequence_merge_method/2_fastq_to_fasta/DNA_Data_2S_assemble.fasta. Result saved as sequence_merge_method/3_align_sam/DNA_Data_2S_assemble.sam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[M::bwa_idx_load_from_disk] read 0 ALT contigs\n",
      "[M::process] read 420 sequences (75600 bp)...\n",
      "[M::mem_process_seqs] Processed 420 reads in 0.356 CPU sec, 0.100 real sec\n",
      "[main] Version: 0.7.17-r1188\n",
      "[main] CMD: bwa mem -M -t 4 reference_sequence/8bit_reference.fasta sequence_merge_method/2_fastq_to_fasta/DNA_Data_3SP26_assemble.fasta\n",
      "[main] Real time: 0.136 sec; CPU: 0.360 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment completed for sequence_merge_method/2_fastq_to_fasta/DNA_Data_3SP26_assemble.fasta. Result saved as sequence_merge_method/3_align_sam/DNA_Data_3SP26_assemble.sam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[M::bwa_idx_load_from_disk] read 0 ALT contigs\n",
      "[M::process] read 61 sequences (10736 bp)...\n",
      "[M::mem_process_seqs] Processed 61 reads in 0.042 CPU sec, 0.015 real sec\n",
      "[main] Version: 0.7.17-r1188\n",
      "[main] CMD: bwa mem -M -t 4 reference_sequence/8bit_reference.fasta sequence_merge_method/2_fastq_to_fasta/DNA_Data_4G_assemble.fasta\n",
      "[main] Real time: 0.038 sec; CPU: 0.045 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment completed for sequence_merge_method/2_fastq_to_fasta/DNA_Data_4G_assemble.fasta. Result saved as sequence_merge_method/3_align_sam/DNA_Data_4G_assemble.sam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[M::bwa_idx_load_from_disk] read 0 ALT contigs\n",
      "[M::process] read 47 sequences (8272 bp)...\n",
      "[M::mem_process_seqs] Processed 47 reads in 0.019 CPU sec, 0.012 real sec\n",
      "[main] Version: 0.7.17-r1188\n",
      "[main] CMD: bwa mem -M -t 4 reference_sequence/8bit_reference.fasta sequence_merge_method/2_fastq_to_fasta/DNA_Data_5I_assemble.fasta\n",
      "[main] Real time: 0.041 sec; CPU: 0.023 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment completed for sequence_merge_method/2_fastq_to_fasta/DNA_Data_5I_assemble.fasta. Result saved as sequence_merge_method/3_align_sam/DNA_Data_5I_assemble.sam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[M::bwa_idx_load_from_disk] read 0 ALT contigs\n",
      "[M::process] read 47 sequences (8366 bp)...\n",
      "[M::mem_process_seqs] Processed 47 reads in 0.025 CPU sec, 0.008 real sec\n",
      "[main] Version: 0.7.17-r1188\n",
      "[main] CMD: bwa mem -M -t 4 reference_sequence/8bit_reference.fasta sequence_merge_method/2_fastq_to_fasta/DNA_Data_6S_assemble.fasta\n",
      "[main] Real time: 0.031 sec; CPU: 0.028 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment completed for sequence_merge_method/2_fastq_to_fasta/DNA_Data_6S_assemble.fasta. Result saved as sequence_merge_method/3_align_sam/DNA_Data_6S_assemble.sam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[M::bwa_idx_load_from_disk] read 0 ALT contigs\n",
      "[M::process] read 50 sequences (9100 bp)...\n",
      "[M::mem_process_seqs] Processed 50 reads in 0.037 CPU sec, 0.011 real sec\n",
      "[main] Version: 0.7.17-r1188\n",
      "[main] CMD: bwa mem -M -t 4 reference_sequence/8bit_reference.fasta sequence_merge_method/2_fastq_to_fasta/DNA_Data_7T_assemble.fasta\n",
      "[main] Real time: 0.037 sec; CPU: 0.041 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment completed for sequence_merge_method/2_fastq_to_fasta/DNA_Data_7T_assemble.fasta. Result saved as sequence_merge_method/3_align_sam/DNA_Data_7T_assemble.sam\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Set the path to the reference sequence file\n",
    "reference_file=\"reference_sequence/8bit_reference.fasta\"\n",
    "\n",
    "# Set the directory containing your filtered FASTA files\n",
    "fasta_directory=\"sequence_merge_method/2_fastq_to_fasta\"\n",
    "# Set the output directory for aligned SAM files\n",
    "output_dir=\"sequence_merge_method/3_align_sam\"\n",
    "\n",
    "# Make sure the output directory exists or create it if necessary\n",
    "mkdir -p \"$output_dir\"\n",
    "\n",
    "# Iterate through filtered FASTA files in the specified directory\n",
    "for fasta_file in \"$fasta_directory\"/*_assemble.fasta; do\n",
    "    # Generate an output file name based on the input filename\n",
    "    output_file=\"$output_dir/$(basename \"$fasta_file\" .fasta).sam\"\n",
    "\n",
    "    # Perform the BWA alignment \n",
    "    bwa mem -M -t 4 \"$reference_file\" \"$fasta_file\" > \"$output_file\"\n",
    "\n",
    "    echo \"Alignment completed for $fasta_file. Result saved as $output_file\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3177f110",
   "metadata": {},
   "source": [
    "## 8.1 sam to bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "055d51f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion from sequence_merge_method/3_align_sam/DNA_Data_0N_assemble.sam to sequence_merge_method/4_align_bam/DNA_Data_0N_assemble.bam is complete.\n",
      "Conversion from sequence_merge_method/3_align_sam/DNA_Data_1D_assemble.sam to sequence_merge_method/4_align_bam/DNA_Data_1D_assemble.bam is complete.\n",
      "Conversion from sequence_merge_method/3_align_sam/DNA_Data_2S_assemble.sam to sequence_merge_method/4_align_bam/DNA_Data_2S_assemble.bam is complete.\n",
      "Conversion from sequence_merge_method/3_align_sam/DNA_Data_3SP26_assemble.sam to sequence_merge_method/4_align_bam/DNA_Data_3SP26_assemble.bam is complete.\n",
      "Conversion from sequence_merge_method/3_align_sam/DNA_Data_4G_assemble.sam to sequence_merge_method/4_align_bam/DNA_Data_4G_assemble.bam is complete.\n",
      "Conversion from sequence_merge_method/3_align_sam/DNA_Data_5I_assemble.sam to sequence_merge_method/4_align_bam/DNA_Data_5I_assemble.bam is complete.\n",
      "Conversion from sequence_merge_method/3_align_sam/DNA_Data_6S_assemble.sam to sequence_merge_method/4_align_bam/DNA_Data_6S_assemble.bam is complete.\n",
      "Conversion from sequence_merge_method/3_align_sam/DNA_Data_7T_assemble.sam to sequence_merge_method/4_align_bam/DNA_Data_7T_assemble.bam is complete.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Set the path to the directory containing SAM files\n",
    "sam_dir=\"sequence_merge_method/3_align_sam\"\n",
    "# Set the output directory for BAM files\n",
    "bam_dir=\"sequence_merge_method/4_align_bam\"\n",
    "\n",
    "# Make sure the output directory exists or create it if necessary\n",
    "mkdir -p \"$bam_dir\"\n",
    "\n",
    "# Convert SAM files to BAM\n",
    "for sam_file in \"$sam_dir\"/*.sam; do\n",
    "    bam_file=\"$bam_dir/$(basename \"$sam_file\" .sam).bam\"\n",
    "    samtools view -bS \"$sam_file\" -o \"$bam_file\"\n",
    "    echo \"Conversion from $sam_file to $bam_file is complete.\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2c0b11",
   "metadata": {},
   "source": [
    "## 8.2  Convert BAM to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abee9791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted: DNA_Data_6S_assemble.bam -> DNA_Data_6S_assemble.csv\n",
      "Converted: DNA_Data_2S_assemble.bam -> DNA_Data_2S_assemble.csv\n",
      "Converted: DNA_Data_0N_assemble.bam -> DNA_Data_0N_assemble.csv\n",
      "Converted: DNA_Data_4G_assemble.bam -> DNA_Data_4G_assemble.csv\n",
      "Converted: DNA_Data_1D_assemble.bam -> DNA_Data_1D_assemble.csv\n",
      "Converted: DNA_Data_3SP26_assemble.bam -> DNA_Data_3SP26_assemble.csv\n",
      "Converted: DNA_Data_5I_assemble.bam -> DNA_Data_5I_assemble.csv\n",
      "Converted: DNA_Data_7T_assemble.bam -> DNA_Data_7T_assemble.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sequence_merge_method/4_align_bam/csv/DNA_Data_6S_assemble.csv',\n",
       " 'sequence_merge_method/4_align_bam/csv/DNA_Data_2S_assemble.csv',\n",
       " 'sequence_merge_method/4_align_bam/csv/DNA_Data_0N_assemble.csv',\n",
       " 'sequence_merge_method/4_align_bam/csv/DNA_Data_4G_assemble.csv',\n",
       " 'sequence_merge_method/4_align_bam/csv/DNA_Data_1D_assemble.csv',\n",
       " 'sequence_merge_method/4_align_bam/csv/DNA_Data_3SP26_assemble.csv',\n",
       " 'sequence_merge_method/4_align_bam/csv/DNA_Data_5I_assemble.csv',\n",
       " 'sequence_merge_method/4_align_bam/csv/DNA_Data_7T_assemble.csv']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pysam\n",
    "import pandas as pd\n",
    "\n",
    "# Input folder (path where BAM files are located)\n",
    "input_folder = \"sequence_merge_method/4_align_bam\"\n",
    "# Output folder (path to save CSV files)\n",
    "output_folder = \"sequence_merge_method/4_align_bam/csv\"\n",
    "\n",
    "# Create the output folder if it does not exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Function to convert a BAM file to CSV, including optional fields\n",
    "def bam_to_csv(bam_file, output_folder):\n",
    "    output_csv = os.path.join(output_folder, os.path.basename(bam_file).replace(\".bam\", \".csv\"))\n",
    "    \n",
    "    # Read the BAM file.\n",
    "    with pysam.AlignmentFile(bam_file, \"rb\") as bam:\n",
    "        records = []\n",
    "        \n",
    "        for read in bam:\n",
    "            # Standard BAM fields.\n",
    "            record = {\n",
    "                \"QNAME\": read.query_name,\n",
    "                \"FLAG\": read.flag,\n",
    "                \"RNAME\": bam.get_reference_name(read.reference_id) if read.reference_id >= 0 else \"*\",\n",
    "                \"POS\": read.reference_start + 1,\n",
    "                \"MAPQ\": read.mapping_quality,\n",
    "                \"CIGAR\": read.cigarstring if read.cigarstring else \"*\",\n",
    "                \"RNEXT\": bam.get_reference_name(read.next_reference_id) if read.next_reference_id >= 0 else \"*\",\n",
    "                \"PNEXT\": read.next_reference_start + 1 if read.next_reference_start >= 0 else 0,\n",
    "                \"TLEN\": read.template_length,\n",
    "                \"SEQ\": read.query_sequence if read.query_sequence else \"*\",\n",
    "                \"QUAL\": read.qual if read.qual else \"*\",\n",
    "            }\n",
    "            \n",
    "            # Add optional fields (tags).\n",
    "            for tag, value in read.tags:\n",
    "                record[tag] = value\n",
    "\n",
    "            records.append(record)\n",
    "    \n",
    "    # Create a DataFrame from the list of records.\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    # Fill any missing optional fields with \"*\" instead of NaN for consistency.\n",
    "    df = df.fillna(\"*\")\n",
    "\n",
    "    # Save the DataFrame to a CSV file.\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Converted: {os.path.basename(bam_file)} -> {os.path.basename(output_csv)}\")\n",
    "    return output_csv\n",
    "\n",
    "# Find all BAM files in the input folder.\n",
    "bam_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".bam\")]\n",
    "\n",
    "# Convert all found BAM files to CSV.\n",
    "csv_files = []\n",
    "for bam_file in bam_files:\n",
    "    csv_file = bam_to_csv(bam_file, output_folder)\n",
    "    csv_files.append(csv_file)\n",
    "\n",
    "# Print the list of newly created CSV files.\n",
    "csv_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f1549",
   "metadata": {},
   "source": [
    "## 8.3 Filter Alignments by MAPQ Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ea2039b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DNA_Data_0N_assemble.csv → DNA_Data_0N_assemble.csv | kept=48, removed=2 | threshold=10, keep_nan=True\n",
      "✅ DNA_Data_1D_assemble.csv → DNA_Data_1D_assemble.csv | kept=59, removed=4 | threshold=10, keep_nan=True\n",
      "✅ DNA_Data_2S_assemble.csv → DNA_Data_2S_assemble.csv | kept=43, removed=4 | threshold=10, keep_nan=True\n",
      "✅ DNA_Data_3SP26_assemble.csv → DNA_Data_3SP26_assemble.csv | kept=366, removed=54 | threshold=10, keep_nan=True\n",
      "✅ DNA_Data_4G_assemble.csv → DNA_Data_4G_assemble.csv | kept=48, removed=13 | threshold=10, keep_nan=True\n",
      "✅ DNA_Data_5I_assemble.csv → DNA_Data_5I_assemble.csv | kept=46, removed=1 | threshold=10, keep_nan=True\n",
      "✅ DNA_Data_6S_assemble.csv → DNA_Data_6S_assemble.csv | kept=43, removed=4 | threshold=10, keep_nan=True\n",
      "✅ DNA_Data_7T_assemble.csv → DNA_Data_7T_assemble.csv | kept=42, removed=8 | threshold=10, keep_nan=True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== Settings =====\n",
    "input_dir = Path(\"sequence_merge_method/4_align_bam/csv\") # Input folder containing CSV files\n",
    "output_dir = input_dir / \"MAPQ_removed\"  # Output folder for filtered CSV files\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MAPQ_THRESHOLD = 10     # Keep rows where MAPQ > this value\n",
    "KEEP_NAN = True         # Keep rows with NaN MAPQ values (e.g., unaligned reads)\n",
    "# ====================\n",
    "\n",
    "def process_one_csv(in_path: Path, out_dir: Path, mapq_threshold: int, keep_nan: bool = True):\n",
    "    out_path = out_dir / in_path.name\n",
    "\n",
    "    # Remove existing output file to avoid duplicates\n",
    "    if out_path.exists():\n",
    "        out_path.unlink()\n",
    "\n",
    "    # Read input CSV\n",
    "    try:\n",
    "        df = pd.read_csv(in_path)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Read fail: {in_path.name} -> {e}\")\n",
    "        return\n",
    "\n",
    "    # Skip if MAPQ column does not exist\n",
    "    if \"MAPQ\" not in df.columns:\n",
    "        print(f\"⚠️  Skip (no MAPQ column): {in_path.name}\")\n",
    "        return\n",
    "\n",
    "    # Convert MAPQ column to numeric (invalid entries become NaN)\n",
    "    m = pd.to_numeric(df[\"MAPQ\"], errors=\"coerce\")\n",
    "\n",
    "    # Filtering mask: keep MAPQ > threshold, optionally keep NaN\n",
    "    keep_mask = (m > mapq_threshold) | (m.isna() if keep_nan else False)\n",
    "\n",
    "    kept = int(keep_mask.sum())\n",
    "    removed = int((~keep_mask).sum())\n",
    "\n",
    "    # Save filtered CSV\n",
    "    df.loc[keep_mask].to_csv(out_path, index=False)\n",
    "    print(\n",
    "        f\"✅ {in_path.name} → {out_path.name} | kept={kept}, removed={removed} \"\n",
    "        f\"| threshold={mapq_threshold}, keep_nan={keep_nan}\"\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    csv_files = sorted(input_dir.glob(\"*.csv\"))\n",
    "    if not csv_files:\n",
    "        print(f\"⚠️  No CSV files in {input_dir}\")\n",
    "        return\n",
    "\n",
    "    for p in csv_files:\n",
    "        process_one_csv(p, output_dir, MAPQ_THRESHOLD, KEEP_NAN)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077d3b00",
   "metadata": {},
   "source": [
    "# Histogram Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ed9bc6",
   "metadata": {},
   "source": [
    "## A. Generate Histogram Data from Aligned Reads(MAPQ filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e353ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved full RNAME histogram: sequence_merge_method/5_histogram/histogram_DNA_Data_7T_assemble.csv\n",
      "✅ Saved full RNAME histogram: sequence_merge_method/5_histogram/histogram_DNA_Data_5I_assemble.csv\n",
      "✅ Saved full RNAME histogram: sequence_merge_method/5_histogram/histogram_DNA_Data_3SP26_assemble.csv\n",
      "✅ Saved full RNAME histogram: sequence_merge_method/5_histogram/histogram_DNA_Data_1D_assemble.csv\n",
      "✅ Saved full RNAME histogram: sequence_merge_method/5_histogram/histogram_DNA_Data_4G_assemble.csv\n",
      "✅ Saved full RNAME histogram: sequence_merge_method/5_histogram/histogram_DNA_Data_0N_assemble.csv\n",
      "✅ Saved full RNAME histogram: sequence_merge_method/5_histogram/histogram_DNA_Data_6S_assemble.csv\n",
      "✅ Saved full RNAME histogram: sequence_merge_method/5_histogram/histogram_DNA_Data_2S_assemble.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder setup\n",
    "input_folder = \"sequence_merge_method/4_align_bam/csv/MAPQ_removed\"\n",
    "histogram_folder = \"sequence_merge_method/5_histogram\"\n",
    "os.makedirs(histogram_folder, exist_ok=True)\n",
    "\n",
    "# Process all CSV files in the input folder\n",
    "files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(input_folder, file_name)\n",
    "    output_csv = os.path.join(histogram_folder, f\"histogram_{file_name}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, dtype=str)\n",
    "        if 'RNAME' not in df.columns:\n",
    "            print(f\"Skipping file: {file_name} (no 'RNAME' column found)\")\n",
    "            continue\n",
    "\n",
    "        # Count the occurrences of each unique RNAME\n",
    "        rname_counts = df['RNAME'].value_counts().reset_index()\n",
    "        rname_counts.columns = ['RNAME', 'Count']\n",
    "        \n",
    "        # Add metadata and calculate normalized counts\n",
    "        rname_counts.insert(0, 'File_Name', file_name)\n",
    "        rname_counts['Count'] = rname_counts['Count'].astype(int)\n",
    "        total_count = rname_counts['Count'].sum()\n",
    "        rname_counts['Normalized_Count'] = rname_counts['Count'] / total_count\n",
    "\n",
    "        # Save the histogram data to a new CSV file\n",
    "        rname_counts.to_csv(output_csv, index=False)\n",
    "        print(f\"✅ Saved full RNAME histogram: {output_csv}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing file '{file_name}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dee3b5",
   "metadata": {},
   "source": [
    "## B. Create Top 5 Histogram Plots for Each Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7b94a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Matched files: 8\n",
      "  - histogram_DNA_Data_0N_assemble.csv\n",
      "  - histogram_DNA_Data_1D_assemble.csv\n",
      "  - histogram_DNA_Data_2S_assemble.csv\n",
      "  - histogram_DNA_Data_3SP26_assemble.csv\n",
      "  - histogram_DNA_Data_4G_assemble.csv\n",
      "  - histogram_DNA_Data_5I_assemble.csv\n",
      "  - histogram_DNA_Data_6S_assemble.csv\n",
      "  - histogram_DNA_Data_7T_assemble.csv\n",
      "[OK] Saved plot: sequence_merge_method/5_histogram/graph_top5/0N_top5.png\n",
      "[OK] Saved plot: sequence_merge_method/5_histogram/graph_top5/1D_top5.png\n",
      "[OK] Saved plot: sequence_merge_method/5_histogram/graph_top5/2S_top5.png\n",
      "[OK] Saved plot: sequence_merge_method/5_histogram/graph_top5/3SP26_top5.png\n",
      "[OK] Saved plot: sequence_merge_method/5_histogram/graph_top5/4G_top5.png\n",
      "[OK] Saved plot: sequence_merge_method/5_histogram/graph_top5/5I_top5.png\n",
      "[OK] Saved plot: sequence_merge_method/5_histogram/graph_top5/6S_top5.png\n",
      "[OK] Saved plot: sequence_merge_method/5_histogram/graph_top5/7T_top5.png\n",
      "[OK] Wrote summary CSV: sequence_merge_method/5_histogram/graph_top5/top5_summary_all_files.csv\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------- Path setup ----------------\n",
    "histogram_folder = \"sequence_merge_method/5_histogram\"\n",
    "summary_folder = os.path.join(histogram_folder, \"graph_top5\")\n",
    "os.makedirs(summary_folder, exist_ok=True)\n",
    "\n",
    "# ---------------- Expected RNAME highlight mapping ----------------\n",
    "highlight_mapping = {\n",
    "    \"0N\": \"seq_013_00001101\",\n",
    "    \"1D\": \"seq_035_00100011\",\n",
    "    \"2S\": \"seq_082_01010010\",\n",
    "    \"3G\": \"seq_102_01100110\",\n",
    "    \"4I\": \"seq_136_10001000\",\n",
    "    \"5S\": \"seq_178_10110010\",\n",
    "    \"6T\": \"seq_211_11010011\",\n",
    "    \"5K\": \"seq_170_10101010\",\n",
    "    \"1X8\": \"seq_255_11111111\",\n",
    "    \"0X8\": \"seq_000_00000000\",\n",
    "    \"4G\": \"seq_134_10000110\",\n",
    "    \"5I\": \"seq_168_10101000\",\n",
    "    \"6S\": \"seq_210_11010010\",\n",
    "    \"7T\": \"seq_243_11110011\",\n",
    "    \"3SP26\": \"seq_122_01111010\",\n",
    "    \"3SP31\": \"seq_127_01111111\",\n",
    "}\n",
    "\n",
    "# ---------------- Collect matching CSV files (support both uppercase/lowercase) ----------------\n",
    "patterns = [\n",
    "    os.path.join(histogram_folder, \"histogram_DNA_Data_*_assemble.csv\"),\n",
    "    os.path.join(histogram_folder, \"Histogram_DNA_Data_*_assemble.csv\"),\n",
    "]\n",
    "csv_files = sorted({p for pat in patterns for p in glob(pat)})\n",
    "\n",
    "print(f\"[INFO] Matched files: {len(csv_files)}\")\n",
    "for f in csv_files:\n",
    "    print(\"  -\", os.path.basename(f))\n",
    "if not csv_files:\n",
    "    print(\"[WARN] No files matched. Check folder path or filename pattern.\")\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "for file_path in csv_files:\n",
    "    file_name = os.path.basename(file_path)\n",
    "\n",
    "    # Extract sample key from filename (histogram_DNA_Data_<SAMPLE>_assemble.csv)\n",
    "    m = re.match(r\"[Hh]istogram_DNA_Data_([A-Za-z0-9]+)_assemble\\.csv\", file_name)\n",
    "    sample_key = m.group(1) if m else None\n",
    "    if not sample_key:\n",
    "        print(f\"[WARN] Failed to extract sample key from {file_name}; highlight may not apply.\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        cols = df.columns.tolist()\n",
    "\n",
    "        # Determine sorting column (Normalized_Count first, fallback to Count)\n",
    "        sort_col = None\n",
    "        if \"Normalized_Count\" in df.columns:\n",
    "            sort_col = \"Normalized_Count\"\n",
    "        elif \"Count\" in df.columns:\n",
    "            sort_col = \"Count\"\n",
    "\n",
    "        if \"RNAME\" not in df.columns or sort_col is None:\n",
    "            print(f\"[SKIP] {file_name}: Missing RNAME or Count/Normalized_Count. Columns: {cols}\")\n",
    "            continue\n",
    "\n",
    "        # Top 5 entries\n",
    "        top_df = df.sort_values(by=sort_col, ascending=False).head(5).reset_index(drop=True)\n",
    "\n",
    "        # Lookup expected RNAME for highlight\n",
    "        expected_rname = highlight_mapping.get(sample_key)\n",
    "\n",
    "        # -------- Plotting (keeps original visual style) --------\n",
    "        fig, ax = plt.subplots(figsize=(7, 4))\n",
    "        bar_labels = top_df[\"RNAME\"].tolist()\n",
    "        values = top_df[sort_col].tolist()\n",
    "\n",
    "        # Highlight color assignment\n",
    "        colors = []\n",
    "        for r in bar_labels:\n",
    "            if expected_rname and r == expected_rname:\n",
    "                colors.append(\"tab:orange\")\n",
    "            else:\n",
    "                colors.append(\"tab:blue\")\n",
    "\n",
    "        ax.bar(range(len(bar_labels)), values, tick_label=bar_labels, color=colors)\n",
    "        ax.set_ylabel(sort_col)\n",
    "        ax.set_title(f\"Top5 RNAMEs — {sample_key}\")\n",
    "\n",
    "        # Highlight marker (*)\n",
    "        if expected_rname and expected_rname in bar_labels:\n",
    "            idx = bar_labels.index(expected_rname)\n",
    "            ax.annotate(\"*\",\n",
    "                        xy=(idx, values[idx]),\n",
    "                        xytext=(0, 5),\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha=\"center\", va=\"bottom\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "        plt.xticks(rotation=20, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        out_png = os.path.join(summary_folder, f\"{sample_key}_top5.png\")\n",
    "        plt.savefig(out_png, dpi=200)\n",
    "        plt.close(fig)\n",
    "        print(f\"[OK] Saved plot: {out_png}\")\n",
    "\n",
    "        # Append summary data\n",
    "        for rank, row in top_df.iterrows():\n",
    "            summary_rows.append({\n",
    "                \"file\": file_name,\n",
    "                \"sample_key\": sample_key,\n",
    "                \"rank\": rank + 1,\n",
    "                \"RNAME\": row[\"RNAME\"],\n",
    "                sort_col: row[sort_col],\n",
    "                \"is_expected\": (expected_rname == row[\"RNAME\"]) if expected_rname else False\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {file_name}: {e}\")\n",
    "\n",
    "# Save summary CSV\n",
    "if summary_rows:\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    out_csv = os.path.join(summary_folder, \"top5_summary_all_files.csv\")\n",
    "    summary_df.to_csv(out_csv, index=False)\n",
    "    print(f\"[OK] Wrote summary CSV: {out_csv}\")\n",
    "else:\n",
    "    print(\"[WARN] No data available to write.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f3964f",
   "metadata": {},
   "source": [
    "## C. Summarize Highlighted Read Counts into a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70c41e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Highlight summary saved to: sequence_merge_method/6_summary/highlight_result.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# === Highlight Mapping (associates sample prefixes with their expected RNAME) ===\n",
    "highlight_mapping = {\n",
    "    \"0N\": \"seq_013_00001101\",\n",
    "    \"1D\": \"seq_035_00100011\",\n",
    "    \"2S\": \"seq_082_01010010\",\n",
    "    \"3G\": \"seq_102_01100110\",\n",
    "    \"4I\": \"seq_136_10001000\",\n",
    "    \"5S\": \"seq_178_10110010\",\n",
    "    \"6T\": \"seq_211_11010011\",\n",
    "    \"5K\": \"seq_170_10101010\",\n",
    "    \"1X8\": \"seq_255_11111111\",\n",
    "    \"0X8\": \"seq_000_00000000\",\n",
    "    \"4G\": \"seq_134_10000110\",\n",
    "    \"5I\": \"seq_168_10101000\",\n",
    "    \"6S\": \"seq_210_11010010\",\n",
    "    \"7T\": \"seq_243_11110011\",\n",
    "    \"3SP26\": \"seq_122_01111010\",\n",
    "    \"3SP31\": \"seq_127_01111111\"\n",
    "}\n",
    "\n",
    "# === Folders ===\n",
    "histogram_folder = \"sequence_merge_method/5_histogram\"\n",
    "summary_folder = \"sequence_merge_method/6_summary\"\n",
    "os.makedirs(summary_folder, exist_ok=True)\n",
    "highlight_result_csv = os.path.join(summary_folder, \"highlight_result.csv\")\n",
    "\n",
    "# === Helpers ===\n",
    "def canonicalize_rname(x: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize RNAME to a canonical form without zero-padding in the index part.\n",
    "    Examples:\n",
    "      'seq_0013_00001101' -> 'seq_13_00001101'\n",
    "      'seq_013_00001101'  -> 'seq_13_00001101'\n",
    "      'seq_13_00001101'   -> 'seq_13_00001101'\n",
    "    If pattern doesn't match, return original stripped string.\n",
    "    \"\"\"\n",
    "    s = str(x).strip()\n",
    "    m = re.fullmatch(r\"seq_(\\d+)_([01]+)\", s)\n",
    "    if not m:\n",
    "        return s\n",
    "    idx = int(m.group(1))   # remove leading zeros\n",
    "    bits = m.group(2)\n",
    "    return f\"seq_{idx}_{bits}\"\n",
    "\n",
    "# Pre-normalize mapping so it matches canonicalized RNAMEs in CSVs\n",
    "normalized_mapping = {k: canonicalize_rname(v) for k, v in highlight_mapping.items()}\n",
    "\n",
    "def extract_prefix_from_filename(file: str) -> str:\n",
    "    \"\"\"\n",
    "    Robustly extract the sample prefix from filenames like:\n",
    "      'histogram_DNA_Data_0N_assemble.csv' -> '0N'\n",
    "      'histogram_DNA_Data_3SP26_assemble.csv' -> '3SP26'\n",
    "    Strategy: take the token right before 'assemble'.\n",
    "    \"\"\"\n",
    "    name = os.path.basename(file)\n",
    "    if name.startswith(\"histogram_\"):\n",
    "        name = name[len(\"histogram_\"):]\n",
    "    if name.endswith(\".csv\"):\n",
    "        name = name[:-4]\n",
    "    tokens = name.split(\"_\")\n",
    "    # find token before 'assemble'\n",
    "    try:\n",
    "        i = tokens.index(\"assemble\")\n",
    "        if i - 1 >= 0:\n",
    "            return tokens[i - 1]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    # fallback heuristic\n",
    "    return tokens[2] if len(tokens) > 2 else (tokens[-1] if tokens else \"\")\n",
    "\n",
    "# === Collect Highlight Summary Information ===\n",
    "highlight_data = []\n",
    "csv_files = [f for f in os.listdir(histogram_folder)\n",
    "             if f.startswith(\"histogram_\") and f.endswith(\".csv\")]\n",
    "\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(histogram_folder, file)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Ensure required columns exist\n",
    "        if \"RNAME\" not in df.columns or \"Count\" not in df.columns:\n",
    "            raise ValueError(f\"Required columns 'RNAME' and 'Count' not found in {file}\")\n",
    "\n",
    "        # Canonicalize RNAMEs for reliable matching\n",
    "        df[\"RNAME\"] = df[\"RNAME\"].map(canonicalize_rname)\n",
    "\n",
    "        # Extract prefix robustly and get normalized highlight RNAME\n",
    "        prefix = extract_prefix_from_filename(file)\n",
    "        highlight_rname = normalized_mapping.get(prefix, \"\")\n",
    "\n",
    "        # Ensure Count is integer\n",
    "        df[\"Count\"] = pd.to_numeric(df[\"Count\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "        total_count = int(df[\"Count\"].sum())\n",
    "\n",
    "        # Highlight stats\n",
    "        highlight_count = int(df.loc[df[\"RNAME\"] == highlight_rname, \"Count\"].sum()) if highlight_rname else 0\n",
    "        highlight_percentage = (highlight_count / total_count * 100.0) if total_count > 0 else 0.0\n",
    "\n",
    "        # Ratio vs second top\n",
    "        sorted_counts = df[\"Count\"].sort_values(ascending=False).to_list()\n",
    "        second_max_count = sorted_counts[1] if len(sorted_counts) >= 2 else 0\n",
    "        highlight_vs_second_ratio = (highlight_count / second_max_count) if second_max_count > 0 else 0.0\n",
    "\n",
    "        # Keep the same 'File' field shape you used before (without the 'histogram_' prefix)\n",
    "        file_name = file.replace(\"histogram_\", \"\")\n",
    "\n",
    "        highlight_data.append([\n",
    "            file_name,\n",
    "            highlight_count,\n",
    "            total_count,\n",
    "            highlight_percentage,\n",
    "            highlight_rname,\n",
    "            highlight_vs_second_ratio\n",
    "        ])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing file '{file}': {e}\")\n",
    "\n",
    "# === Save the Summary to a CSV File (same columns/order as your original) ===\n",
    "highlight_df = pd.DataFrame(highlight_data, columns=[\n",
    "    'File',\n",
    "    'Highlight_Count',\n",
    "    'Total_Count',\n",
    "    'Highlight_Percentage',\n",
    "    'Highlight_RNAME',\n",
    "    'Highlight_vs_SecondTop_Ratio'\n",
    "])\n",
    "highlight_df = highlight_df.sort_values(by='File')\n",
    "highlight_df.to_csv(highlight_result_csv, index=False)\n",
    "\n",
    "print(f\"📌 Highlight summary saved to: {highlight_result_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfd47fc",
   "metadata": {},
   "source": [
    "## D. Plot Stacked Bar Graph top5_gray_rest_white_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a70fd38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 8 samples.\n",
      "✅ Saved outputs:\n",
      " - sequence_merge_method/6_summary/stacked_bar_rank_color_RGB.png\n",
      " - sequence_merge_method/6_summary/stacked_bar_rank_color_RGB.svg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ---------------- Paths ----------------\n",
    "histogram_folder = \"sequence_merge_method/5_histogram\"\n",
    "summary_folder = \"sequence_merge_method/6_summary\"\n",
    "os.makedirs(summary_folder, exist_ok=True)\n",
    "\n",
    "# ---------------- Expected RNAME map (by sample prefix) ----------------\n",
    "highlight_mapping = {\n",
    "    \"0N\": \"seq_013_00001101\",\n",
    "    \"1D\": \"seq_035_00100011\",\n",
    "    \"2S\": \"seq_082_01010010\",\n",
    "    \"3G\": \"seq_102_01100110\",\n",
    "    \"4I\": \"seq_136_10001000\",\n",
    "    \"5S\": \"seq_178_10110010\",\n",
    "    \"6T\": \"seq_211_11010011\",\n",
    "    \"5K\": \"seq_170_10101010\",\n",
    "    \"1X8\": \"seq_255_11111111\",\n",
    "    \"0X8\": \"seq_000_00000000\",\n",
    "    \"4G\": \"seq_134_10000110\",\n",
    "    \"5I\": \"seq_168_10101000\",\n",
    "    \"6S\": \"seq_210_11010010\",\n",
    "    \"7T\": \"seq_243_11110011\",\n",
    "    \"3SP26\": \"seq_122_01111010\",\n",
    "    \"3SP31\": \"seq_127_01111111\",\n",
    "}\n",
    "\n",
    "# ---------------- Visual options ----------------\n",
    "# If True, use only the prefix (e.g., 0N, 1D) as x-axis label; else use full sample name.\n",
    "USE_PREFIX_LABELS = True\n",
    "\n",
    "# Rank-based colors: 1st=red, 2nd~4th gray shades (RGB 0–1), 5+=white merged\n",
    "RANK_COLORS = [\n",
    "    \"red\",              # 1st\n",
    "    (0.30, 0.30, 0.30), # 2nd\n",
    "    (0.5, 0.5, 0.5), # 3rd\n",
    "    (0.7, 0.7, 0.7), # 4th\n",
    "]\n",
    "\n",
    "# ---------------- Load per-sample data ----------------\n",
    "sample_rname_dfs = {}\n",
    "for file_name in sorted(os.listdir(histogram_folder)):\n",
    "    if not (file_name.lower().startswith(\"histogram_\") and file_name.endswith(\".csv\")):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(histogram_folder, file_name)\n",
    "    sample_name = file_name.replace(\"histogram_\", \"\").replace(\".csv\", \"\")  # e.g., DNA_Data_0N_assemble\n",
    "\n",
    "    # robust prefix extraction: token right before \"_assemble\"\n",
    "    m = re.search(r\"([A-Za-z0-9]+)(?=_assemble$)\", sample_name)\n",
    "    prefix = m.group(1) if m else None\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to read {file_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if \"RNAME\" not in df.columns or \"Count\" not in df.columns:\n",
    "        print(f\"[SKIP] {file_name}: missing RNAME/Count columns.\")\n",
    "        continue\n",
    "\n",
    "    # Normalize safely (avoid zero division)\n",
    "    df[\"Count\"] = df[\"Count\"].astype(int)\n",
    "    total = df[\"Count\"].sum()\n",
    "    if total == 0:\n",
    "        print(f\"[SKIP] {file_name}: total Count is zero.\")\n",
    "        continue\n",
    "\n",
    "    df[\"Normalized_Count\"] = df[\"Count\"] / total\n",
    "    df = df.sort_values(by=\"Count\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    sample_rname_dfs[sample_name] = (prefix, df)\n",
    "\n",
    "if not sample_rname_dfs:\n",
    "    print(\"[WARN] No valid histogram CSV files found. Check folder and filenames.\")\n",
    "else:\n",
    "    print(f\"[INFO] Loaded {len(sample_rname_dfs)} samples.\")\n",
    "\n",
    "# ---------------- Plot ----------------\n",
    "fig, ax = plt.subplots(figsize=(24, 12))\n",
    "\n",
    "for sample_name, (prefix, df) in sample_rname_dfs.items():\n",
    "    # expected highlight RNAME by prefix (may be None if prefix not in map)\n",
    "    highlight_rname = highlight_mapping.get(prefix)\n",
    "\n",
    "    # split top4 and the rest\n",
    "    top4 = df.head(4).copy()\n",
    "    rest_sum = df[\"Normalized_Count\"].iloc[4:].sum() if len(df) > 4 else 0.0\n",
    "\n",
    "    # x label\n",
    "    x_label = prefix if (USE_PREFIX_LABELS and prefix) else sample_name\n",
    "\n",
    "    bottom = 0.0\n",
    "    for i, row in top4.iterrows():\n",
    "        height = float(row[\"Normalized_Count\"])\n",
    "        rname = str(row[\"RNAME\"]).strip()\n",
    "\n",
    "        # 1st should be red if it equals expected highlight; otherwise still rank color\n",
    "        if highlight_rname and rname == highlight_rname:\n",
    "            bar_color = RANK_COLORS[0]\n",
    "        else:\n",
    "            # i: 0..3  →  choose rank color slot\n",
    "            bar_color = RANK_COLORS[i if i < len(RANK_COLORS) else -1]\n",
    "\n",
    "        ax.bar(\n",
    "            x_label,\n",
    "            height,\n",
    "            bottom=bottom,\n",
    "            color=bar_color,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.2,\n",
    "        )\n",
    "        bottom += height\n",
    "\n",
    "    if rest_sum > 0:\n",
    "        ax.bar(\n",
    "            x_label,\n",
    "            rest_sum,\n",
    "            bottom=bottom,\n",
    "            color=\"white\",\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.2,\n",
    "        )\n",
    "\n",
    "# reference line and styling\n",
    "ax.axhline(y=0.5, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "ax.set_ylabel(\"Normalized Count\", fontsize=20)\n",
    "ax.set_xlabel(\"Sample\", fontsize=20)\n",
    "ax.set_title(\n",
    "    \"Stacked Bar Chart (1st=Red, 2nd-4th=Gray Shades, Other=White)\",\n",
    "    fontsize=18,\n",
    ")\n",
    "ax.tick_params(axis=\"x\", labelsize=18)\n",
    "ax.tick_params(axis=\"y\", labelsize=18)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# ---------------- Save ----------------\n",
    "png_path = os.path.join(summary_folder, \"stacked_bar_rank_color_RGB.png\")\n",
    "svg_path = os.path.join(summary_folder, \"stacked_bar_rank_color_RGB.svg\")\n",
    "plt.savefig(png_path, dpi=300)\n",
    "plt.savefig(svg_path)\n",
    "# pdf_path = os.path.join(summary_folder, \"stacked_bar_rank_color_RGB.pdf\"); plt.savefig(pdf_path)\n",
    "plt.close()\n",
    "\n",
    "print(\"✅ Saved outputs:\")\n",
    "print(\" -\", png_path)\n",
    "print(\" -\", svg_path)\n",
    "# print(\" -\", pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c92c29a",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0303529c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Analyzing a total of 8 files.\n",
      "✅ histogram_DNA_Data_0N_assemble.csv processing complete.\n",
      "✅ histogram_DNA_Data_1D_assemble.csv processing complete.\n",
      "✅ histogram_DNA_Data_2S_assemble.csv processing complete.\n",
      "✅ histogram_DNA_Data_3SP26_assemble.csv processing complete.\n",
      "✅ histogram_DNA_Data_4G_assemble.csv processing complete.\n",
      "✅ histogram_DNA_Data_5I_assemble.csv processing complete.\n",
      "✅ histogram_DNA_Data_6S_assemble.csv processing complete.\n",
      "✅ histogram_DNA_Data_7T_assemble.csv processing complete.\n",
      "\n",
      "--- Final Combined Analysis Results (including summary row) ---\n",
      "                            File_Name  Total_Count  Avg_Accuracy\n",
      "4  histogram_DNA_Data_4G_assemble.csv         48.0      0.932292\n",
      "5  histogram_DNA_Data_5I_assemble.csv         46.0      0.904891\n",
      "6  histogram_DNA_Data_6S_assemble.csv         43.0      0.930233\n",
      "7  histogram_DNA_Data_7T_assemble.csv         42.0      0.904762\n",
      "8                       Avg. Accuracy          NaN      0.921023\n",
      "\n",
      "📄 All analysis results and the summary row have been saved to 'sequence_merge_method/6_summary/summary_combined.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Set Answer Key (same as before) ---\n",
    "answer_data = {\n",
    "    \"0N\": \"seq_013_00001101\", \"1D\": \"seq_035_00100011\",\n",
    "    \"2S\": \"seq_082_01010010\", \"3G\": \"seq_102_01100110\",\n",
    "    \"4I\": \"seq_136_10001000\", \"5S\": \"seq_178_10110010\",\n",
    "    \"6T\": \"seq_211_11010011\", \"5K\": \"seq_170_10101010\",\n",
    "    \"1X8\": \"seq_255_11111111\", \"0X8\": \"seq_000_00000000\",\n",
    "    \"4G\": \"seq_134_10000110\", \"5I\": \"seq_168_10101000\",\n",
    "    \"6S\": \"seq_210_11010010\", \"7T\": \"seq_243_11110011\",\n",
    "    \"3SP26\": \"seq_122_01111010\", \"3SP31\": \"seq_127_01111111\"\n",
    "}\n",
    "# Extract only the 8-digit number for convenience and create answer_key_map\n",
    "# Result example: {'0N': '00001101', '1D': '00100011', ...}\n",
    "answer_key_map = {key: re.search(r'([01]{8})$', value).group(1) for key, value in answer_data.items()}\n",
    "\n",
    "\n",
    "# --- 2. File Processing and Combined Calculation (same as before) ---\n",
    "input_folder = \"sequence_merge_method/5_histogram\"\n",
    "output_folder = \"sequence_merge_method/6_summary\"\n",
    "output_path = os.path.join(output_folder, \"summary_combined.csv\")\n",
    "\n",
    "try:\n",
    "    files = [f for f in os.listdir(input_folder) if f.endswith('.csv') and f.startswith(\"histogram_\")]\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: Folder '{input_folder}' not found.\")\n",
    "    files = []\n",
    "\n",
    "combined_summary_list = []\n",
    "\n",
    "if not files:\n",
    "    print(\"⚠️ No files found for analysis.\")\n",
    "else:\n",
    "    print(f\"📁 Analyzing a total of {len(files)} files.\")\n",
    "    for file_name in sorted(files):\n",
    "        try:\n",
    "            df = pd.read_csv(os.path.join(input_folder, file_name))\n",
    "            total_count = df['Count'].sum()\n",
    "            \n",
    "            position_counts = [{'0': 0, '1': 0} for _ in range(8)]\n",
    "            for _, row in df.iterrows():\n",
    "                rname = row.get('RNAME', '')\n",
    "                count = int(row['Count'])\n",
    "                match = re.search(r'seq_[^_]+_([01]{8})', rname)\n",
    "                if match:\n",
    "                    eight_digits = match.group(1)\n",
    "                    for i, digit in enumerate(eight_digits):\n",
    "                        position_counts[i][digit] += count\n",
    "            \n",
    "            answer_key = None\n",
    "            for key in answer_key_map:\n",
    "                if key in file_name:\n",
    "                    answer_key = answer_key_map[key]\n",
    "                    break\n",
    "            \n",
    "            combined_row = {\n",
    "                \"File_Name\": file_name,\n",
    "                \"Total_Count\": total_count\n",
    "            }\n",
    "            \n",
    "            accuracies_for_avg = [] \n",
    "            for i in range(8):\n",
    "                zeros_count = position_counts[i]['0']\n",
    "                ones_count = position_counts[i]['1']\n",
    "                \n",
    "                combined_row[f\"Pos{i+1}_Zeros_Count\"] = zeros_count\n",
    "                combined_row[f\"Pos{i+1}_Ones_Count\"] = ones_count\n",
    "                \n",
    "                accuracy = np.nan\n",
    "                if answer_key:\n",
    "                    correct_digit = answer_key[i]\n",
    "                    current_total = zeros_count + ones_count\n",
    "                    correct_count = position_counts[i][correct_digit]\n",
    "                    accuracy = correct_count / current_total if current_total > 0 else 0\n",
    "                \n",
    "                combined_row[f\"Pos{i+1}_Accuracy\"] = accuracy\n",
    "                accuracies_for_avg.append(accuracy)\n",
    "\n",
    "            avg_accuracy = np.nanmean(accuracies_for_avg)\n",
    "            combined_row[\"Avg_Accuracy\"] = avg_accuracy\n",
    "\n",
    "            combined_summary_list.append(combined_row)\n",
    "            if not answer_key:\n",
    "                print(f\"⚠️ Warning: Answer key not found for {file_name}.\")\n",
    "            print(f\"✅ {file_name} processing complete.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file_name}: {e}\")\n",
    "\n",
    "# --- 3. Save Combined Results and Add Summary Row (Modified Section) ---\n",
    "if combined_summary_list:\n",
    "    combined_df = pd.DataFrame(combined_summary_list)\n",
    "    \n",
    "    column_order = ['File_Name', 'Total_Count', 'Avg_Accuracy']\n",
    "    for i in range(1, 9):\n",
    "        column_order.append(f'Pos{i}_Zeros_Count')\n",
    "        column_order.append(f'Pos{i}_Ones_Count')\n",
    "        column_order.append(f'Pos{i}_Accuracy')\n",
    "        \n",
    "    final_df = combined_df[[col for col in column_order if col in combined_df.columns]]\n",
    "    \n",
    "    # ★★★ Create Summary Row (Modified Logic) ★★★\n",
    "    summary_row = {'File_Name': 'Avg. Accuracy'}\n",
    "    # Find and iterate over all columns containing 'Accuracy'\n",
    "    for col_name in final_df.columns:\n",
    "        if 'Accuracy' in col_name:\n",
    "            # Calculate the mean of that Accuracy column and add to the summary row\n",
    "            summary_row[col_name] = final_df[col_name].mean()\n",
    "\n",
    "    # ★★★ Add Summary Row to the existing DataFrame ★★★\n",
    "    summary_row_df = pd.DataFrame([summary_row]) # Convert dictionary to DataFrame\n",
    "    final_df = pd.concat([final_df, summary_row_df], ignore_index=True)\n",
    "\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(\"\\n--- Final Combined Analysis Results (including summary row) ---\")\n",
    "    print(final_df[['File_Name', 'Total_Count', 'Avg_Accuracy']].tail())\n",
    "    print(f\"\\n📄 All analysis results and the summary row have been saved to '{output_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3679660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
