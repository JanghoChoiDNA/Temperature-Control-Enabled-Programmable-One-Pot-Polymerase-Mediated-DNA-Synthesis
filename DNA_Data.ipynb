{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "926b74f3",
   "metadata": {},
   "source": [
    "# 1. Install Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5867155f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: biopython in /usr/local/lib/python3.12/dist-packages (1.84)\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (from biopython) (1.26.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "fastp is already the newest version (0.23.4+dfsg-1).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  pigz python3-xopen\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 242 not upgraded.\n",
      "Get:1 https://packages.microsoft.com/repos/code stable InRelease [3,590 B]     \n",
      "Get:2 https://packages.microsoft.com/repos/code stable/main armhf Packages [20.3 kB]\n",
      "Hit:3 http://ports.ubuntu.com/ubuntu-ports noble InRelease                     \n",
      "Get:4 https://packages.microsoft.com/repos/code stable/main amd64 Packages [20.2 kB]\n",
      "Get:5 http://ports.ubuntu.com/ubuntu-ports noble-updates InRelease [126 kB]    \n",
      "Get:6 https://packages.microsoft.com/repos/code stable/main arm64 Packages [20.3 kB]\n",
      "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu noble InRelease   \n",
      "Get:8 http://ports.ubuntu.com/ubuntu-ports noble-backports InRelease [126 kB]\n",
      "Get:9 http://ports.ubuntu.com/ubuntu-ports noble-security InRelease [126 kB]\n",
      "Get:10 http://ports.ubuntu.com/ubuntu-ports noble-updates/main arm64 Packages [1,403 kB]\n",
      "Get:11 http://ports.ubuntu.com/ubuntu-ports noble-updates/main Translation-en [269 kB]\n",
      "Get:12 http://ports.ubuntu.com/ubuntu-ports noble-updates/main arm64 Components [172 kB]\n",
      "Get:13 http://ports.ubuntu.com/ubuntu-ports noble-updates/restricted arm64 Packages [2,320 kB]\n",
      "Get:14 http://ports.ubuntu.com/ubuntu-ports noble-updates/restricted Translation-en [384 kB]\n",
      "Get:15 http://ports.ubuntu.com/ubuntu-ports noble-updates/restricted arm64 Components [212 B]\n",
      "Get:16 http://ports.ubuntu.com/ubuntu-ports noble-updates/universe arm64 Packages [1,101 kB]\n",
      "Get:17 http://ports.ubuntu.com/ubuntu-ports noble-updates/universe Translation-en [288 kB]\n",
      "Get:18 http://ports.ubuntu.com/ubuntu-ports noble-updates/universe arm64 Components [376 kB]\n",
      "Get:19 http://ports.ubuntu.com/ubuntu-ports noble-updates/multiverse arm64 Components [212 B]\n",
      "Get:20 http://ports.ubuntu.com/ubuntu-ports noble-backports/main arm64 Components [3,580 B]\n",
      "Get:21 http://ports.ubuntu.com/ubuntu-ports noble-backports/restricted arm64 Components [216 B]\n",
      "Get:22 http://ports.ubuntu.com/ubuntu-ports noble-backports/universe arm64 Components [19.2 kB]\n",
      "Get:23 http://ports.ubuntu.com/ubuntu-ports noble-backports/multiverse arm64 Components [212 B]\n",
      "Get:24 http://ports.ubuntu.com/ubuntu-ports noble-security/main arm64 Packages [1,136 kB]\n",
      "Get:25 http://ports.ubuntu.com/ubuntu-ports noble-security/main Translation-en [187 kB]\n",
      "Get:26 http://ports.ubuntu.com/ubuntu-ports noble-security/main arm64 Components [18.4 kB]\n",
      "Get:27 http://ports.ubuntu.com/ubuntu-ports noble-security/restricted arm64 Packages [2,181 kB]\n",
      "Get:28 http://ports.ubuntu.com/ubuntu-ports noble-security/restricted Translation-en [361 kB]\n",
      "Get:29 http://ports.ubuntu.com/ubuntu-ports noble-security/restricted arm64 Components [212 B]\n",
      "Get:30 http://ports.ubuntu.com/ubuntu-ports noble-security/universe arm64 Packages [859 kB]\n",
      "Get:31 http://ports.ubuntu.com/ubuntu-ports noble-security/universe Translation-en [195 kB]\n",
      "Get:32 http://ports.ubuntu.com/ubuntu-ports noble-security/universe arm64 Components [52.3 kB]\n",
      "Get:33 http://ports.ubuntu.com/ubuntu-ports noble-security/multiverse arm64 Components [212 B]\n",
      "Fetched 11.8 MB in 5s (2,219 kB/s)                                      \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "flash is already the newest version (1.2.11-2).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  pigz python3-xopen\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 264 not upgraded.\n",
      "Requirement already satisfied: cutadapt in /usr/local/lib/python3.12/dist-packages (5.0)\n",
      "Requirement already satisfied: dnaio>=1.2.3 in /usr/local/lib/python3.12/dist-packages (from cutadapt) (1.2.3)\n",
      "Requirement already satisfied: xopen>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from cutadapt) (2.0.2)\n",
      "Requirement already satisfied: isal>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from xopen>=1.6.0->cutadapt) (1.7.1)\n",
      "Requirement already satisfied: zlib-ng>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from xopen>=1.6.0->cutadapt) (0.5.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "bwa is already the newest version (0.7.17-7).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  pigz python3-xopen\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 264 not upgraded.\n",
      "Requirement already satisfied: pysam in /usr/local/lib/python3.12/dist-packages (0.22.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "samtools is already the newest version (1.19.2-1build2).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  pigz python3-xopen\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 264 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# Bioinformatics Tools (Ubuntu)\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y fastp flash bwa samtools\n",
    "\n",
    "# Python Library\n",
    "!pip3 install biopython cutadapt pysam --break-system-packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0d9529",
   "metadata": {},
   "source": [
    "# 2 Trimming and Discard trimmed sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e1b17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrimmed sequences saved: sequence_merge_method/A_untrimmed_output/6S_1_250821_batch17_07_R1_untrimmed.fastq.gz, sequence_merge_method/A_untrimmed_output/6S_1_250821_batch17_07_R2_untrimmed.fastq.gz\n",
      "Untrimmed sequences saved: sequence_merge_method/A_untrimmed_output/5I_1_250821_batch17_06_R1_untrimmed.fastq.gz, sequence_merge_method/A_untrimmed_output/5I_1_250821_batch17_06_R2_untrimmed.fastq.gz\n",
      "Untrimmed sequences saved: sequence_merge_method/A_untrimmed_output/7T_1_250821_batch17_08_R1_untrimmed.fastq.gz, sequence_merge_method/A_untrimmed_output/7T_1_250821_batch17_08_R2_untrimmed.fastq.gz\n",
      "Untrimmed sequences saved: sequence_merge_method/A_untrimmed_output/3SP26_250828_batch18_04_R1_untrimmed.fastq.gz, sequence_merge_method/A_untrimmed_output/3SP26_250828_batch18_04_R2_untrimmed.fastq.gz\n",
      "Untrimmed sequences saved: sequence_merge_method/A_untrimmed_output/0N_1_250821_batch17_02_R1_untrimmed.fastq.gz, sequence_merge_method/A_untrimmed_output/0N_1_250821_batch17_02_R2_untrimmed.fastq.gz\n",
      "Untrimmed sequences saved: sequence_merge_method/A_untrimmed_output/2S_1_250821_batch17_03_R1_untrimmed.fastq.gz, sequence_merge_method/A_untrimmed_output/2S_1_250821_batch17_03_R2_untrimmed.fastq.gz\n",
      "Untrimmed sequences saved: sequence_merge_method/A_untrimmed_output/4G_1_250821_batch17_05_R1_untrimmed.fastq.gz, sequence_merge_method/A_untrimmed_output/4G_1_250821_batch17_05_R2_untrimmed.fastq.gz\n",
      "Untrimmed sequences saved: sequence_merge_method/A_untrimmed_output/1D_1_250821_batch17_01_R1_untrimmed.fastq.gz, sequence_merge_method/A_untrimmed_output/1D_1_250821_batch17_01_R2_untrimmed.fastq.gz\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Specify the folder containing your input files.\n",
    "input_folder = \"fastq\"\n",
    "# Specify the folder where you want to save the untrimmed (adapter-free) sequences.\n",
    "untrimmed_output_folder = \"sequence_merge_method/A_untrimmed_output\"\n",
    "\n",
    "# Define the adapter sequences for R1 and R2.\n",
    "adapter_sequence_r1 = \"AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC\"\n",
    "adapter_sequence_r2 = \"AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT\"\n",
    "\n",
    "# Use glob to get a list of all input file pairs (R1 and R2) in the folder.\n",
    "input_file_pairs = []\n",
    "for input_r1 in glob.glob(os.path.join(input_folder, \"*_R1.fastq.gz\")):\n",
    "    # Assuming R2 files have the same naming format as R1 files.\n",
    "    input_r2 = input_r1.replace(\"_R1.fastq.gz\", \"_R2.fastq.gz\")\n",
    "    if os.path.exists(input_r2):  # Ensure R2 file exists.\n",
    "        input_file_pairs.append({\"r1\": input_r1, \"r2\": input_r2})\n",
    "\n",
    "# Create the output folder if it doesn't exist.\n",
    "os.makedirs(untrimmed_output_folder, exist_ok=True)\n",
    "\n",
    "for input_files in input_file_pairs:\n",
    "    input_r1 = input_files[\"r1\"]\n",
    "    input_r2 = input_files[\"r2\"]\n",
    "\n",
    "    # Define output file paths for untrimmed (clean, adapter-free) sequences.\n",
    "    untrimmed_r1 = os.path.join(untrimmed_output_folder, os.path.basename(input_r1).replace(\".fastq.gz\", \"_untrimmed.fastq.gz\"))\n",
    "    untrimmed_r2 = os.path.join(untrimmed_output_folder, os.path.basename(input_r2).replace(\".fastq.gz\", \"_untrimmed.fastq.gz\"))\n",
    "\n",
    "    # Use cutadapt to keep only untrimmed sequences (completely adapter-free).\n",
    "    result = subprocess.run([\n",
    "        \"cutadapt\",\n",
    "        \"-a\", adapter_sequence_r1,  # Adapter for R1\n",
    "        \"-A\", adapter_sequence_r2,  # Adapter for R2\n",
    "        \"-O\", \"15\",                  # Minimum overlap for adapter trimming\n",
    "        \"--discard-trimmed\",         # Discard sequences where trimming occurred\n",
    "        \"-o\", untrimmed_r1,          # Save only untrimmed R1 reads\n",
    "        \"-p\", untrimmed_r2,          # Save only untrimmed R2 reads\n",
    "        input_r1, input_r2\n",
    "    ], capture_output=True, text=True)\n",
    "\n",
    "    # Log the result.\n",
    "    if result.returncode == 0:\n",
    "        print(f\"Untrimmed sequences saved: {untrimmed_r1}, {untrimmed_r2}\")\n",
    "    else:\n",
    "        print(f\"Error processing {input_r1} and {input_r2}:\\n{result.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7b17fe",
   "metadata": {},
   "source": [
    "# 3. Q filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b123bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n",
      "Read1 before filtering:\n",
      "total reads: 721\n",
      "total bases: 108871\n",
      "Q20 bases: 94451(86.755%)\n",
      "Q30 bases: 81732(75.0723%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 236\n",
      "total bases: 35636\n",
      "Q20 bases: 34651(97.2359%)\n",
      "Q30 bases: 32101(90.0803%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 236\n",
      "reads failed due to low quality: 485\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 23.8558%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/0N_1_250821_batch17_02_R2_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/0N_1_250821_batch17_02_R2_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/0N_1_250821_batch17_02_R2_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/0N_1_250821_batch17_02_R2_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/0N_1_250821_batch17_02_R2_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/0N_1_250821_batch17_02_R2_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for 0N_1_250821_batch17_02_R2_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/0N_1_250821_batch17_02_R2_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/0N_1_250821_batch17_02_R2_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/0N_1_250821_batch17_02_R2_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 825\n",
      "total bases: 124575\n",
      "Q20 bases: 106281(85.3149%)\n",
      "Q30 bases: 92297(74.0895%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 227\n",
      "total bases: 34277\n",
      "Q20 bases: 33136(96.6712%)\n",
      "Q30 bases: 30917(90.1975%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 227\n",
      "reads failed due to low quality: 598\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 14.5455%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/6S_1_250821_batch17_07_R1_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/6S_1_250821_batch17_07_R1_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/6S_1_250821_batch17_07_R1_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/6S_1_250821_batch17_07_R1_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/6S_1_250821_batch17_07_R1_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/6S_1_250821_batch17_07_R1_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 1 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for 6S_1_250821_batch17_07_R1_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/6S_1_250821_batch17_07_R1_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/6S_1_250821_batch17_07_R1_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/6S_1_250821_batch17_07_R1_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 4144\n",
      "total bases: 625744\n",
      "Q20 bases: 567365(90.6705%)\n",
      "Q30 bases: 523150(83.6045%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 2527\n",
      "total bases: 381577\n",
      "Q20 bases: 374028(98.0216%)\n",
      "Q30 bases: 359194(94.1341%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 2527\n",
      "reads failed due to low quality: 1617\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 41.4817%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/3SP26_250828_batch18_04_R2_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/3SP26_250828_batch18_04_R2_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/3SP26_250828_batch18_04_R2_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/3SP26_250828_batch18_04_R2_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/3SP26_250828_batch18_04_R2_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/3SP26_250828_batch18_04_R2_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for 3SP26_250828_batch18_04_R2_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/3SP26_250828_batch18_04_R2_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/3SP26_250828_batch18_04_R2_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/3SP26_250828_batch18_04_R2_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 804\n",
      "total bases: 121404\n",
      "Q20 bases: 105164(86.6232%)\n",
      "Q30 bases: 90694(74.7043%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 231\n",
      "total bases: 34881\n",
      "Q20 bases: 33915(97.2306%)\n",
      "Q30 bases: 31283(89.6849%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 231\n",
      "reads failed due to low quality: 573\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 21.7662%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/1D_1_250821_batch17_01_R2_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/1D_1_250821_batch17_01_R2_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/1D_1_250821_batch17_01_R2_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/1D_1_250821_batch17_01_R2_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/1D_1_250821_batch17_01_R2_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/1D_1_250821_batch17_01_R2_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for 1D_1_250821_batch17_01_R2_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/1D_1_250821_batch17_01_R2_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/1D_1_250821_batch17_01_R2_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/1D_1_250821_batch17_01_R2_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 804\n",
      "total bases: 121404\n",
      "Q20 bases: 104389(85.9848%)\n",
      "Q30 bases: 91086(75.0272%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 218\n",
      "total bases: 32918\n",
      "Q20 bases: 31587(95.9566%)\n",
      "Q30 bases: 29409(89.3402%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 218\n",
      "reads failed due to low quality: 586\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 13.3085%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/1D_1_250821_batch17_01_R1_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/1D_1_250821_batch17_01_R1_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/1D_1_250821_batch17_01_R1_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/1D_1_250821_batch17_01_R1_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/1D_1_250821_batch17_01_R1_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/1D_1_250821_batch17_01_R1_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for 1D_1_250821_batch17_01_R1_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/1D_1_250821_batch17_01_R1_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/1D_1_250821_batch17_01_R1_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/1D_1_250821_batch17_01_R1_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 4144\n",
      "total bases: 625744\n",
      "Q20 bases: 569626(91.0318%)\n",
      "Q30 bases: 520618(83.1998%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 2438\n",
      "total bases: 368138\n",
      "Q20 bases: 362244(98.399%)\n",
      "Q30 bases: 348230(94.5922%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 2438\n",
      "reads failed due to low quality: 1706\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 45.6081%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/3SP26_250828_batch18_04_R1_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/3SP26_250828_batch18_04_R1_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/3SP26_250828_batch18_04_R1_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/3SP26_250828_batch18_04_R1_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/3SP26_250828_batch18_04_R1_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/3SP26_250828_batch18_04_R1_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 1 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for 3SP26_250828_batch18_04_R1_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/3SP26_250828_batch18_04_R1_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/3SP26_250828_batch18_04_R1_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/3SP26_250828_batch18_04_R1_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 825\n",
      "total bases: 124575\n",
      "Q20 bases: 107887(86.6041%)\n",
      "Q30 bases: 92988(74.6442%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 228\n",
      "total bases: 34428\n",
      "Q20 bases: 33388(96.9792%)\n",
      "Q30 bases: 30766(89.3633%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 228\n",
      "reads failed due to low quality: 597\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 15.3939%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/6S_1_250821_batch17_07_R2_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/6S_1_250821_batch17_07_R2_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/6S_1_250821_batch17_07_R2_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/6S_1_250821_batch17_07_R2_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/6S_1_250821_batch17_07_R2_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/6S_1_250821_batch17_07_R2_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for 6S_1_250821_batch17_07_R2_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/6S_1_250821_batch17_07_R2_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/6S_1_250821_batch17_07_R2_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/6S_1_250821_batch17_07_R2_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 721\n",
      "total bases: 108871\n",
      "Q20 bases: 92957(85.3827%)\n",
      "Q30 bases: 81028(74.4257%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 194\n",
      "total bases: 29294\n",
      "Q20 bases: 28171(96.1665%)\n",
      "Q30 bases: 26218(89.4996%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 194\n",
      "reads failed due to low quality: 527\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 14.9792%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/0N_1_250821_batch17_02_R1_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/0N_1_250821_batch17_02_R1_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/0N_1_250821_batch17_02_R1_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/0N_1_250821_batch17_02_R1_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/0N_1_250821_batch17_02_R1_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/0N_1_250821_batch17_02_R1_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for 0N_1_250821_batch17_02_R1_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/0N_1_250821_batch17_02_R1_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/0N_1_250821_batch17_02_R1_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/0N_1_250821_batch17_02_R1_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 867\n",
      "total bases: 130917\n",
      "Q20 bases: 113222(86.4838%)\n",
      "Q30 bases: 97238(74.2745%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 250\n",
      "total bases: 37750\n",
      "Q20 bases: 36731(97.3007%)\n",
      "Q30 bases: 33973(89.9947%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 250\n",
      "reads failed due to low quality: 617\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 20.7612%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/4G_1_250821_batch17_05_R2_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/4G_1_250821_batch17_05_R2_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/4G_1_250821_batch17_05_R2_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/4G_1_250821_batch17_05_R2_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/4G_1_250821_batch17_05_R2_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/4G_1_250821_batch17_05_R2_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for 4G_1_250821_batch17_05_R2_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/4G_1_250821_batch17_05_R2_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/4G_1_250821_batch17_05_R2_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/4G_1_250821_batch17_05_R2_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 867\n",
      "total bases: 130917\n",
      "Q20 bases: 110998(84.785%)\n",
      "Q30 bases: 96576(73.7689%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 213\n",
      "total bases: 32163\n",
      "Q20 bases: 30934(96.1788%)\n",
      "Q30 bases: 28893(89.833%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 213\n",
      "reads failed due to low quality: 654\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 12.9181%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/4G_1_250821_batch17_05_R1_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/4G_1_250821_batch17_05_R1_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/4G_1_250821_batch17_05_R1_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/4G_1_250821_batch17_05_R1_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/4G_1_250821_batch17_05_R1_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/4G_1_250821_batch17_05_R1_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 1 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for 4G_1_250821_batch17_05_R1_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/4G_1_250821_batch17_05_R1_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/4G_1_250821_batch17_05_R1_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/4G_1_250821_batch17_05_R1_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 771\n",
      "total bases: 116421\n",
      "Q20 bases: 99711(85.6469%)\n",
      "Q30 bases: 86468(74.2718%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 214\n",
      "total bases: 32314\n",
      "Q20 bases: 31248(96.7011%)\n",
      "Q30 bases: 29065(89.9455%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 214\n",
      "reads failed due to low quality: 557\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 14.5266%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/7T_1_250821_batch17_08_R1_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/7T_1_250821_batch17_08_R1_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/7T_1_250821_batch17_08_R1_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/7T_1_250821_batch17_08_R1_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/7T_1_250821_batch17_08_R1_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/7T_1_250821_batch17_08_R1_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for 7T_1_250821_batch17_08_R1_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/7T_1_250821_batch17_08_R1_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/7T_1_250821_batch17_08_R1_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/7T_1_250821_batch17_08_R1_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 808\n",
      "total bases: 122008\n",
      "Q20 bases: 104048(85.2797%)\n",
      "Q30 bases: 90596(74.2541%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 200\n",
      "total bases: 30200\n",
      "Q20 bases: 29104(96.3709%)\n",
      "Q30 bases: 27065(89.6192%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 200\n",
      "reads failed due to low quality: 608\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 9.77723%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/2S_1_250821_batch17_03_R1_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/2S_1_250821_batch17_03_R1_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/2S_1_250821_batch17_03_R1_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/2S_1_250821_batch17_03_R1_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/2S_1_250821_batch17_03_R1_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/2S_1_250821_batch17_03_R1_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for 2S_1_250821_batch17_03_R1_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/2S_1_250821_batch17_03_R1_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/2S_1_250821_batch17_03_R1_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/2S_1_250821_batch17_03_R1_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 808\n",
      "total bases: 122008\n",
      "Q20 bases: 104834(85.9239%)\n",
      "Q30 bases: 89977(73.7468%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 200\n",
      "total bases: 30200\n",
      "Q20 bases: 29240(96.8212%)\n",
      "Q30 bases: 26938(89.1987%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 200\n",
      "reads failed due to low quality: 608\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 16.2129%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/2S_1_250821_batch17_03_R2_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/2S_1_250821_batch17_03_R2_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/2S_1_250821_batch17_03_R2_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/2S_1_250821_batch17_03_R2_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/2S_1_250821_batch17_03_R2_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/2S_1_250821_batch17_03_R2_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 1 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for 2S_1_250821_batch17_03_R2_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/2S_1_250821_batch17_03_R2_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/2S_1_250821_batch17_03_R2_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/2S_1_250821_batch17_03_R2_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 771\n",
      "total bases: 116421\n",
      "Q20 bases: 99857(85.7723%)\n",
      "Q30 bases: 85933(73.8123%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 230\n",
      "total bases: 34730\n",
      "Q20 bases: 33795(97.3078%)\n",
      "Q30 bases: 31170(89.7495%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 230\n",
      "reads failed due to low quality: 541\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 18.9364%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/7T_1_250821_batch17_08_R2_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/7T_1_250821_batch17_08_R2_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/7T_1_250821_batch17_08_R2_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/7T_1_250821_batch17_08_R2_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/7T_1_250821_batch17_08_R2_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/7T_1_250821_batch17_08_R2_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for 7T_1_250821_batch17_08_R2_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/7T_1_250821_batch17_08_R2_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/7T_1_250821_batch17_08_R2_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/7T_1_250821_batch17_08_R2_Qfiltered.fastq.gz.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 774\n",
      "total bases: 116874\n",
      "Q20 bases: 99962(85.5297%)\n",
      "Q30 bases: 87120(74.5418%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 212\n",
      "total bases: 32012\n",
      "Q20 bases: 31048(96.9886%)\n",
      "Q30 bases: 29026(90.6722%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 212\n",
      "reads failed due to low quality: 562\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 14.9871%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/5I_1_250821_batch17_06_R1_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/5I_1_250821_batch17_06_R1_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/5I_1_250821_batch17_06_R1_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/5I_1_250821_batch17_06_R1_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/5I_1_250821_batch17_06_R1_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/5I_1_250821_batch17_06_R1_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n",
      "WARNING: you specified the options for cutting by quality, but forogt to enable any of cut_front/cut_tail/cut_right. This will have no effect.\n",
      "Detecting adapter sequence for read1...\n",
      "No adapter detected for read1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for 5I_1_250821_batch17_06_R1_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/5I_1_250821_batch17_06_R1_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/5I_1_250821_batch17_06_R1_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/5I_1_250821_batch17_06_R1_Qfiltered.fastq.gz.json\n",
      "\n",
      "Filtering for 5I_1_250821_batch17_06_R2_untrimmed.fastq.gz is complete.\n",
      "Output FASTQ : sequence_merge_method/B_Qfiltered/5I_1_250821_batch17_06_R2_Qfiltered.fastq.gz\n",
      "Reports      : sequence_merge_method/B_Qfiltered/5I_1_250821_batch17_06_R2_Qfiltered.fastq.gz.html / sequence_merge_method/B_Qfiltered/5I_1_250821_batch17_06_R2_Qfiltered.fastq.gz.json\n",
      "\n",
      "All filtering processes are done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read1 before filtering:\n",
      "total reads: 774\n",
      "total bases: 116874\n",
      "Q20 bases: 100577(86.0559%)\n",
      "Q30 bases: 86450(73.9685%)\n",
      "\n",
      "Read1 after filtering:\n",
      "total reads: 216\n",
      "total bases: 32616\n",
      "Q20 bases: 31631(96.98%)\n",
      "Q30 bases: 29247(89.6707%)\n",
      "\n",
      "Filtering result:\n",
      "reads passed filter: 216\n",
      "reads failed due to low quality: 558\n",
      "reads failed due to too many N: 0\n",
      "reads failed due to too short: 0\n",
      "reads with adapter trimmed: 0\n",
      "bases trimmed due to adapters: 0\n",
      "\n",
      "Duplication rate (may be overestimated since this is SE data): 14.4703%\n",
      "\n",
      "JSON report: sequence_merge_method/B_Qfiltered/5I_1_250821_batch17_06_R2_Qfiltered.fastq.gz.json\n",
      "HTML report: sequence_merge_method/B_Qfiltered/5I_1_250821_batch17_06_R2_Qfiltered.fastq.gz.html\n",
      "\n",
      "fastp -i sequence_merge_method/A_untrimmed_output/5I_1_250821_batch17_06_R2_untrimmed.fastq.gz -o sequence_merge_method/B_Qfiltered/5I_1_250821_batch17_06_R2_Qfiltered.fastq.gz -q 30 -u 15 -l 151 --cut_mean_quality 30 --html sequence_merge_method/B_Qfiltered/5I_1_250821_batch17_06_R2_Qfiltered.fastq.gz.html --json sequence_merge_method/B_Qfiltered/5I_1_250821_batch17_06_R2_Qfiltered.fastq.gz.json \n",
      "fastp v0.23.4, time used: 0 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Quality threshold (Phred score)\n",
    "quality_threshold = 30\n",
    "\n",
    "# Set input and output folders\n",
    "input_folder = \"sequence_merge_method/A_untrimmed_output\"\n",
    "output_folder = \"sequence_merge_method/B_Qfiltered\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate through files in the input folder, processing only those ending with \"_untrimmed.fastq.gz\"\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\"_untrimmed.fastq.gz\"):\n",
    "        # Input file path\n",
    "        input_file = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Output filename (e.g., sample_untrimmed.fastq.gz -> sample_Qfiltered.fastq.gz)\n",
    "        output_file = os.path.join(\n",
    "            output_folder, \n",
    "            filename.replace(\"_untrimmed.fastq.gz\", \"_Qfiltered.fastq.gz\")\n",
    "        )\n",
    "        \n",
    "        # Execute fastp in single-end mode for each file\n",
    "        subprocess.call([\n",
    "            \"fastp\",\n",
    "            \"-i\", input_file,                      # Input file\n",
    "            \"-o\", output_file,                     # Output file\n",
    "            \"-q\", str(quality_threshold),          # Quality threshold for a base to be qualified\n",
    "            \"-u\", \"15\",                            # Discard reads if the percentage of unqualified bases is >= 15%\n",
    "            \"-l\", \"151\",                           # Minimum read length to keep\n",
    "            \"--cut_mean_quality\", \"30\",            # Discard reads if mean quality is less than 30\n",
    "            \"--html\", f\"{output_file}.html\",       # HTML report file path\n",
    "            \"--json\", f\"{output_file}.json\"        # JSON report file path\n",
    "        ])\n",
    "        \n",
    "        print(f\"Filtering for {filename} is complete.\\n\"\n",
    "              f\"Output FASTQ : {output_file}\\n\"\n",
    "              f\"Reports      : {output_file}.html / {output_file}.json\\n\")\n",
    "\n",
    "print(\"All filtering processes are done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b121a804",
   "metadata": {},
   "source": [
    "# 4. Match Paired-End Read IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc591cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 7T_1_250821_batch17_08_R1_Qfiltered.fastq.gz and 7T_1_250821_batch17_08_R2_Qfiltered.fastq.gz\n",
      "Total R1 IDs: 214, Total R2 IDs: 230, Matching IDs: 139\n",
      "IDs only in R1: 75, IDs only in R2: 91\n",
      "\n",
      "Processing 2S_1_250821_batch17_03_R1_Qfiltered.fastq.gz and 2S_1_250821_batch17_03_R2_Qfiltered.fastq.gz\n",
      "Total R1 IDs: 200, Total R2 IDs: 200, Matching IDs: 119\n",
      "IDs only in R1: 81, IDs only in R2: 81\n",
      "\n",
      "Processing 5I_1_250821_batch17_06_R1_Qfiltered.fastq.gz and 5I_1_250821_batch17_06_R2_Qfiltered.fastq.gz\n",
      "Total R1 IDs: 212, Total R2 IDs: 216, Matching IDs: 135\n",
      "IDs only in R1: 77, IDs only in R2: 81\n",
      "\n",
      "Processing 3SP26_250828_batch18_04_R1_Qfiltered.fastq.gz and 3SP26_250828_batch18_04_R2_Qfiltered.fastq.gz\n",
      "Total R1 IDs: 2438, Total R2 IDs: 2527, Matching IDs: 2148\n",
      "IDs only in R1: 290, IDs only in R2: 379\n",
      "\n",
      "Processing 1D_1_250821_batch17_01_R1_Qfiltered.fastq.gz and 1D_1_250821_batch17_01_R2_Qfiltered.fastq.gz\n",
      "Total R1 IDs: 218, Total R2 IDs: 231, Matching IDs: 146\n",
      "IDs only in R1: 72, IDs only in R2: 85\n",
      "\n",
      "Processing 0N_1_250821_batch17_02_R1_Qfiltered.fastq.gz and 0N_1_250821_batch17_02_R2_Qfiltered.fastq.gz\n",
      "Total R1 IDs: 194, Total R2 IDs: 236, Matching IDs: 136\n",
      "IDs only in R1: 58, IDs only in R2: 100\n",
      "\n",
      "Processing 6S_1_250821_batch17_07_R1_Qfiltered.fastq.gz and 6S_1_250821_batch17_07_R2_Qfiltered.fastq.gz\n",
      "Total R1 IDs: 227, Total R2 IDs: 228, Matching IDs: 140\n",
      "IDs only in R1: 87, IDs only in R2: 88\n",
      "\n",
      "Processing 4G_1_250821_batch17_05_R1_Qfiltered.fastq.gz and 4G_1_250821_batch17_05_R2_Qfiltered.fastq.gz\n",
      "Total R1 IDs: 213, Total R2 IDs: 250, Matching IDs: 152\n",
      "IDs only in R1: 61, IDs only in R2: 98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def extract_matching_reads(r1_path, r2_path, out_r1_path, out_r2_path):\n",
    "    def get_read_id(header):\n",
    "        # Extract ID from the FASTQ header\n",
    "        return header.split()[0].replace('/1', '').replace('/2', '')\n",
    "\n",
    "    r1_ids = set()\n",
    "    r2_ids = set()\n",
    "\n",
    "    # Extract all read IDs from the R1 file\n",
    "    with gzip.open(r1_path, 'rt') as r1_file:\n",
    "        while True:\n",
    "            header = r1_file.readline()\n",
    "            if not header:\n",
    "                break\n",
    "            r1_ids.add(get_read_id(header.strip()))\n",
    "            # Skip the other 3 lines of the read (sequence, +, quality)\n",
    "            [r1_file.readline() for _ in range(3)] \n",
    "\n",
    "    # Extract all read IDs from the R2 file\n",
    "    with gzip.open(r2_path, 'rt') as r2_file:\n",
    "        while True:\n",
    "            header = r2_file.readline()\n",
    "            if not header:\n",
    "                break\n",
    "            r2_ids.add(get_read_id(header.strip()))\n",
    "            [r2_file.readline() for _ in range(3)]\n",
    "\n",
    "    # Find common and unique IDs\n",
    "    matching_ids = r1_ids & r2_ids\n",
    "    r1_only = r1_ids - r2_ids\n",
    "    r2_only = r2_ids - r1_ids\n",
    "\n",
    "    print(f\"Processing {os.path.basename(r1_path)} and {os.path.basename(r2_path)}\")\n",
    "    print(f\"Total R1 IDs: {len(r1_ids)}, Total R2 IDs: {len(r2_ids)}, Matching IDs: {len(matching_ids)}\")\n",
    "    print(f\"IDs only in R1: {len(r1_only)}, IDs only in R2: {len(r2_only)}\\n\")\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(out_r1_path), exist_ok=True)\n",
    "\n",
    "    # Function to write only the reads with matching IDs to a new file\n",
    "    def write_matching_reads(input_path, output_path, matching_ids):\n",
    "        with gzip.open(input_path, 'rt') as infile, gzip.open(output_path, 'wt') as outfile:\n",
    "            while True:\n",
    "                lines = [infile.readline() for _ in range(4)]\n",
    "                if not lines[0]:\n",
    "                    break\n",
    "                read_id = get_read_id(lines[0].strip())\n",
    "                if read_id in matching_ids:\n",
    "                    outfile.writelines(lines)\n",
    "\n",
    "    # Write the filtered R1 and R2 files\n",
    "    write_matching_reads(r1_path, out_r1_path, matching_ids)\n",
    "    write_matching_reads(r2_path, out_r2_path, matching_ids)\n",
    "\n",
    "# --------------------------\n",
    "# Apply to all file pairs\n",
    "# --------------------------\n",
    "\n",
    "input_folder = \"sequence_merge_method/B_Qfiltered\"\n",
    "output_folder = \"sequence_merge_method/C_id_matched\"\n",
    "\n",
    "# Find all R1 files\n",
    "r1_files = glob.glob(os.path.join(input_folder, \"*_R1_Qfiltered.fastq.gz\"))\n",
    "\n",
    "# For each R1, find the corresponding R2 file and run the process\n",
    "for r1_file in r1_files:\n",
    "    r2_file = r1_file.replace(\"_R1_Qfiltered.fastq.gz\", \"_R2_Qfiltered.fastq.gz\")\n",
    "    \n",
    "    if os.path.exists(r2_file):\n",
    "        # Set the output file paths\n",
    "        base_name = os.path.basename(r1_file).replace(\"_R1_Qfiltered.fastq.gz\", \"\")\n",
    "        out_r1 = os.path.join(output_folder, f\"{base_name}_ID_match_R1.fastq.gz\")\n",
    "        out_r2 = os.path.join(output_folder, f\"{base_name}_ID_match_R2.fastq.gz\")\n",
    "        \n",
    "        # Execute the function\n",
    "        extract_matching_reads(r1_file, r2_file, out_r1, out_r2)\n",
    "    else:\n",
    "        print(f\"Warning: Corresponding R2 file not found for {r1_file}. Skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e00e67",
   "metadata": {},
   "source": [
    "# 5 Merge W/ Flash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09a7b98",
   "metadata": {},
   "source": [
    "## 5.1 R1(Front, Back), R2(Front, Back) Fragmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24827b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  : 2S_1_250821_batch17_03  sequence_merge_method/D_split_reads (N=126)\n",
      "  : 5I_1_250821_batch17_06  sequence_merge_method/D_split_reads (N=126)\n",
      "  : 4G_1_250821_batch17_05  sequence_merge_method/D_split_reads (N=126)\n",
      "  : 0N_1_250821_batch17_02  sequence_merge_method/D_split_reads (N=126)\n",
      "  : 1D_1_250821_batch17_01  sequence_merge_method/D_split_reads (N=126)\n",
      "  : 3SP26_250828_batch18_04  sequence_merge_method/D_split_reads (N=122)\n",
      "  : 7T_1_250821_batch17_08  sequence_merge_method/D_split_reads (N=120)\n",
      "  : 6S_1_250821_batch17_07  sequence_merge_method/D_split_reads (N=124)\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def split_fastq_by_position(r1_path, r2_path, n, output_dir):\n",
    "    \"\"\"Splits each read in R1 and R2 files into front and back parts.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    sample_base = os.path.basename(r1_path).replace(\"_ID_match_R1.fastq.gz\", \"\")\n",
    "    r1_f_path = os.path.join(output_dir, f\"{sample_base}_R1_F.fastq.gz\")\n",
    "    r1_b_path = os.path.join(output_dir, f\"{sample_base}_R1_B.fastq.gz\")\n",
    "    r2_f_path = os.path.join(output_dir, f\"{sample_base}_R2_F.fastq.gz\")\n",
    "    r2_b_path = os.path.join(output_dir, f\"{sample_base}_R2_B.fastq.gz\")\n",
    "\n",
    "    with gzip.open(r1_path, 'rt') as r1_file, \\\n",
    "         gzip.open(r2_path, 'rt') as r2_file, \\\n",
    "         gzip.open(r1_f_path, 'wt') as r1_f_out, \\\n",
    "         gzip.open(r1_b_path, 'wt') as r1_b_out, \\\n",
    "         gzip.open(r2_f_path, 'wt') as r2_f_out, \\\n",
    "         gzip.open(r2_b_path, 'wt') as r2_b_out:\n",
    "\n",
    "        while True:\n",
    "            r1_lines = [r1_file.readline() for _ in range(4)]\n",
    "            r2_lines = [r2_file.readline() for _ in range(4)]\n",
    "\n",
    "            if not r1_lines[0] or not r2_lines[0]:\n",
    "                break\n",
    "\n",
    "            header1, seq1, plus1, qual1 = [line.strip() for line in r1_lines]\n",
    "            header2, seq2, plus2, qual2 = [line.strip() for line in r2_lines]\n",
    "\n",
    "            # Split R1 read\n",
    "            r1_f_out.write(f\"{header1}\\n{seq1[:151-n]}\\n{plus1}\\n{qual1[:151-n]}\\n\")\n",
    "            r1_b_out.write(f\"{header1}\\n{seq1[-n:]}\\n{plus1}\\n{qual1[-n:]}\\n\")\n",
    "            # Split R2 read\n",
    "            r2_f_out.write(f\"{header2}\\n{seq2[:151-n]}\\n{plus2}\\n{qual2[:151-n]}\\n\")\n",
    "            r2_b_out.write(f\"{header2}\\n{seq2[-n:]}\\n{plus2}\\n{qual2[-n:]}\\n\")\n",
    "\n",
    "    print(f\" Split complete for: {sample_base}  {output_dir} (N={n})\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Apply the split function to all files\n",
    "# -----------------------------------\n",
    "\n",
    "input_folder = \"sequence_merge_method/C_id_matched\"\n",
    "output_folder = \"sequence_merge_method/D_split_reads\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define the N-value (length of the back part) for each sample prefix\n",
    "sample_n_mapping = {\n",
    "    \"0N\": 126,\n",
    "    \"1D\": 126,\n",
    "    \"2S\": 126,\n",
    "    \"3G\": 124,\n",
    "    \"4I\": 128,\n",
    "    \"5S\": 124,\n",
    "    \"6T\": 122,\n",
    "    \"5K\": 124,\n",
    "    \"1X8\": 116,\n",
    "    \"0X8\": 132,\n",
    "    \"4G\": 126,\n",
    "    \"5I\": 126,\n",
    "    \"6S\": 124,\n",
    "    \"7T\": 120,\n",
    "    \"3SP26\": 122,\n",
    "    \"3SP31\": 118   \n",
    "}\n",
    "\n",
    "\n",
    "# Find all R1 files\n",
    "r1_files = glob.glob(os.path.join(input_folder, \"*_ID_match_R1.fastq.gz\"))\n",
    "\n",
    "for r1_file in r1_files:\n",
    "    r2_file = r1_file.replace(\"_R1.fastq.gz\", \"_R2.fastq.gz\")\n",
    "\n",
    "    if not os.path.exists(r2_file):\n",
    "        print(f\" Matching R2 file not found: {r2_file}\")\n",
    "        continue\n",
    "\n",
    "    # Find the corresponding N value based on the filename prefix\n",
    "    matched_n = None\n",
    "    for prefix, n_value in sample_n_mapping.items():\n",
    "        if prefix in os.path.basename(r1_file):\n",
    "            matched_n = n_value\n",
    "            break\n",
    "\n",
    "    if matched_n is None:\n",
    "        print(f\" Could not find N value for: {r1_file}  Skipping\")\n",
    "        continue\n",
    "\n",
    "    # Execute the split function\n",
    "    split_fastq_by_position(r1_file, r2_file, matched_n, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59cd6b2",
   "metadata": {},
   "source": [
    "## 5.2 R2 DNA reverse complementary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fc929e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Reverse complemented: 7T_1_250821_batch17_08_R2_F_revcomp.fastq.gz\n",
      " Reverse complemented: 4G_1_250821_batch17_05_R2_B_revcomp.fastq.gz\n",
      " Reverse complemented: 2S_1_250821_batch17_03_R2_B_revcomp.fastq.gz\n",
      " Reverse complemented: 5I_1_250821_batch17_06_R2_B_revcomp.fastq.gz\n",
      " Reverse complemented: 6S_1_250821_batch17_07_R2_F_revcomp.fastq.gz\n",
      " Reverse complemented: 3SP26_250828_batch18_04_R2_B_revcomp.fastq.gz\n",
      " Reverse complemented: 0N_1_250821_batch17_02_R2_B_revcomp.fastq.gz\n",
      " Reverse complemented: 1D_1_250821_batch17_01_R2_F_revcomp.fastq.gz\n",
      " Reverse complemented: 6S_1_250821_batch17_07_R2_B_revcomp.fastq.gz\n",
      " Reverse complemented: 0N_1_250821_batch17_02_R2_F_revcomp.fastq.gz\n",
      " Reverse complemented: 1D_1_250821_batch17_01_R2_B_revcomp.fastq.gz\n",
      " Reverse complemented: 3SP26_250828_batch18_04_R2_F_revcomp.fastq.gz\n",
      " Reverse complemented: 5I_1_250821_batch17_06_R2_F_revcomp.fastq.gz\n",
      " Reverse complemented: 4G_1_250821_batch17_05_R2_F_revcomp.fastq.gz\n",
      " Reverse complemented: 7T_1_250821_batch17_08_R2_B_revcomp.fastq.gz\n",
      " Reverse complemented: 2S_1_250821_batch17_03_R2_F_revcomp.fastq.gz\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import glob\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "def reverse_complement_fastq(input_fastq_path, output_fastq_path):\n",
    "    \"\"\"\n",
    "    Reads a FASTQ file, creates the reverse complement of each record, \n",
    "    and writes it to a new file.\n",
    "    \"\"\"\n",
    "    with gzip.open(input_fastq_path, \"rt\") as infile, gzip.open(output_fastq_path, \"wt\") as outfile:\n",
    "        for record in SeqIO.parse(infile, \"fastq\"):\n",
    "            # Create the reverse complement record, preserving the ID and description\n",
    "            rev_comp_record = record.reverse_complement(id=True, description=True)\n",
    "            SeqIO.write(rev_comp_record, outfile, \"fastq\")\n",
    "            \n",
    "    print(f\" Reverse complemented: {os.path.basename(output_fastq_path)}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Perform reverse complement on all relevant R2 files\n",
    "# --------------------------------------------------\n",
    "\n",
    "input_folder = \"sequence_merge_method/D_split_reads\"\n",
    "os.makedirs(input_folder, exist_ok=True)\n",
    "\n",
    "# Find only the R2 front (F) and back (B) fragment files\n",
    "input_files = glob.glob(os.path.join(input_folder, \"*_R2_[BF].fastq.gz\"))\n",
    "\n",
    "for input_path in input_files:\n",
    "    base = os.path.basename(input_path)\n",
    "    # Remove the .fastq.gz extension to create a new filename\n",
    "    name_without_ext = base.replace(\".fastq.gz\", \"\")\n",
    "    output_path = os.path.join(input_folder, f\"{name_without_ext}_revcomp.fastq.gz\")\n",
    "    \n",
    "    reverse_complement_fastq(input_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc11851f",
   "metadata": {},
   "source": [
    "## 5.3 [R1_back]-[R2_back] merge (FLASH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9a5c1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 8 R1_B files.\n",
      " Running FLASH for sample: 7T_1_250821_batch17_08 (N=120)\n",
      "[FLASH] Starting FLASH v1.2.11\n",
      "[FLASH] Fast Length Adjustment of SHort reads\n",
      "[FLASH]  \n",
      "[FLASH] Input files:\n",
      "[FLASH]     sequence_merge_method/D_split_reads/7T_1_250821_batch17_08_R1_B.fastq.gz\n",
      "[FLASH]     sequence_merge_method/D_split_reads/7T_1_250821_batch17_08_R2_B.fastq.gz\n",
      "[FLASH]  \n",
      "[FLASH] Output files:\n",
      "[FLASH]     sequence_merge_method/E_merged_output/7T_1_250821_batch17_08_FLASH.extendedFrags.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/7T_1_250821_batch17_08_FLASH.notCombined_1.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/7T_1_250821_batch17_08_FLASH.notCombined_2.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/7T_1_250821_batch17_08_FLASH.hist\n",
      "[FLASH]     sequence_merge_method/E_merged_output/7T_1_250821_batch17_08_FLASH.histogram\n",
      "[FLASH]  \n",
      "[FLASH] Parameters:\n",
      "[FLASH]     Min overlap:           120\n",
      "[FLASH]     Max overlap:           120\n",
      "[FLASH]     Max mismatch density:  0.250000\n",
      "[FLASH]     Allow \"outie\" pairs:   false\n",
      "[FLASH]     Cap mismatch quals:    false\n",
      "[FLASH]     Combiner threads:      4\n",
      "[FLASH]     Input format:          FASTQ, phred_offset=33\n",
      "[FLASH]     Output format:         FASTQ, phred_offset=33\n",
      "[FLASH]  \n",
      "[FLASH] Starting reader and writer threads\n",
      "[FLASH] Starting 4 combiner threads\n",
      "[FLASH] Processed 139 read pairs\n",
      "[FLASH]  \n",
      "[FLASH] Read combination statistics:\n",
      "[FLASH]     Total pairs:      139\n",
      "[FLASH]     Combined pairs:   50\n",
      "[FLASH]     Uncombined pairs: 89\n",
      "[FLASH]     Percent combined: 35.97%\n",
      "[FLASH]  \n",
      "[FLASH] Writing histogram files.\n",
      "[FLASH]  \n",
      "[FLASH] FLASH v1.2.11 complete!\n",
      "[FLASH] 0.015 seconds elapsed\n",
      " FLASH merging complete  sequence_merge_method/E_merged_output/7T_1_250821_batch17_08_FLASH.fastq\n",
      " Running FLASH for sample: 1D_1_250821_batch17_01 (N=126)\n",
      "[FLASH] Starting FLASH v1.2.11\n",
      "[FLASH] Fast Length Adjustment of SHort reads\n",
      "[FLASH]  \n",
      "[FLASH] Input files:\n",
      "[FLASH]     sequence_merge_method/D_split_reads/1D_1_250821_batch17_01_R1_B.fastq.gz\n",
      "[FLASH]     sequence_merge_method/D_split_reads/1D_1_250821_batch17_01_R2_B.fastq.gz\n",
      "[FLASH]  \n",
      "[FLASH] Output files:\n",
      "[FLASH]     sequence_merge_method/E_merged_output/1D_1_250821_batch17_01_FLASH.extendedFrags.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/1D_1_250821_batch17_01_FLASH.notCombined_1.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/1D_1_250821_batch17_01_FLASH.notCombined_2.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/1D_1_250821_batch17_01_FLASH.hist\n",
      "[FLASH]     sequence_merge_method/E_merged_output/1D_1_250821_batch17_01_FLASH.histogram\n",
      "[FLASH]  \n",
      "[FLASH] Parameters:\n",
      "[FLASH]     Min overlap:           126\n",
      "[FLASH]     Max overlap:           126\n",
      "[FLASH]     Max mismatch density:  0.250000\n",
      "[FLASH]     Allow \"outie\" pairs:   false\n",
      "[FLASH]     Cap mismatch quals:    false\n",
      "[FLASH]     Combiner threads:      4\n",
      "[FLASH]     Input format:          FASTQ, phred_offset=33\n",
      "[FLASH]     Output format:         FASTQ, phred_offset=33\n",
      "[FLASH]  \n",
      "[FLASH] Starting reader and writer threads\n",
      "[FLASH] Starting 4 combiner threads\n",
      "[FLASH] Processed 146 read pairs\n",
      "[FLASH]  \n",
      "[FLASH] Read combination statistics:\n",
      "[FLASH]     Total pairs:      146\n",
      "[FLASH]     Combined pairs:   63\n",
      "[FLASH]     Uncombined pairs: 83\n",
      "[FLASH]     Percent combined: 43.15%\n",
      "[FLASH]  \n",
      "[FLASH] Writing histogram files.\n",
      "[FLASH]  \n",
      "[FLASH] FLASH v1.2.11 complete!\n",
      "[FLASH] 0.015 seconds elapsed\n",
      " FLASH merging complete  sequence_merge_method/E_merged_output/1D_1_250821_batch17_01_FLASH.fastq\n",
      " Running FLASH for sample: 6S_1_250821_batch17_07 (N=124)\n",
      "[FLASH] Starting FLASH v1.2.11\n",
      "[FLASH] Fast Length Adjustment of SHort reads\n",
      "[FLASH]  \n",
      "[FLASH] Input files:\n",
      "[FLASH]     sequence_merge_method/D_split_reads/6S_1_250821_batch17_07_R1_B.fastq.gz\n",
      "[FLASH]     sequence_merge_method/D_split_reads/6S_1_250821_batch17_07_R2_B.fastq.gz\n",
      "[FLASH]  \n",
      "[FLASH] Output files:\n",
      "[FLASH]     sequence_merge_method/E_merged_output/6S_1_250821_batch17_07_FLASH.extendedFrags.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/6S_1_250821_batch17_07_FLASH.notCombined_1.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/6S_1_250821_batch17_07_FLASH.notCombined_2.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/6S_1_250821_batch17_07_FLASH.hist\n",
      "[FLASH]     sequence_merge_method/E_merged_output/6S_1_250821_batch17_07_FLASH.histogram\n",
      "[FLASH]  \n",
      "[FLASH] Parameters:\n",
      "[FLASH]     Min overlap:           124\n",
      "[FLASH]     Max overlap:           124\n",
      "[FLASH]     Max mismatch density:  0.250000\n",
      "[FLASH]     Allow \"outie\" pairs:   false\n",
      "[FLASH]     Cap mismatch quals:    false\n",
      "[FLASH]     Combiner threads:      4\n",
      "[FLASH]     Input format:          FASTQ, phred_offset=33\n",
      "[FLASH]     Output format:         FASTQ, phred_offset=33\n",
      "[FLASH]  \n",
      "[FLASH] Starting reader and writer threads\n",
      "[FLASH] Starting 4 combiner threads\n",
      "[FLASH] Processed 140 read pairs\n",
      "[FLASH]  \n",
      "[FLASH] Read combination statistics:\n",
      "[FLASH]     Total pairs:      140\n",
      "[FLASH]     Combined pairs:   47\n",
      "[FLASH]     Uncombined pairs: 93\n",
      "[FLASH]     Percent combined: 33.57%\n",
      "[FLASH]  \n",
      "[FLASH] Writing histogram files.\n",
      "[FLASH]  \n",
      "[FLASH] FLASH v1.2.11 complete!\n",
      "[FLASH] 0.014 seconds elapsed\n",
      " FLASH merging complete  sequence_merge_method/E_merged_output/6S_1_250821_batch17_07_FLASH.fastq\n",
      " Running FLASH for sample: 3SP26_250828_batch18_04 (N=122)\n",
      "[FLASH] Starting FLASH v1.2.11\n",
      "[FLASH] Fast Length Adjustment of SHort reads\n",
      "[FLASH]  \n",
      "[FLASH] Input files:\n",
      "[FLASH]     sequence_merge_method/D_split_reads/3SP26_250828_batch18_04_R1_B.fastq.gz\n",
      "[FLASH]     sequence_merge_method/D_split_reads/3SP26_250828_batch18_04_R2_B.fastq.gz\n",
      "[FLASH]  \n",
      "[FLASH] Output files:\n",
      "[FLASH]     sequence_merge_method/E_merged_output/3SP26_250828_batch18_04_FLASH.extendedFrags.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/3SP26_250828_batch18_04_FLASH.notCombined_1.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/3SP26_250828_batch18_04_FLASH.notCombined_2.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/3SP26_250828_batch18_04_FLASH.hist\n",
      "[FLASH]     sequence_merge_method/E_merged_output/3SP26_250828_batch18_04_FLASH.histogram\n",
      "[FLASH]  \n",
      "[FLASH] Parameters:\n",
      "[FLASH]     Min overlap:           122\n",
      "[FLASH]     Max overlap:           122\n",
      "[FLASH]     Max mismatch density:  0.250000\n",
      "[FLASH]     Allow \"outie\" pairs:   false\n",
      "[FLASH]     Cap mismatch quals:    false\n",
      "[FLASH]     Combiner threads:      4\n",
      "[FLASH]     Input format:          FASTQ, phred_offset=33\n",
      "[FLASH]     Output format:         FASTQ, phred_offset=33\n",
      "[FLASH]  \n",
      "[FLASH] Starting reader and writer threads\n",
      "[FLASH] Starting 4 combiner threads\n",
      "[FLASH] Processed 2148 read pairs\n",
      "[FLASH]  \n",
      "[FLASH] Read combination statistics:\n",
      "[FLASH]     Total pairs:      2148\n",
      "[FLASH]     Combined pairs:   420\n",
      "[FLASH]     Uncombined pairs: 1728\n",
      "[FLASH]     Percent combined: 19.55%\n",
      "[FLASH]  \n",
      "[FLASH] Writing histogram files.\n",
      "[FLASH]  \n",
      "[FLASH] FLASH v1.2.11 complete!\n",
      "[FLASH] 0.039 seconds elapsed\n",
      " FLASH merging complete  sequence_merge_method/E_merged_output/3SP26_250828_batch18_04_FLASH.fastq\n",
      " Running FLASH for sample: 0N_1_250821_batch17_02 (N=126)\n",
      "[FLASH] Starting FLASH v1.2.11\n",
      "[FLASH] Fast Length Adjustment of SHort reads\n",
      "[FLASH]  \n",
      "[FLASH] Input files:\n",
      "[FLASH]     sequence_merge_method/D_split_reads/0N_1_250821_batch17_02_R1_B.fastq.gz\n",
      "[FLASH]     sequence_merge_method/D_split_reads/0N_1_250821_batch17_02_R2_B.fastq.gz\n",
      "[FLASH]  \n",
      "[FLASH] Output files:\n",
      "[FLASH]     sequence_merge_method/E_merged_output/0N_1_250821_batch17_02_FLASH.extendedFrags.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/0N_1_250821_batch17_02_FLASH.notCombined_1.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/0N_1_250821_batch17_02_FLASH.notCombined_2.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/0N_1_250821_batch17_02_FLASH.hist\n",
      "[FLASH]     sequence_merge_method/E_merged_output/0N_1_250821_batch17_02_FLASH.histogram\n",
      "[FLASH]  \n",
      "[FLASH] Parameters:\n",
      "[FLASH]     Min overlap:           126\n",
      "[FLASH]     Max overlap:           126\n",
      "[FLASH]     Max mismatch density:  0.250000\n",
      "[FLASH]     Allow \"outie\" pairs:   false\n",
      "[FLASH]     Cap mismatch quals:    false\n",
      "[FLASH]     Combiner threads:      4\n",
      "[FLASH]     Input format:          FASTQ, phred_offset=33\n",
      "[FLASH]     Output format:         FASTQ, phred_offset=33\n",
      "[FLASH]  \n",
      "[FLASH] Starting reader and writer threads\n",
      "[FLASH] Starting 4 combiner threads\n",
      "[FLASH] Processed 136 read pairs\n",
      "[FLASH]  \n",
      "[FLASH] Read combination statistics:\n",
      "[FLASH]     Total pairs:      136\n",
      "[FLASH]     Combined pairs:   50\n",
      "[FLASH]     Uncombined pairs: 86\n",
      "[FLASH]     Percent combined: 36.76%\n",
      "[FLASH]  \n",
      "[FLASH] Writing histogram files.\n",
      "[FLASH]  \n",
      "[FLASH] FLASH v1.2.11 complete!\n",
      "[FLASH] 0.015 seconds elapsed\n",
      " FLASH merging complete  sequence_merge_method/E_merged_output/0N_1_250821_batch17_02_FLASH.fastq\n",
      " Running FLASH for sample: 2S_1_250821_batch17_03 (N=126)\n",
      "[FLASH] Starting FLASH v1.2.11\n",
      "[FLASH] Fast Length Adjustment of SHort reads\n",
      "[FLASH]  \n",
      "[FLASH] Input files:\n",
      "[FLASH]     sequence_merge_method/D_split_reads/2S_1_250821_batch17_03_R1_B.fastq.gz\n",
      "[FLASH]     sequence_merge_method/D_split_reads/2S_1_250821_batch17_03_R2_B.fastq.gz\n",
      "[FLASH]  \n",
      "[FLASH] Output files:\n",
      "[FLASH]     sequence_merge_method/E_merged_output/2S_1_250821_batch17_03_FLASH.extendedFrags.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/2S_1_250821_batch17_03_FLASH.notCombined_1.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/2S_1_250821_batch17_03_FLASH.notCombined_2.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/2S_1_250821_batch17_03_FLASH.hist\n",
      "[FLASH]     sequence_merge_method/E_merged_output/2S_1_250821_batch17_03_FLASH.histogram\n",
      "[FLASH]  \n",
      "[FLASH] Parameters:\n",
      "[FLASH]     Min overlap:           126\n",
      "[FLASH]     Max overlap:           126\n",
      "[FLASH]     Max mismatch density:  0.250000\n",
      "[FLASH]     Allow \"outie\" pairs:   false\n",
      "[FLASH]     Cap mismatch quals:    false\n",
      "[FLASH]     Combiner threads:      4\n",
      "[FLASH]     Input format:          FASTQ, phred_offset=33\n",
      "[FLASH]     Output format:         FASTQ, phred_offset=33\n",
      "[FLASH]  \n",
      "[FLASH] Starting reader and writer threads\n",
      "[FLASH] Starting 4 combiner threads\n",
      "[FLASH] Processed 119 read pairs\n",
      "[FLASH]  \n",
      "[FLASH] Read combination statistics:\n",
      "[FLASH]     Total pairs:      119\n",
      "[FLASH]     Combined pairs:   47\n",
      "[FLASH]     Uncombined pairs: 72\n",
      "[FLASH]     Percent combined: 39.50%\n",
      "[FLASH]  \n",
      "[FLASH] Writing histogram files.\n",
      "[FLASH]  \n",
      "[FLASH] FLASH v1.2.11 complete!\n",
      "[FLASH] 0.017 seconds elapsed\n",
      " FLASH merging complete  sequence_merge_method/E_merged_output/2S_1_250821_batch17_03_FLASH.fastq\n",
      " Running FLASH for sample: 4G_1_250821_batch17_05 (N=126)\n",
      "[FLASH] Starting FLASH v1.2.11\n",
      "[FLASH] Fast Length Adjustment of SHort reads\n",
      "[FLASH]  \n",
      "[FLASH] Input files:\n",
      "[FLASH]     sequence_merge_method/D_split_reads/4G_1_250821_batch17_05_R1_B.fastq.gz\n",
      "[FLASH]     sequence_merge_method/D_split_reads/4G_1_250821_batch17_05_R2_B.fastq.gz\n",
      "[FLASH]  \n",
      "[FLASH] Output files:\n",
      "[FLASH]     sequence_merge_method/E_merged_output/4G_1_250821_batch17_05_FLASH.extendedFrags.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/4G_1_250821_batch17_05_FLASH.notCombined_1.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/4G_1_250821_batch17_05_FLASH.notCombined_2.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/4G_1_250821_batch17_05_FLASH.hist\n",
      "[FLASH]     sequence_merge_method/E_merged_output/4G_1_250821_batch17_05_FLASH.histogram\n",
      "[FLASH]  \n",
      "[FLASH] Parameters:\n",
      "[FLASH]     Min overlap:           126\n",
      "[FLASH]     Max overlap:           126\n",
      "[FLASH]     Max mismatch density:  0.250000\n",
      "[FLASH]     Allow \"outie\" pairs:   false\n",
      "[FLASH]     Cap mismatch quals:    false\n",
      "[FLASH]     Combiner threads:      4\n",
      "[FLASH]     Input format:          FASTQ, phred_offset=33\n",
      "[FLASH]     Output format:         FASTQ, phred_offset=33\n",
      "[FLASH]  \n",
      "[FLASH] Starting reader and writer threads\n",
      "[FLASH] Starting 4 combiner threads\n",
      "[FLASH] Processed 152 read pairs\n",
      "[FLASH]  \n",
      "[FLASH] Read combination statistics:\n",
      "[FLASH]     Total pairs:      152\n",
      "[FLASH]     Combined pairs:   61\n",
      "[FLASH]     Uncombined pairs: 91\n",
      "[FLASH]     Percent combined: 40.13%\n",
      "[FLASH]  \n",
      "[FLASH] Writing histogram files.\n",
      "[FLASH]  \n",
      "[FLASH] FLASH v1.2.11 complete!\n",
      "[FLASH] 0.016 seconds elapsed\n",
      " FLASH merging complete  sequence_merge_method/E_merged_output/4G_1_250821_batch17_05_FLASH.fastq\n",
      " Running FLASH for sample: 5I_1_250821_batch17_06 (N=126)\n",
      "[FLASH] Starting FLASH v1.2.11\n",
      "[FLASH] Fast Length Adjustment of SHort reads\n",
      "[FLASH]  \n",
      "[FLASH] Input files:\n",
      "[FLASH]     sequence_merge_method/D_split_reads/5I_1_250821_batch17_06_R1_B.fastq.gz\n",
      "[FLASH]     sequence_merge_method/D_split_reads/5I_1_250821_batch17_06_R2_B.fastq.gz\n",
      "[FLASH]  \n",
      "[FLASH] Output files:\n",
      "[FLASH]     sequence_merge_method/E_merged_output/5I_1_250821_batch17_06_FLASH.extendedFrags.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/5I_1_250821_batch17_06_FLASH.notCombined_1.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/5I_1_250821_batch17_06_FLASH.notCombined_2.fastq\n",
      "[FLASH]     sequence_merge_method/E_merged_output/5I_1_250821_batch17_06_FLASH.hist\n",
      "[FLASH]     sequence_merge_method/E_merged_output/5I_1_250821_batch17_06_FLASH.histogram\n",
      "[FLASH]  \n",
      "[FLASH] Parameters:\n",
      "[FLASH]     Min overlap:           126\n",
      "[FLASH]     Max overlap:           126\n",
      "[FLASH]     Max mismatch density:  0.250000\n",
      "[FLASH]     Allow \"outie\" pairs:   false\n",
      "[FLASH]     Cap mismatch quals:    false\n",
      "[FLASH]     Combiner threads:      4\n",
      "[FLASH]     Input format:          FASTQ, phred_offset=33\n",
      "[FLASH]     Output format:         FASTQ, phred_offset=33\n",
      "[FLASH]  \n",
      "[FLASH] Starting reader and writer threads\n",
      "[FLASH] Starting 4 combiner threads\n",
      "[FLASH] Processed 135 read pairs\n",
      "[FLASH]  \n",
      "[FLASH] Read combination statistics:\n",
      "[FLASH]     Total pairs:      135\n",
      "[FLASH]     Combined pairs:   47\n",
      "[FLASH]     Uncombined pairs: 88\n",
      "[FLASH]     Percent combined: 34.81%\n",
      "[FLASH]  \n",
      "[FLASH] Writing histogram files.\n",
      "[FLASH]  \n",
      "[FLASH] FLASH v1.2.11 complete!\n",
      "[FLASH] 0.014 seconds elapsed\n",
      " FLASH merging complete  sequence_merge_method/E_merged_output/5I_1_250821_batch17_06_FLASH.fastq\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "# === Folder Setup ===\n",
    "input_folder = \"sequence_merge_method/D_split_reads\"\n",
    "output_folder = \"sequence_merge_method/E_merged_output\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# === Set N-values (Overlap Length) per Sample Prefix ===\n",
    "sample_n_mapping = {\n",
    "    \"0N\": 126,\n",
    "    \"1D\": 126,\n",
    "    \"2S\": 126,\n",
    "    \"3G\": 124,\n",
    "    \"4I\": 128,\n",
    "    \"5S\": 124,\n",
    "    \"6T\": 122,\n",
    "    \"5K\": 124,\n",
    "    \"1X8\": 116,\n",
    "    \"0X8\": 132,\n",
    "    \"4G\": 126,\n",
    "    \"5I\": 126,\n",
    "    \"6S\": 124,\n",
    "    \"7T\": 120,\n",
    "    \"3SP26\": 122,\n",
    "    \"3SP31\": 118   \n",
    "}\n",
    "\n",
    "# === Find List of all R1_B Files ===\n",
    "r1_files = glob.glob(os.path.join(input_folder, \"*_R1_B.fastq.gz\"))\n",
    "\n",
    "print(f\" Found {len(r1_files)} R1_B files.\")\n",
    "\n",
    "# === Process Each R1_B File ===\n",
    "for r1_path in r1_files:\n",
    "    sample_base = os.path.basename(r1_path).replace(\"_R1_B.fastq.gz\", \"\")\n",
    "    r2_path = os.path.join(input_folder, f\"{sample_base}_R2_B.fastq.gz\")\n",
    "\n",
    "    if not os.path.exists(r2_path):\n",
    "        print(f\" Matching R2_B file not found for {sample_base}  Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Find the corresponding N value for the filename\n",
    "    matched_n = None\n",
    "    for prefix, n_value in sample_n_mapping.items():\n",
    "        if prefix in sample_base:\n",
    "            matched_n = n_value\n",
    "            break\n",
    "\n",
    "    if matched_n is None:\n",
    "        print(f\" No N value matched for {sample_base}  Skipping.\")\n",
    "        continue\n",
    "\n",
    "    output_name = f\"{sample_base}_FLASH\"\n",
    "\n",
    "    print(f\" Running FLASH for sample: {sample_base} (N={matched_n})\")\n",
    "\n",
    "    try:\n",
    "        # Execute the FLASH command\n",
    "        subprocess.check_call([\n",
    "            \"flash\",\n",
    "            \"-m\", str(matched_n),   # Minimum overlap\n",
    "            \"-M\", str(matched_n),   # Maximum overlap\n",
    "            \"-o\", output_name,      # Output file prefix\n",
    "            \"-d\", output_folder,    # Output directory\n",
    "            r1_path,\n",
    "            r2_path\n",
    "        ])\n",
    "        print(f\" FLASH merging complete  {os.path.join(output_folder, output_name)}.extendedFrags.fastq\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\" FLASH merging failed for {sample_base}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077413cc",
   "metadata": {},
   "source": [
    "## 5.4 Assemble \n",
    "## R1_Front - [R1_Back]-[R2_Back]_merged (FLASH) - R2_Front_ReverseComplement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3c6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 8 merged samples to assemble.\n",
      " Assembling for sample: 2S_1_250821_batch17_03_assemble.fastq.gz\n",
      " Assembled FASTQ saved: sequence_merge_method/1_assemble/2S_1_250821_batch17_03_assemble.fastq.gz\n",
      " Assembling for sample: 0N_1_250821_batch17_02_assemble.fastq.gz\n",
      " Assembled FASTQ saved: sequence_merge_method/1_assemble/0N_1_250821_batch17_02_assemble.fastq.gz\n",
      " Assembling for sample: 3SP26_250828_batch18_04_assemble.fastq.gz\n",
      " Assembled FASTQ saved: sequence_merge_method/1_assemble/3SP26_250828_batch18_04_assemble.fastq.gz\n",
      " Assembling for sample: 6S_1_250821_batch17_07_assemble.fastq.gz\n",
      " Assembled FASTQ saved: sequence_merge_method/1_assemble/6S_1_250821_batch17_07_assemble.fastq.gz\n",
      " Assembling for sample: 4G_1_250821_batch17_05_assemble.fastq.gz\n",
      " Assembled FASTQ saved: sequence_merge_method/1_assemble/4G_1_250821_batch17_05_assemble.fastq.gz\n",
      " Assembling for sample: 7T_1_250821_batch17_08_assemble.fastq.gz\n",
      " Assembled FASTQ saved: sequence_merge_method/1_assemble/7T_1_250821_batch17_08_assemble.fastq.gz\n",
      " Assembling for sample: 5I_1_250821_batch17_06_assemble.fastq.gz\n",
      " Assembled FASTQ saved: sequence_merge_method/1_assemble/5I_1_250821_batch17_06_assemble.fastq.gz\n",
      " Assembling for sample: 1D_1_250821_batch17_01_assemble.fastq.gz\n",
      " Assembled FASTQ saved: sequence_merge_method/1_assemble/1D_1_250821_batch17_01_assemble.fastq.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "import glob\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "def load_fastq_to_dict(file_path):\n",
    "    \"\"\"Loads a FASTQ file into a dictionary: key=read_id, value=(sequence, quality).\"\"\"\n",
    "    data = {}\n",
    "    open_func = gzip.open if file_path.endswith(\".gz\") else open\n",
    "\n",
    "    with open_func(file_path, \"rt\") as handle:\n",
    "        for record in SeqIO.parse(handle, \"fastq\"):\n",
    "            seq = str(record.seq)\n",
    "            qual = record.letter_annotations[\"phred_quality\"]\n",
    "            data[record.id] = (seq, qual)\n",
    "    return data\n",
    "\n",
    "def assemble_fastq(r1_path, merged_path, r2_path, output_path):\n",
    "    \"\"\"Assembles the final sequence from R1_F, Merged, and R2_F_revcomp fragments.\"\"\"\n",
    "    print(f\" Assembling for sample: {os.path.basename(output_path)}\")\n",
    "    r1_dict = load_fastq_to_dict(r1_path)\n",
    "    r2_dict = load_fastq_to_dict(r2_path)\n",
    "\n",
    "    with open(merged_path, \"r\") as merged_file, gzip.open(output_path, \"wt\") as output_file:\n",
    "        for record in SeqIO.parse(merged_file, \"fastq\"):\n",
    "            read_id = record.id\n",
    "            merged_seq = str(record.seq)\n",
    "            merged_qual = record.letter_annotations[\"phred_quality\"]\n",
    "\n",
    "            # A read must have corresponding R1 and R2 fragments to be assembled.\n",
    "            if read_id not in r1_dict or read_id not in r2_dict:\n",
    "                continue  \n",
    "\n",
    "            r1_seq, r1_qual = r1_dict[read_id]\n",
    "            r2_seq, r2_qual = r2_dict[read_id]\n",
    "\n",
    "            # Concatenate in order: R1_F  Merged_Fragment  R2_F_revcomp\n",
    "            full_seq = r1_seq + merged_seq + r2_seq\n",
    "            full_qual = r1_qual + merged_qual + r2_qual\n",
    "\n",
    "            new_record = SeqRecord(\n",
    "                Seq(full_seq),\n",
    "                id=read_id,\n",
    "                description=\"\",\n",
    "                letter_annotations={\"phred_quality\": full_qual}\n",
    "            )\n",
    "\n",
    "            SeqIO.write(new_record, output_file, \"fastq\")\n",
    "\n",
    "    print(f\" Assembled FASTQ saved: {output_path}\")\n",
    "\n",
    "# ===== Automate Processing for All Samples =====\n",
    "\n",
    "# Set up paths\n",
    "input_merged_folder = \"sequence_merge_method/E_merged_output\"\n",
    "input_split_folder = \"sequence_merge_method/D_split_reads\"\n",
    "output_folder = \"sequence_merge_method/1_assemble\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get a list of all merged files from FLASH\n",
    "merged_files = glob.glob(os.path.join(input_merged_folder, \"*_FLASH.extendedFrags.fastq\"))\n",
    "\n",
    "print(f\" Found {len(merged_files)} merged samples to assemble.\")\n",
    "\n",
    "for merged_file in merged_files:\n",
    "    sample_base = os.path.basename(merged_file).replace(\"_FLASH.extendedFrags.fastq\", \"\")\n",
    "\n",
    "    r1_path = os.path.join(input_split_folder, f\"{sample_base}_R1_F.fastq.gz\")\n",
    "    r2_path = os.path.join(input_split_folder, f\"{sample_base}_R2_F_revcomp.fastq.gz\")\n",
    "    output_path = os.path.join(output_folder, f\"{sample_base}_assemble.fastq.gz\")\n",
    "\n",
    "    if os.path.exists(r1_path) and os.path.exists(r2_path):\n",
    "        assemble_fastq(r1_path, merged_file, r2_path, output_path)\n",
    "    else:\n",
    "        print(f\" Missing split files for {sample_base}, skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea775e95",
   "metadata": {},
   "source": [
    "# 6. fastq -> fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442f9045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion from 4G_1_250821_batch17_05_assemble.fastq.gz  4G_1_250821_batch17_05_assemble.fasta is complete.\n",
      "Conversion from 0N_1_250821_batch17_02_assemble.fastq.gz  0N_1_250821_batch17_02_assemble.fasta is complete.\n",
      "Conversion from 1D_1_250821_batch17_01_assemble.fastq.gz  1D_1_250821_batch17_01_assemble.fasta is complete.\n",
      "Conversion from 5I_1_250821_batch17_06_assemble.fastq.gz  5I_1_250821_batch17_06_assemble.fasta is complete.\n",
      "Conversion from 6S_1_250821_batch17_07_assemble.fastq.gz  6S_1_250821_batch17_07_assemble.fasta is complete.\n",
      "Conversion from 7T_1_250821_batch17_08_assemble.fastq.gz  7T_1_250821_batch17_08_assemble.fasta is complete.\n",
      "Conversion from 3SP26_250828_batch18_04_assemble.fastq.gz  3SP26_250828_batch18_04_assemble.fasta is complete.\n",
      "Conversion from 2S_1_250821_batch17_03_assemble.fastq.gz  2S_1_250821_batch17_03_assemble.fasta is complete.\n",
      "All conversions are done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "from Bio import SeqIO\n",
    "\n",
    "# Folder containing the assembled FASTQ.GZ files.\n",
    "input_folder = \"sequence_merge_method/1_assemble\"\n",
    "# Folder to save the converted FASTA files.\n",
    "output_folder = \"sequence_merge_method/2_fastq_to_fasta\"\n",
    "\n",
    "# Create the output folder if it doesn't exist.\n",
    "os.makedirs(output_folder, exist_ok=True)  \n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    # Convert only files ending with \"_assemble.fastq.gz\".\n",
    "    if filename.endswith(\"_assemble.fastq.gz\"):\n",
    "        # Input FASTQ.GZ file path.\n",
    "        input_file = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Output FASTA file path (change extension to .fasta).\n",
    "        output_file = os.path.join(\n",
    "            output_folder,\n",
    "            filename.replace(\"_assemble.fastq.gz\", \"_assemble.fasta\")\n",
    "        )\n",
    "\n",
    "        # Read the input FASTQ.GZ and write to the output FASTA file.\n",
    "        # Using SeqIO.convert is more memory-efficient than loading all records into a list first.\n",
    "        with gzip.open(input_file, \"rt\") as in_handle, open(output_file, \"w\") as out_handle:\n",
    "            SeqIO.convert(in_handle, \"fastq\", out_handle, \"fasta\")\n",
    "\n",
    "        print(f\"Conversion from {filename}  {os.path.basename(output_file)} is complete.\")\n",
    "\n",
    "print(\"All conversions are done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87eb850",
   "metadata": {},
   "source": [
    "# 7. reference sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2a7f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bwa_index] Pack FASTA... 0.00 sec\r\n",
      "[bwa_index] Construct BWT for the packed sequence...\r\n",
      "[bwa_index] 0.00 seconds elapse.\r\n",
      "[bwa_index] Update BWT... 0.00 sec\r\n",
      "[bwa_index] Pack forward-only FASTA... 0.00 sec\r\n",
      "[bwa_index] Construct SA from BWT and Occ... 0.00 sec\r\n",
      "[main] Version: 0.7.17-r1188\r\n",
      "[main] CMD: bwa index Answer_sequence/full_sequences.fasta\r\n",
      "[main] Real time: 0.039 sec; CPU: 0.011 sec\r\n"
     ]
    }
   ],
   "source": [
    "!bwa index \"reference_sequence/full_sequences.fasta\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d4c714",
   "metadata": {},
   "source": [
    "# 8. reference sequence - Sample matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dedb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[M::bwa_idx_load_from_disk] read 0 ALT contigs\n",
      "[M::process] read 50 sequences (8800 bp)...\n",
      "[M::mem_process_seqs] Processed 50 reads in 0.020 CPU sec, 0.010 real sec\n",
      "[main] Version: 0.7.17-r1188\n",
      "[main] CMD: bwa mem -M -t 4 Answer_sequence/full_sequences.fasta sequence_merge_method/2_fastq_to_fasta/0N_1_250821_batch17_02_assemble.fasta\n",
      "[main] Real time: 0.030 sec; CPU: 0.023 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment completed for sequence_merge_method/2_fastq_to_fasta/0N_1_250821_batch17_02_assemble.fasta. Result saved as sequence_merge_method/3_align_sam/0N_1_250821_batch17_02_assemble.sam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[M::bwa_idx_load_from_disk] read 0 ALT contigs\n",
      "[M::process] read 63 sequences (11088 bp)...\n",
      "[M::mem_process_seqs] Processed 63 reads in 0.021 CPU sec, 0.010 real sec\n",
      "[main] Version: 0.7.17-r1188\n",
      "[main] CMD: bwa mem -M -t 4 Answer_sequence/full_sequences.fasta sequence_merge_method/2_fastq_to_fasta/1D_1_250821_batch17_01_assemble.fasta\n",
      "[main] Real time: 0.035 sec; CPU: 0.024 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment completed for sequence_merge_method/2_fastq_to_fasta/1D_1_250821_batch17_01_assemble.fasta. Result saved as sequence_merge_method/3_align_sam/1D_1_250821_batch17_01_assemble.sam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[M::bwa_idx_load_from_disk] read 0 ALT contigs\n",
      "[M::process] read 47 sequences (8272 bp)...\n",
      "[M::mem_process_seqs] Processed 47 reads in 0.025 CPU sec, 0.013 real sec\n",
      "[main] Version: 0.7.17-r1188\n",
      "[main] CMD: bwa mem -M -t 4 Answer_sequence/full_sequences.fasta sequence_merge_method/2_fastq_to_fasta/2S_1_250821_batch17_03_assemble.fasta\n",
      "[main] Real time: 0.036 sec; CPU: 0.028 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment completed for sequence_merge_method/2_fastq_to_fasta/2S_1_250821_batch17_03_assemble.fasta. Result saved as sequence_merge_method/3_align_sam/2S_1_250821_batch17_03_assemble.sam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[M::bwa_idx_load_from_disk] read 0 ALT contigs\n",
      "[M::process] read 420 sequences (75600 bp)...\n",
      "[M::mem_process_seqs] Processed 420 reads in 0.349 CPU sec, 0.099 real sec\n",
      "[main] Version: 0.7.17-r1188\n",
      "[main] CMD: bwa mem -M -t 4 Answer_sequence/full_sequences.fasta sequence_merge_method/2_fastq_to_fasta/3SP26_250828_batch18_04_assemble.fasta\n",
      "[main] Real time: 0.132 sec; CPU: 0.354 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment completed for sequence_merge_method/2_fastq_to_fasta/3SP26_250828_batch18_04_assemble.fasta. Result saved as sequence_merge_method/3_align_sam/3SP26_250828_batch18_04_assemble.sam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[M::bwa_idx_load_from_disk] read 0 ALT contigs\n",
      "[M::process] read 61 sequences (10736 bp)...\n",
      "[M::mem_process_seqs] Processed 61 reads in 0.039 CPU sec, 0.013 real sec\n",
      "[main] Version: 0.7.17-r1188\n",
      "[main] CMD: bwa mem -M -t 4 Answer_sequence/full_sequences.fasta sequence_merge_method/2_fastq_to_fasta/4G_1_250821_batch17_05_assemble.fasta\n",
      "[main] Real time: 0.038 sec; CPU: 0.042 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment completed for sequence_merge_method/2_fastq_to_fasta/4G_1_250821_batch17_05_assemble.fasta. Result saved as sequence_merge_method/3_align_sam/4G_1_250821_batch17_05_assemble.sam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[M::bwa_idx_load_from_disk] read 0 ALT contigs\n",
      "[M::process] read 47 sequences (8272 bp)...\n",
      "[M::mem_process_seqs] Processed 47 reads in 0.018 CPU sec, 0.013 real sec\n",
      "[main] Version: 0.7.17-r1188\n",
      "[main] CMD: bwa mem -M -t 4 Answer_sequence/full_sequences.fasta sequence_merge_method/2_fastq_to_fasta/5I_1_250821_batch17_06_assemble.fasta\n",
      "[main] Real time: 0.033 sec; CPU: 0.021 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment completed for sequence_merge_method/2_fastq_to_fasta/5I_1_250821_batch17_06_assemble.fasta. Result saved as sequence_merge_method/3_align_sam/5I_1_250821_batch17_06_assemble.sam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[M::bwa_idx_load_from_disk] read 0 ALT contigs\n",
      "[M::process] read 47 sequences (8366 bp)...\n",
      "[M::mem_process_seqs] Processed 47 reads in 0.029 CPU sec, 0.012 real sec\n",
      "[main] Version: 0.7.17-r1188\n",
      "[main] CMD: bwa mem -M -t 4 Answer_sequence/full_sequences.fasta sequence_merge_method/2_fastq_to_fasta/6S_1_250821_batch17_07_assemble.fasta\n",
      "[main] Real time: 0.033 sec; CPU: 0.032 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment completed for sequence_merge_method/2_fastq_to_fasta/6S_1_250821_batch17_07_assemble.fasta. Result saved as sequence_merge_method/3_align_sam/6S_1_250821_batch17_07_assemble.sam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[M::bwa_idx_load_from_disk] read 0 ALT contigs\n",
      "[M::process] read 50 sequences (9100 bp)...\n",
      "[M::mem_process_seqs] Processed 50 reads in 0.031 CPU sec, 0.015 real sec\n",
      "[main] Version: 0.7.17-r1188\n",
      "[main] CMD: bwa mem -M -t 4 Answer_sequence/full_sequences.fasta sequence_merge_method/2_fastq_to_fasta/7T_1_250821_batch17_08_assemble.fasta\n",
      "[main] Real time: 0.037 sec; CPU: 0.035 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment completed for sequence_merge_method/2_fastq_to_fasta/7T_1_250821_batch17_08_assemble.fasta. Result saved as sequence_merge_method/3_align_sam/7T_1_250821_batch17_08_assemble.sam\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Set the path to the reference sequence file\n",
    "reference_file=\"reference_sequence/full_sequences.fasta\"\n",
    "\n",
    "# Set the directory containing your filtered FASTA files\n",
    "fasta_directory=\"sequence_merge_method/2_fastq_to_fasta\"\n",
    "# Set the output directory for aligned SAM files\n",
    "output_dir=\"sequence_merge_method/3_align_sam\"\n",
    "\n",
    "# Make sure the output directory exists or create it if necessary\n",
    "mkdir -p \"$output_dir\"\n",
    "\n",
    "# Iterate through filtered FASTA files in the specified directory\n",
    "for fasta_file in \"$fasta_directory\"/*_assemble.fasta; do\n",
    "    # Generate an output file name based on the input filename\n",
    "    output_file=\"$output_dir/$(basename \"$fasta_file\" .fasta).sam\"\n",
    "\n",
    "    # Perform the BWA alignment \n",
    "    bwa mem -M -t 4 \"$reference_file\" \"$fasta_file\" > \"$output_file\"\n",
    "\n",
    "    echo \"Alignment completed for $fasta_file. Result saved as $output_file\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3177f110",
   "metadata": {},
   "source": [
    "## 8.1 sam to bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "055d51f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion from sequence_merge_method/3_align_sam/0N_1_250821_batch17_02_assemble.sam to sequence_merge_method/4_align_bam/0N_1_250821_batch17_02_assemble.bam is complete.\n",
      "Conversion from sequence_merge_method/3_align_sam/1D_1_250821_batch17_01_assemble.sam to sequence_merge_method/4_align_bam/1D_1_250821_batch17_01_assemble.bam is complete.\n",
      "Conversion from sequence_merge_method/3_align_sam/2S_1_250821_batch17_03_assemble.sam to sequence_merge_method/4_align_bam/2S_1_250821_batch17_03_assemble.bam is complete.\n",
      "Conversion from sequence_merge_method/3_align_sam/3SP26_250828_batch18_04_assemble.sam to sequence_merge_method/4_align_bam/3SP26_250828_batch18_04_assemble.bam is complete.\n",
      "Conversion from sequence_merge_method/3_align_sam/4G_1_250821_batch17_05_assemble.sam to sequence_merge_method/4_align_bam/4G_1_250821_batch17_05_assemble.bam is complete.\n",
      "Conversion from sequence_merge_method/3_align_sam/5I_1_250821_batch17_06_assemble.sam to sequence_merge_method/4_align_bam/5I_1_250821_batch17_06_assemble.bam is complete.\n",
      "Conversion from sequence_merge_method/3_align_sam/6S_1_250821_batch17_07_assemble.sam to sequence_merge_method/4_align_bam/6S_1_250821_batch17_07_assemble.bam is complete.\n",
      "Conversion from sequence_merge_method/3_align_sam/7T_1_250821_batch17_08_assemble.sam to sequence_merge_method/4_align_bam/7T_1_250821_batch17_08_assemble.bam is complete.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Set the path to the directory containing SAM files\n",
    "sam_dir=\"sequence_merge_method/3_align_sam\"\n",
    "# Set the output directory for BAM files\n",
    "bam_dir=\"sequence_merge_method/4_align_bam\"\n",
    "\n",
    "\n",
    "# Make sure the output directory exists or create it if necessary\n",
    "mkdir -p \"$bam_dir\"\n",
    "\n",
    "# Convert SAM files to BAM\n",
    "for sam_file in \"$sam_dir\"/*.sam; do\n",
    "    bam_file=\"$bam_dir/$(basename \"$sam_file\" .sam).bam\"\n",
    "    samtools view -bS \"$sam_file\" -o \"$bam_file\"\n",
    "    echo \"Conversion from $sam_file to $bam_file is complete.\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2c0b11",
   "metadata": {},
   "source": [
    "## 8.2  Convert BAM to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abee9791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sequence_merge_method/4_align_bam/csv/1D_1_250821_batch17_01_assemble.csv',\n",
       " 'sequence_merge_method/4_align_bam/csv/7T_1_250821_batch17_08_assemble.csv',\n",
       " 'sequence_merge_method/4_align_bam/csv/4G_1_250821_batch17_05_assemble.csv',\n",
       " 'sequence_merge_method/4_align_bam/csv/3SP26_250828_batch18_04_assemble.csv',\n",
       " 'sequence_merge_method/4_align_bam/csv/5I_1_250821_batch17_06_assemble.csv',\n",
       " 'sequence_merge_method/4_align_bam/csv/0N_1_250821_batch17_02_assemble.csv',\n",
       " 'sequence_merge_method/4_align_bam/csv/6S_1_250821_batch17_07_assemble.csv',\n",
       " 'sequence_merge_method/4_align_bam/csv/2S_1_250821_batch17_03_assemble.csv']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pysam\n",
    "import pandas as pd\n",
    "\n",
    "# Input folder (path where BAM files are located).\n",
    "input_folder = \"sequence_merge_method/4_align_bam\"\n",
    "# Output folder (path to save CSV files).\n",
    "output_folder = \"sequence_merge_method/4_align_bam/csv\"\n",
    "\n",
    "# Create the output folder if it does not exist.\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Function to convert a BAM file to CSV, including optional fields.\n",
    "def bam_to_csv(bam_file, output_folder):\n",
    "    output_csv = os.path.join(output_folder, os.path.basename(bam_file).replace(\".bam\", \".csv\"))\n",
    "    \n",
    "    # Read the BAM file.\n",
    "    with pysam.AlignmentFile(bam_file, \"rb\") as bam:\n",
    "        records = []\n",
    "        \n",
    "        for read in bam:\n",
    "            # Standard BAM fields.\n",
    "            record = {\n",
    "                \"QNAME\": read.query_name,\n",
    "                \"FLAG\": read.flag,\n",
    "                \"RNAME\": bam.get_reference_name(read.reference_id) if read.reference_id >= 0 else \"*\",\n",
    "                \"POS\": read.reference_start + 1,\n",
    "                \"MAPQ\": read.mapping_quality,\n",
    "                \"CIGAR\": read.cigarstring if read.cigarstring else \"*\",\n",
    "                \"RNEXT\": bam.get_reference_name(read.next_reference_id) if read.next_reference_id >= 0 else \"*\",\n",
    "                \"PNEXT\": read.next_reference_start + 1 if read.next_reference_start >= 0 else 0,\n",
    "                \"TLEN\": read.template_length,\n",
    "                \"SEQ\": read.query_sequence if read.query_sequence else \"*\",\n",
    "                \"QUAL\": read.qual if read.qual else \"*\",\n",
    "            }\n",
    "            \n",
    "            # Add optional fields (tags).\n",
    "            for tag, value in read.tags:\n",
    "                record[tag] = value\n",
    "\n",
    "            records.append(record)\n",
    "    \n",
    "    # Create a DataFrame from the list of records.\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    # Fill any missing optional fields with \"*\" instead of NaN for consistency.\n",
    "    df = df.fillna(\"*\")\n",
    "\n",
    "    # Save the DataFrame to a CSV file.\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\" Converted: {os.path.basename(bam_file)} -> {os.path.basename(output_csv)}\")\n",
    "    return output_csv\n",
    "\n",
    "# Find all BAM files in the input folder.\n",
    "bam_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".bam\")]\n",
    "\n",
    "# Convert all found BAM files to CSV.\n",
    "csv_files = []\n",
    "for bam_file in bam_files:\n",
    "    csv_file = bam_to_csv(bam_file, output_folder)\n",
    "    csv_files.append(csv_file)\n",
    "\n",
    "# Print the list of newly created CSV files.\n",
    "print(\"\\nList of converted files:\")\n",
    "print(csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f1549",
   "metadata": {},
   "source": [
    "## 8.3 Filter Alignments by MAPQ Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328d307d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0N_1_250821_batch17_02_assemble.csv -> kept=48, removed=2, saved: 0N_1_250821_batch17_02_assemble.csv\n",
      " 1D_1_250821_batch17_01_assemble.csv -> kept=59, removed=4, saved: 1D_1_250821_batch17_01_assemble.csv\n",
      " 2S_1_250821_batch17_03_assemble.csv -> kept=43, removed=4, saved: 2S_1_250821_batch17_03_assemble.csv\n",
      " 3SP26_250828_batch18_04_assemble.csv -> kept=366, removed=54, saved: 3SP26_250828_batch18_04_assemble.csv\n",
      " 4G_1_250821_batch17_05_assemble.csv -> kept=48, removed=13, saved: 4G_1_250821_batch17_05_assemble.csv\n",
      " 5I_1_250821_batch17_06_assemble.csv -> kept=46, removed=1, saved: 5I_1_250821_batch17_06_assemble.csv\n",
      " 6S_1_250821_batch17_07_assemble.csv -> kept=43, removed=4, saved: 6S_1_250821_batch17_07_assemble.csv\n",
      " 7T_1_250821_batch17_08_assemble.csv -> kept=42, removed=8, saved: 7T_1_250821_batch17_08_assemble.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Input/Output folders\n",
    "csv_dir = Path(\"sequence_merge_method/4_align_bam/csv\")\n",
    "out_dir = csv_dir / \"MAPQ_removed\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ===== Set the threshold (alignments with MAPQ <= this value will be removed) =====\n",
    "MAPQ_THRESHOLD = 10\n",
    "# =================================================================================\n",
    "\n",
    "for in_path in sorted(csv_dir.glob(\"*.csv\")):\n",
    "    try:\n",
    "        df = pd.read_csv(in_path)\n",
    "    except Exception as e:\n",
    "        print(f\"  Read fail: {in_path.name} -> {e}\")\n",
    "        continue\n",
    "\n",
    "    if \"MAPQ\" not in df.columns:\n",
    "        print(f\"  Skip (no MAPQ column): {in_path.name}\")\n",
    "        continue\n",
    "\n",
    "    # Convert MAPQ column to a numeric type, coercing errors into 'Not a Number' (NaN)\n",
    "    m = pd.to_numeric(df[\"MAPQ\"], errors=\"coerce\")\n",
    "    \n",
    "    # Create a boolean mask to identify rows to keep.\n",
    "    # Keep rows where MAPQ > threshold.\n",
    "    # Also, keep rows where MAPQ is NaN (e.g., unaligned reads), to remove them, delete '| m.isna()'.\n",
    "    keep_mask = (m > MAPQ_THRESHOLD) | m.isna()\n",
    "    \n",
    "    kept = int(keep_mask.sum())\n",
    "    removed = int((~keep_mask).sum())\n",
    "\n",
    "    out_path = out_dir / in_path.name\n",
    "    df.loc[keep_mask].to_csv(out_path, index=False)\n",
    "    print(f\" {in_path.name} -> kept={kept}, removed={removed}, saved: {out_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077d3b00",
   "metadata": {},
   "source": [
    "# Histogram Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ed9bc6",
   "metadata": {},
   "source": [
    "## A. Generate Histogram Data from Aligned Reads(MAPQ filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e353ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved full RNAME histogram: sequence_merge_method/5_histogram/histogram_2S_1_250821_batch17_03_assemble.csv\n",
      " Saved full RNAME histogram: sequence_merge_method/5_histogram/histogram_6S_1_250821_batch17_07_assemble.csv\n",
      " Saved full RNAME histogram: sequence_merge_method/5_histogram/histogram_0N_1_250821_batch17_02_assemble.csv\n",
      " Saved full RNAME histogram: sequence_merge_method/5_histogram/histogram_5I_1_250821_batch17_06_assemble.csv\n",
      " Saved full RNAME histogram: sequence_merge_method/5_histogram/histogram_4G_1_250821_batch17_05_assemble.csv\n",
      " Saved full RNAME histogram: sequence_merge_method/5_histogram/histogram_3SP26_250828_batch18_04_assemble.csv\n",
      " Saved full RNAME histogram: sequence_merge_method/5_histogram/histogram_7T_1_250821_batch17_08_assemble.csv\n",
      " Saved full RNAME histogram: sequence_merge_method/5_histogram/histogram_1D_1_250821_batch17_01_assemble.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder setup\n",
    "input_folder = \"sequence_merge_method/4_align_bam/csv/MAPQ_removed\"\n",
    "histogram_folder = \"sequence_merge_method/5_histogram\"\n",
    "os.makedirs(histogram_folder, exist_ok=True)\n",
    "\n",
    "# Process all CSV files in the input folder\n",
    "files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(input_folder, file_name)\n",
    "    output_csv = os.path.join(histogram_folder, f\"histogram_{file_name}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, dtype=str)\n",
    "        if 'RNAME' not in df.columns:\n",
    "            print(f\"Skipping file: {file_name} (no 'RNAME' column found)\")\n",
    "            continue\n",
    "\n",
    "        # Count the occurrences of each unique RNAME\n",
    "        rname_counts = df['RNAME'].value_counts().reset_index()\n",
    "        rname_counts.columns = ['RNAME', 'Count']\n",
    "        \n",
    "        # Add metadata and calculate normalized counts\n",
    "        rname_counts.insert(0, 'File_Name', file_name)\n",
    "        rname_counts['Count'] = rname_counts['Count'].astype(int)\n",
    "        total_count = rname_counts['Count'].sum()\n",
    "        rname_counts['Normalized_Count'] = rname_counts['Count'] / total_count\n",
    "\n",
    "        # Save the histogram data to a new CSV file\n",
    "        rname_counts.to_csv(output_csv, index=False)\n",
    "        print(f\" Saved full RNAME histogram: {output_csv}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error processing file '{file_name}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfa0f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved plot: sequence_merge_method/5_histogram/graph_top5/histogram_0N_1_250821_batch17_02_assemble.png, sequence_merge_method/5_histogram/graph_top5/histogram_0N_1_250821_batch17_02_assemble.svg\n",
      " Saved plot: sequence_merge_method/5_histogram/graph_top5/histogram_5I_1_250821_batch17_06_assemble.png, sequence_merge_method/5_histogram/graph_top5/histogram_5I_1_250821_batch17_06_assemble.svg\n",
      " Saved plot: sequence_merge_method/5_histogram/graph_top5/histogram_7T_1_250821_batch17_08_assemble.png, sequence_merge_method/5_histogram/graph_top5/histogram_7T_1_250821_batch17_08_assemble.svg\n",
      " Saved plot: sequence_merge_method/5_histogram/graph_top5/histogram_4G_1_250821_batch17_05_assemble.png, sequence_merge_method/5_histogram/graph_top5/histogram_4G_1_250821_batch17_05_assemble.svg\n",
      " Saved plot: sequence_merge_method/5_histogram/graph_top5/histogram_1D_1_250821_batch17_01_assemble.png, sequence_merge_method/5_histogram/graph_top5/histogram_1D_1_250821_batch17_01_assemble.svg\n",
      " Saved plot: sequence_merge_method/5_histogram/graph_top5/histogram_3SP26_250828_batch18_04_assemble.png, sequence_merge_method/5_histogram/graph_top5/histogram_3SP26_250828_batch18_04_assemble.svg\n",
      " Saved plot: sequence_merge_method/5_histogram/graph_top5/histogram_6S_1_250821_batch17_07_assemble.png, sequence_merge_method/5_histogram/graph_top5/histogram_6S_1_250821_batch17_07_assemble.svg\n",
      " Saved plot: sequence_merge_method/5_histogram/graph_top5/histogram_2S_1_250821_batch17_03_assemble.png, sequence_merge_method/5_histogram/graph_top5/histogram_2S_1_250821_batch17_03_assemble.svg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Folder setup\n",
    "histogram_folder = \"sequence_merge_method/5_histogram\"\n",
    "summary_folder = \"sequence_merge_method/5_histogram/graph_top5\"\n",
    "os.makedirs(summary_folder, exist_ok=True)\n",
    "\n",
    "# Highlight mapping (associates sample prefixes with their expected correct RNAME)\n",
    "highlight_mapping = {\n",
    "    \"0N\": \"seq_013_00001101\",\n",
    "    \"1D\": \"seq_035_00100011\",\n",
    "    \"2S\": \"seq_082_01010010\",\n",
    "    \"3G\": \"seq_102_01100110\",\n",
    "    \"4I\": \"seq_136_10001000\",\n",
    "    \"5S\": \"seq_178_10110010\",\n",
    "    \"6T\": \"seq_211_11010011\",\n",
    "    \"5K\": \"seq_170_10101010\",\n",
    "    \"1X8\": \"seq_255_11111111\",\n",
    "    \"0X8\": \"seq_000_00000000\",\n",
    "    \"4G\": \"seq_134_10000110\",\n",
    "    \"5I\": \"seq_168_10101000\",\n",
    "    \"6S\": \"seq_210_11010010\",\n",
    "    \"7T\": \"seq_243_11110011\",\n",
    "    \"3SP26\": \"seq_122_01111010\",\n",
    "    \"3SP31\": \"seq_127_01111111\"\n",
    "}\n",
    "\n",
    "# Iterate through all histogram CSV files\n",
    "csv_files = [f for f in os.listdir(histogram_folder) if f.startswith(\"histogram_\") and f.endswith(\".csv\")]\n",
    "\n",
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(histogram_folder, file_name)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'RNAME' not in df.columns or 'Normalized_Count' not in df.columns:\n",
    "            print(f\"Skipping file: {file_name} (missing column)\")\n",
    "            continue\n",
    "\n",
    "        # Get the top 5 RNAMEs by count\n",
    "        top_df = df.sort_values(by=\"Count\", ascending=False).head(5).reset_index(drop=True)\n",
    "        sample_name = file_name.replace(\"histogram_\", \"\").replace(\".csv\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dee3b5",
   "metadata": {},
   "source": [
    "## B. Create Top 5 Histogram Plots for Each Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b94a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  :\n",
      " - PNG: sequence_merge_method/6_summary/stacked_bar_top5_gray_rest_white_box.png\n",
      " - SVG: sequence_merge_method/6_summary/stacked_bar_top5_gray_rest_white_box.svg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Folder setup\n",
    "histogram_folder = \"sequence_merge_method/5_histogram\"\n",
    "summary_folder = \"sequence_merge_method/5_histogram/graph_top5\"\n",
    "os.makedirs(summary_folder, exist_ok=True)\n",
    "\n",
    "# Highlight mapping (associates sample prefixes with their expected correct RNAME)\n",
    "highlight_mapping = {\n",
    "    \"0N\": \"seq_013_00001101\",\n",
    "    \"1D\": \"seq_035_00100011\",\n",
    "    \"2S\": \"seq_082_01010010\",\n",
    "    \"3G\": \"seq_102_01100110\",\n",
    "    \"4I\": \"seq_136_10001000\",\n",
    "    \"5S\": \"seq_178_10110010\",\n",
    "    \"6T\": \"seq_211_11010011\",\n",
    "    \"5K\": \"seq_170_10101010\",\n",
    "    \"1X8\": \"seq_255_11111111\",\n",
    "    \"0X8\": \"seq_000_00000000\",\n",
    "    \"4G\": \"seq_134_10000110\",\n",
    "    \"5I\": \"seq_168_10101000\",\n",
    "    \"6S\": \"seq_210_11010010\",\n",
    "    \"7T\": \"seq_243_11110011\",\n",
    "    \"3SP26\": \"seq_122_01111010\",\n",
    "    \"3SP31\": \"seq_127_01111111\"\n",
    "}\n",
    "\n",
    "# Iterate through all histogram CSV files\n",
    "csv_files = [f for f in os.listdir(histogram_folder) if f.startswith(\"histogram_\") and f.endswith(\".csv\")]\n",
    "\n",
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(histogram_folder, file_name)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'RNAME' not in df.columns or 'Normalized_Count' not in df.columns:\n",
    "            print(f\"Skipping file: {file_name} (missing column)\")\n",
    "            continue\n",
    "\n",
    "        # Get the top 5 RNAMEs by count\n",
    "        top_df = df.sort_values(by=\"Count\", ascending=False).head(5).reset_index(drop=True)\n",
    "        sample_name = file_name.replace(\"histogram_\", \"\").replace(\".csv\", \"\")\n",
    "        \n",
    "        #  Extract prefix to find the correct RNAME to highlight (e.g., from \"1D_sample_X.csv\" -> \"1D\")\n",
    "        prefix = sample_name.split(\"_\")[0]  \n",
    "        highlight_rname = highlight_mapping.get(prefix, None)\n",
    "\n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(top_df[\"RNAME\"], top_df[\"Normalized_Count\"], color='blue')\n",
    "\n",
    "        # Highlight the expected correct RNAME in red\n",
    "        for bar, rname in zip(bars, top_df[\"RNAME\"]):\n",
    "            if rname == highlight_rname:\n",
    "                bar.set_color('red')\n",
    "\n",
    "        plt.title(f\"Top 5 RNAME Histogram - {sample_name}\")\n",
    "        plt.xlabel(\"RNAME\")\n",
    "        plt.ylabel(\"Normalized Count\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.ylim(0, 1)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the plot in both PNG and SVG formats\n",
    "        output_png = os.path.join(summary_folder, file_name.replace(\".csv\", \".png\"))\n",
    "        output_svg = os.path.join(summary_folder, file_name.replace(\".csv\", \".svg\"))\n",
    "        plt.savefig(output_png)\n",
    "        plt.savefig(output_svg)\n",
    "        plt.close()\n",
    "\n",
    "        print(f\" Saved plot: {output_png}, {output_svg}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error processing {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f3964f",
   "metadata": {},
   "source": [
    "## C. Summarize Highlighted Read Counts into a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c1336f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Highlight summary saved to: sequence_merge_method/6_summary/highlight_result.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === Highlight Mapping (associates sample prefixes with their expected RNAME) ===\n",
    "highlight_mapping = {\n",
    "    \"0N\": \"seq_013_00001101\",\n",
    "    \"1D\": \"seq_035_00100011\",\n",
    "    \"2S\": \"seq_082_01010010\",\n",
    "    \"3G\": \"seq_102_01100110\",\n",
    "    \"4I\": \"seq_136_10001000\",\n",
    "    \"5S\": \"seq_178_10110010\",\n",
    "    \"6T\": \"seq_211_11010011\",\n",
    "    \"5K\": \"seq_170_10101010\",\n",
    "    \"1X8\": \"seq_255_11111111\",\n",
    "    \"0X8\": \"seq_000_00000000\",\n",
    "    \"4G\": \"seq_134_10000110\",\n",
    "    \"5I\": \"seq_168_10101000\",\n",
    "    \"6S\": \"seq_210_11010010\",\n",
    "    \"7T\": \"seq_243_11110011\",\n",
    "    \"3SP26\": \"seq_122_01111010\",\n",
    "    \"3SP31\": \"seq_127_01111111\"\n",
    "}\n",
    "\n",
    "# === Folder Setup ===\n",
    "histogram_folder = \"sequence_merge_method/5_histogram\"\n",
    "summary_folder = \"sequence_merge_method/6_summary\"\n",
    "os.makedirs(summary_folder, exist_ok=True)\n",
    "highlight_result_csv = os.path.join(summary_folder, \"highlight_result.csv\")\n",
    "\n",
    "# === Collect Highlight Summary Information ===\n",
    "highlight_data = []\n",
    "csv_files = [f for f in os.listdir(histogram_folder) if f.startswith(\"histogram_\") and f.endswith(\".csv\")]\n",
    "\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(histogram_folder, file)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        file_name = file.replace(\"histogram_\", \"\")\n",
    "        prefix = file_name.split(\"_\")[0]\n",
    "        highlight_rname = highlight_mapping.get(prefix, \"\")\n",
    "\n",
    "        df['Count'] = df['Count'].astype(int)\n",
    "        total_count = df['Count'].sum()\n",
    "\n",
    "        # Calculate count and percentage for the highlighted (expected) RNAME\n",
    "        highlight_count = df[df['RNAME'] == highlight_rname]['Count'].sum() if highlight_rname else 0\n",
    "        highlight_percentage = (highlight_count / total_count) * 100 if total_count > 0 else 0\n",
    "\n",
    "        # Calculate the ratio of the highlighted count to the second-highest count\n",
    "        sorted_counts = df['Count'].sort_values(ascending=False).values\n",
    "        second_max_count = sorted_counts[1] if len(sorted_counts) >= 2 else 0\n",
    "        highlight_vs_second_ratio = (highlight_count / second_max_count) if second_max_count > 0 else 0\n",
    "\n",
    "        highlight_data.append([\n",
    "            file_name,\n",
    "            highlight_count,\n",
    "            total_count,\n",
    "            highlight_percentage,\n",
    "            highlight_rname,\n",
    "            highlight_vs_second_ratio\n",
    "        ])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error processing file '{file}': {e}\")\n",
    "\n",
    "# === Save the Summary to a CSV File ===\n",
    "highlight_df = pd.DataFrame(highlight_data, columns=[\n",
    "    'File',\n",
    "    'Highlight_Count',\n",
    "    'Total_Count',\n",
    "    'Highlight_Percentage',\n",
    "    'Highlight_RNAME',\n",
    "    'Highlight_vs_SecondTop_Ratio'\n",
    "])\n",
    "highlight_df = highlight_df.sort_values(by='File')\n",
    "highlight_df.to_csv(highlight_result_csv, index=False)\n",
    "\n",
    "print(f\" Highlight summary saved to: {highlight_result_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c92c29a",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0303529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Set Answer Key (same as before) ---\n",
    "answer_data = {\n",
    "    \"0N\": \"seq_013_00001101\", \"1D\": \"seq_035_00100011\",\n",
    "    \"2S\": \"seq_082_01010010\", \"3G\": \"seq_102_01100110\",\n",
    "    \"4I\": \"seq_136_10001000\", \"5S\": \"seq_178_10110010\",\n",
    "    \"6T\": \"seq_211_11010011\", \"5K\": \"seq_170_10101010\",\n",
    "    \"1X8\": \"seq_255_11111111\", \"0X8\": \"seq_000_00000000\",\n",
    "    \"4G\": \"seq_134_10000110\", \"5I\": \"seq_168_10101000\",\n",
    "    \"6S\": \"seq_210_11010010\", \"7T\": \"seq_243_11110011\",\n",
    "    \"3SP26\": \"seq_122_01111010\", \"3SP31\": \"seq_127_01111111\"\n",
    "}\n",
    "# Extract only the 8-digit number for convenience and create answer_key_map\n",
    "# Result example: {'0N': '00001101', '1D': '00100011', ...}\n",
    "answer_key_map = {key: re.search(r'([01]{8})$', value).group(1) for key, value in answer_data.items()}\n",
    "\n",
    "\n",
    "# --- 2. File Processing and Combined Calculation (same as before) ---\n",
    "input_folder = \"/Users/janghochoi/Documents/Ubuntu/TEMPER_final_data/DNA_memory_final_figure/sequence_merge_method/5_histogram\" # or specify directly, e.g., \"fastq/4_align_histogram_MAPQ_removed\"\n",
    "output_path = os.path.join(input_folder, \"summary_combined.csv\")\n",
    "\n",
    "try:\n",
    "    files = [f for f in os.listdir(input_folder) if f.endswith('.csv') and f.startswith(\"histogram_\")]\n",
    "except FileNotFoundError:\n",
    "    print(f\" Error: Folder '{input_folder}' not found.\")\n",
    "    files = []\n",
    "\n",
    "combined_summary_list = []\n",
    "\n",
    "if not files:\n",
    "    print(\" No files found for analysis.\")\n",
    "else:\n",
    "    print(f\" Analyzing a total of {len(files)} files.\")\n",
    "    for file_name in sorted(files):\n",
    "        try:\n",
    "            df = pd.read_csv(os.path.join(input_folder, file_name))\n",
    "            total_count = df['Count'].sum()\n",
    "            \n",
    "            position_counts = [{'0': 0, '1': 0} for _ in range(8)]\n",
    "            for _, row in df.iterrows():\n",
    "                rname = row.get('RNAME', '')\n",
    "                count = int(row['Count'])\n",
    "                match = re.search(r'seq_[^_]+_([01]{8})', rname)\n",
    "                if match:\n",
    "                    eight_digits = match.group(1)\n",
    "                    for i, digit in enumerate(eight_digits):\n",
    "                        position_counts[i][digit] += count\n",
    "            \n",
    "            answer_key = None\n",
    "            for key in answer_key_map:\n",
    "                if key in file_name:\n",
    "                    answer_key = answer_key_map[key]\n",
    "                    break\n",
    "            \n",
    "            combined_row = {\n",
    "                \"File_Name\": file_name,\n",
    "                \"Total_Count\": total_count\n",
    "            }\n",
    "            \n",
    "            accuracies_for_avg = [] \n",
    "            for i in range(8):\n",
    "                zeros_count = position_counts[i]['0']\n",
    "                ones_count = position_counts[i]['1']\n",
    "                \n",
    "                combined_row[f\"Pos{i+1}_Zeros_Count\"] = zeros_count\n",
    "                combined_row[f\"Pos{i+1}_Ones_Count\"] = ones_count\n",
    "                \n",
    "                accuracy = np.nan\n",
    "                if answer_key:\n",
    "                    correct_digit = answer_key[i]\n",
    "                    current_total = zeros_count + ones_count\n",
    "                    correct_count = position_counts[i][correct_digit]\n",
    "                    accuracy = correct_count / current_total if current_total > 0 else 0\n",
    "                \n",
    "                combined_row[f\"Pos{i+1}_Accuracy\"] = accuracy\n",
    "                accuracies_for_avg.append(accuracy)\n",
    "\n",
    "            avg_accuracy = np.nanmean(accuracies_for_avg)\n",
    "            combined_row[\"Avg_Accuracy\"] = avg_accuracy\n",
    "\n",
    "            combined_summary_list.append(combined_row)\n",
    "            if not answer_key:\n",
    "                print(f\" Warning: Answer key not found for {file_name}.\")\n",
    "            print(f\" {file_name} processing complete.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\" Error processing {file_name}: {e}\")\n",
    "\n",
    "# --- 3. Save Combined Results and Add Summary Row (Modified Section) ---\n",
    "if combined_summary_list:\n",
    "    combined_df = pd.DataFrame(combined_summary_list)\n",
    "    \n",
    "    column_order = ['File_Name', 'Total_Count', 'Avg_Accuracy']\n",
    "    for i in range(1, 9):\n",
    "        column_order.append(f'Pos{i}_Zeros_Count')\n",
    "        column_order.append(f'Pos{i}_Ones_Count')\n",
    "        column_order.append(f'Pos{i}_Accuracy')\n",
    "        \n",
    "    final_df = combined_df[[col for col in column_order if col in combined_df.columns]]\n",
    "    \n",
    "    #  Create Summary Row (Modified Logic) \n",
    "    summary_row = {'File_Name': 'Avg. Accuracy'}\n",
    "    # Find and iterate over all columns containing 'Accuracy'\n",
    "    for col_name in final_df.columns:\n",
    "        if 'Accuracy' in col_name:\n",
    "            # Calculate the mean of that Accuracy column and add to the summary row\n",
    "            summary_row[col_name] = final_df[col_name].mean()\n",
    "\n",
    "    #  Add Summary Row to the existing DataFrame \n",
    "    summary_row_df = pd.DataFrame([summary_row]) # Convert dictionary to DataFrame\n",
    "    final_df = pd.concat([final_df, summary_row_df], ignore_index=True)\n",
    "\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(\"\\n--- Final Combined Analysis Results (including summary row) ---\")\n",
    "    print(final_df[['File_Name', 'Total_Count', 'Avg_Accuracy']].tail())\n",
    "    print(f\"\\n All analysis results and the summary row have been saved to '{output_path}'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
